title:: 【Reading Highlights】神器 CLIP：连接文本和图像，打造可迁移的视觉模型
source:: https://zhuanlan.zhihu.com/p/493489688
summary:: 
tags:: [[简悦]] [[clip]]  [[openai]]   [[reading_highlights]]
date:: 20220708  

- > Open AI 在 2021 年 1 月份发布的 [DALL-E](https://openai.com/blog/dall-e/) 和 [CLIP](https://openai.com/blog/clip/)，这两个都属于结合图像和文本的多模态模型，其中 **DALL-E 是基于文本来生成模型的模型**，而 **CLIP 是用文本作为监督信号来训练可迁移的视觉模型**  ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/493489688#js_content:~:text=Open%20AI%20%E5%9C%A8%202021%20%E5%B9%B4%201%20%E6%9C%88%E4%BB%BD%E5%8F%91%E5%B8%83%E7%9A%84%20DALL-E%20%E5%92%8C%20CLIP%EF%BC%8C%E8%BF%99%E4%B8%A4%E4%B8%AA%E9%83%BD%E5%B1%9E%E4%BA%8E%E7%BB%93%E5%90%88%E5%9B%BE%E5%83%8F%E5%92%8C%E6%96%87%E6%9C%AC%E7%9A%84%E5%A4%9A%E6%A8%A1%E6%80%81%E6%A8%A1%E5%9E%8B%EF%BC%8C%E5%85%B6%E4%B8%AD%20DALL-E%20%E6%98%AF%E5%9F%BA%E4%BA%8E%E6%96%87%E6%9C%AC%E6%9D%A5%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%A8%A1%E5%9E%8B%20CLIP%20%E6%98%AF%E7%94%A8%E6%96%87%E6%9C%AC%E4%BD%9C%E4%B8%BA%E7%9B%91%E7%9D%A3%E4%BF%A1%E5%8F%B7%E6%9D%A5%E8%AE%AD%E7%BB%83%E5%8F%AF%E8%BF%81%E7%A7%BB%E7%9A%84%E8%A7%86%E8%A7%89%E6%A8%A1%E5%9E%8B%EF%BC%8C%E8%80%8C))

- > CLIP 的英文全称是 **Contrastive Language-Image Pre-training**，即**一种基于对比文本 - 图像对的预训练方法或者模型**。CLIP 是一种基于对比学习的多模态模型，与 CV 中的一些对比学习方法如 moco 和 simclr 不同的是，CLIP 的训练数据是文本 - 图像对：一张图像和它对应的文本描述，这里希望通过对比学习，模型能够学习到文本 - 图像对的匹配关系。  ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/493489688#js_content:~:text=%20Contrastive%20Language-Image%20Pre-training%E4%B8%80%E7%A7%8D%E5%9F%BA%E4%BA%8E%E5%AF%B9%E6%AF%94%E6%96%87%E6%9C%AC%20-%20%E5%9B%BE%E5%83%8F%E5%AF%B9%E7%9A%84%E9%A2%84%E8%AE%AD%E7%BB%83%E6%96%B9%E6%B3%95%E6%88%96%E8%80%85%E6%A8%A1%E5%9E%8BCLIP%20%E7%9A%84%E8%8B%B1%E6%96%87%E5%85%A8%E7%A7%B0%E6%98%AF%EF%BC%8C%E5%8D%B3%E3%80%82CLIP%20%E6%98%AF%E4%B8%80%E7%A7%8D%E5%9F%BA%E4%BA%8E%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%A4%9A%E6%A8%A1%E6%80%81%E6%A8%A1%E5%9E%8B%EF%BC%8C%E4%B8%8E%20CV%20%E4%B8%AD%E7%9A%84%E4%B8%80%E4%BA%9B%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%E5%A6%82%20moco%20%E5%92%8C%20simclr%20%E4%B8%8D%E5%90%8C%E7%9A%84%E6%98%AF%EF%BC%8CCLIP%20%E7%9A%84%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE%E6%98%AF%E6%96%87%E6%9C%AC%20-%20%E5%9B%BE%E5%83%8F%E5%AF%B9%EF%BC%9A%E4%B8%80%E5%BC%A0%E5%9B%BE%E5%83%8F%E5%92%8C%E5%AE%83%E5%AF%B9%E5%BA%94%E7%9A%84%E6%96%87%E6%9C%AC%E6%8F%8F%E8%BF%B0%EF%BC%8C%E8%BF%99%E9%87%8C%E5%B8%8C%E6%9C%9B%E9%80%9A%E8%BF%87%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0%EF%BC%8C%E6%A8%A1%E5%9E%8B%E8%83%BD%E5%A4%9F%E5%AD%A6%E4%B9%A0%E5%88%B0%E6%96%87%E6%9C%AC%20-%20%E5%9B%BE%E5%83%8F%E5%AF%B9%E7%9A%84%E5%8C%B9%E9%85%8D%E5%85%B3%E7%B3%BB%E3%80%82))

- > CLIP 包括两个模型：**Text Encoder** 和 **Image Encoder**，其中 Text Encoder 用来提取文本的特征，可以采用 NLP 中常用的 text transformer 模型；而 Image Encoder 用来提取图像的特征，可以采用常用 CNN 模型或者 vision transformer。  ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/493489688#js_content:~:text=Text%20Encoder%20Image%20EncoderCLIP%20%E5%8C%85%E6%8B%AC%E4%B8%A4%E4%B8%AA%E6%A8%A1%E5%9E%8B%EF%BC%9A%20%E5%92%8C%EF%BC%8C%E5%85%B6%E4%B8%AD%20Text%20Encoder%20%E7%94%A8%E6%9D%A5%E6%8F%90%E5%8F%96%E6%96%87%E6%9C%AC%E7%9A%84%E7%89%B9%E5%BE%81%EF%BC%8C%E5%8F%AF%E4%BB%A5%E9%87%87%E7%94%A8%20NLP%20%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%20text%20transformer%20%E6%A8%A1%E5%9E%8B%EF%BC%9B%E8%80%8C%20Image%20Encoder%20%E7%94%A8%E6%9D%A5%E6%8F%90%E5%8F%96%E5%9B%BE%E5%83%8F%E7%9A%84%E7%89%B9%E5%BE%81%EF%BC%8C%E5%8F%AF%E4%BB%A5%E9%87%87%E7%94%A8%E5%B8%B8%E7%94%A8%20CNN%20%E6%A8%A1%E5%9E%8B%E6%88%96%E8%80%85%20vision%20transformer%E3%80%82))

- > ![](https://pic2.zhimg.com/v2-b86361b47d4db80258439b8ad33bdf8d_r.jpg) [[模型结构]]  [[clip]]  [[openai]]   ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/493489688#js_content:~:text=https://pic2.zhimg.com/v2-b86361b47d4db80258439b8ad33bdf8d_r.jpg))

- > ![](https://www.zhihu.com/equation?tex=N)  ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/493489688#js_content:~:text=https://www.zhihu.com/equation?tex=N))

- > 相似度直接**计算文本特征和图像特征的余弦相似性（cosine similarity）**  ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/493489688#js_content:~:text=%E8%AE%A1%E7%AE%97%E6%96%87%E6%9C%AC%E7%89%B9%E5%BE%81%E5%92%8C%E5%9B%BE%E5%83%8F%E7%89%B9%E5%BE%81%E7%9A%84%E4%BD%99%E5%BC%A6%E7%9B%B8%E4%BC%BC%E6%80%A7%EF%BC%88cosine%20similarity%EF%BC%89%E7%9B%B8%E4%BC%BC%E5%BA%A6%E7%9B%B4%E6%8E%A5))
  - 📝 N 段文本，N 张图片，两两对比

- > ```
# image_encoder - ResNet or Vision Transformer
# text_encoder - CBOW or Text Transformer
# I[n, h, w, c] - minibatch of aligned images
# T[n, l] - minibatch of aligned texts
# W_i[d_i, d_e] - learned proj of image to embed
# W_t[d_t, d_e] - learned proj of text to embed
# t - learned temperature parameter

# 分别提取图像特征和文本特征
I_f = image_encoder(I) #[n, d_i]
T_f = text_encoder(T) #[n, d_t]

# 对两个特征进行线性投射，得到相同维度的特征，并进行l2归一化
I_e = l2_normalize(np.dot(I_f, W_i), axis=1)
T_e = l2_normalize(np.dot(T_f, W_t), axis=1)

# 计算缩放的余弦相似度：[n, n]
logits = np.dot(I_e, T_e.T) * np.exp(t)

# 对称的对比学习损失：等价于N个类别的cross_entropy_loss
labels = np.arange(n) # 对角线元素的labels
loss_i = cross_entropy_loss(logits, labels, axis=0)
loss_t = cross_entropy_loss(logits, labels, axis=1)
loss = (loss_i + loss_t)/2
``` [[代码]]  [[clip]]   ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/493489688#js_content:~:text=#%20image_encoder%20-%20ResNet%20or%20Vision%20Transformer%0A#%20text_encoder%20-%20CBOW%20or%20Text%20Transformer%0A#%20I%5Bn,%20h,%20w,%20c%5D%20-%20minibatch%20of%20aligned%20images%0A#%20T%5Bn,%20l%5D%20-%20minibatch%20of%20aligned%20texts%0A#%20W_i%5Bd_i,%20d_e%5D%20-%20learned%20proj%20of%20image%20to%20embed%0A#%20W_t%5Bd_t,%20d_e%5D%20-%20learned%20proj%20of%20text%20to%20embed%0A#%20t%20-%20learned%20temperature%20parameter%0A%0A#%20%E5%88%86%E5%88%AB%E6%8F%90%E5%8F%96%E5%9B%BE%E5%83%8F%E7%89%B9%E5%BE%81%E5%92%8C%E6%96%87%E6%9C%AC%E7%89%B9%E5%BE%81%0AI_f%20=%20image_encoder(I)%20#%5Bn,%20d_i%5D%0AT_f%20=%20text_encoder(T)%20#%5Bn,%20d_t%5D%0A%0A#%20%E5%AF%B9%E4%B8%A4%E4%B8%AA%E7%89%B9%E5%BE%81%E8%BF%9B%E8%A1%8C%E7%BA%BF%E6%80%A7%E6%8A%95%E5%B0%84%EF%BC%8C%E5%BE%97%E5%88%B0%E7%9B%B8%E5%90%8C%E7%BB%B4%E5%BA%A6%E7%9A%84%E7%89%B9%E5%BE%81%EF%BC%8C%E5%B9%B6%E8%BF%9B%E8%A1%8Cl2%E5%BD%92%E4%B8%80%E5%8C%96%0AI_e%20=%20l2_normalize(np.dot(I_f,%20W_i),%20axis=1)%0AT_e%20=%20l2_normalize(np.dot(T_f,%20W_t),%20axis=1)%0A%0A#%20%E8%AE%A1%E7%AE%97%E7%BC%A9%E6%94%BE%E7%9A%84%E4%BD%99%E5%BC%A6%E7%9B%B8%E4%BC%BC%E5%BA%A6%EF%BC%9A%5Bn,%20n%5D%0Alogits%20=%20np.dot(I_e,%20T_e.T)%20*%20np.exp(t)%0A%0A#%20%E5%AF%B9%E7%A7%B0%E7%9A%84%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0%E6%8D%9F%E5%A4%B1%EF%BC%9A%E7%AD%89%E4%BB%B7%E4%BA%8EN%E4%B8%AA%E7%B1%BB%E5%88%AB%E7%9A%84cross_entropy_loss%0Alabels%20=%20np.arange(n)%20#%20%E5%AF%B9%E8%A7%92%E7%BA%BF%E5%85%83%E7%B4%A0%E7%9A%84labels%0Aloss_i%20=%20cross_entropy_loss(logits,%20labels,%20axis=0)%0Aloss_t%20=%20cross_entropy_loss(logits,%20labels,%20axis=1)%0Aloss%20=%20(loss_i%20+%20loss_t)/2%0A))

- > 为了训练 CLIP，OpenAI 从互联网收集了共 **4 个亿的文本 - 图像对**，论文称之为 **WebImageText**，如果按照文本的单词量，它和训练 GPT-2 的 WebText 规模类似，如果从数量上对比的话，它还比谷歌的 JFT-300M 数据集多一个亿，所以说这是一个很大规模的数据集。  ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/493489688#js_content:~:text=%204%20%E4%B8%AA%E4%BA%BF%E7%9A%84%E6%96%87%E6%9C%AC%20-%20%E5%9B%BE%E5%83%8F%E5%AF%B9%E4%B8%BA%E4%BA%86%E8%AE%AD%E7%BB%83%20CLIP%EF%BC%8COpenAI%20%E4%BB%8E%E4%BA%92%E8%81%94%E7%BD%91%E6%94%B6%E9%9B%86%E4%BA%86%E5%85%B1%EF%BC%8C%E8%AE%BA%E6%96%87%E7%A7%B0%E4%B9%8B%E4%B8%BA%20WebImageText%EF%BC%8C%E5%A6%82%E6%9E%9C%E6%8C%89%E7%85%A7%E6%96%87%E6%9C%AC%E7%9A%84%E5%8D%95%E8%AF%8D%E9%87%8F%EF%BC%8C%E5%AE%83%E5%92%8C%E8%AE%AD%E7%BB%83%20GPT-2%20%E7%9A%84%20WebText%20%E8%A7%84%E6%A8%A1%E7%B1%BB%E4%BC%BC%EF%BC%8C%E5%A6%82%E6%9E%9C%E4%BB%8E%E6%95%B0%E9%87%8F%E4%B8%8A%E5%AF%B9%E6%AF%94%E7%9A%84%E8%AF%9D%EF%BC%8C%E5%AE%83%E8%BF%98%E6%AF%94%E8%B0%B7%E6%AD%8C%E7%9A%84%20JFT-300M%20%E6%95%B0%E6%8D%AE%E9%9B%86%E5%A4%9A%E4%B8%80%E4%B8%AA%E4%BA%BF%EF%BC%8C%E6%89%80%E4%BB%A5%E8%AF%B4%E8%BF%99%E6%98%AF%E4%B8%80%E4%B8%AA%E5%BE%88%E5%A4%A7%E8%A7%84%E6%A8%A1%E7%9A%84%E6%95%B0%E6%8D%AE%E9%9B%86%E3%80%82))

- > 它主要是用来**训练可迁移的视觉模型**。论文中 Text Encoder 固定选择一个包含 63M 参数的 text transformer 模型，而 Image Encoder 采用了两种的不同的架构，一是常用的 CNN 架构 ResNet，二是基于 transformer 的 ViT，其中 ResNet 包含 5 个不同大小的模型：**ResNet50**，**ResNet101**，**RN50x4**，**RN50x16** 和 **RNx64**（后面三个模型是按照 EfficientNet 缩放规则对 ResNet 分别增大 4x，16x 和 64x 得到），而 ViT 选择 3 个不同大小的模型：**ViT-B/32**，**ViT-B/16** 和 **ViT-L/14**。  ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/493489688#js_content:~:text=%E8%AE%AD%E7%BB%83%E5%8F%AF%E8%BF%81%E7%A7%BB%E7%9A%84%E8%A7%86%E8%A7%89%E6%A8%A1%E5%9E%8B%E5%AE%83%E4%B8%BB%E8%A6%81%E6%98%AF%E7%94%A8%E6%9D%A5%E3%80%82%E8%AE%BA%E6%96%87%E4%B8%AD%20Text%20Encoder%20%E5%9B%BA%E5%AE%9A%E9%80%89%E6%8B%A9%E4%B8%80%E4%B8%AA%E5%8C%85%E5%90%AB%2063M%20%E5%8F%82%E6%95%B0%E7%9A%84%20text%20transformer%20%E6%A8%A1%E5%9E%8B%EF%BC%8C%E8%80%8C%20Image%20Encoder%20%E9%87%87%E7%94%A8%E4%BA%86%E4%B8%A4%E7%A7%8D%E7%9A%84%E4%B8%8D%E5%90%8C%E7%9A%84%E6%9E%B6%E6%9E%84%EF%BC%8C%E4%B8%80%E6%98%AF%E5%B8%B8%E7%94%A8%E7%9A%84%20CNN%20%E6%9E%B6%E6%9E%84%20ResNet%EF%BC%8C%E4%BA%8C%E6%98%AF%E5%9F%BA%E4%BA%8E%20transformer%20%E7%9A%84%20ViT%EF%BC%8C%E5%85%B6%E4%B8%AD%20ResNet%20%E5%8C%85%E5%90%AB%205%20%E4%B8%AA%E4%B8%8D%E5%90%8C%E5%A4%A7%E5%B0%8F%E7%9A%84%E6%A8%A1%E5%9E%8B%EF%BC%9AResNet50ResNet101RN50x4RN50x16%20RNx64%EF%BC%8C%EF%BC%8C%EF%BC%8C%20%E5%92%8C%EF%BC%88%E5%90%8E%E9%9D%A2%E4%B8%89%E4%B8%AA%E6%A8%A1%E5%9E%8B%E6%98%AF%E6%8C%89%E7%85%A7%20EfficientNet%20%E7%BC%A9%E6%94%BE%E8%A7%84%E5%88%99%E5%AF%B9%20ResNet%20%E5%88%86%E5%88%AB%E5%A2%9E%E5%A4%A7%204x%EF%BC%8C16x%20%E5%92%8C%2064x%20%E5%BE%97%E5%88%B0%EF%BC%89%EF%BC%8C%E8%80%8C%20ViT%20%E9%80%89%E6%8B%A9%203%20%E4%B8%AA%E4%B8%8D%E5%90%8C%E5%A4%A7%E5%B0%8F%E7%9A%84%E6%A8%A1%E5%9E%8B%EF%BC%9AViT-B/32ViT-B/16%20ViT-L/14%EF%BC%8C%20%E5%92%8C%E3%80%82))

- > 所有的模型都训练 32 个 epochs，采用 AdamW 优化器，而且训练过程采用了一个**较大的 batch size：32768**。  ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/493489688#js_content:~:text=%E6%89%80%E6%9C%89%E7%9A%84%E6%A8%A1%E5%9E%8B%E9%83%BD%E8%AE%AD%E7%BB%83%2032%20%E4%B8%AA%20epochs%EF%BC%8C%E9%87%87%E7%94%A8%20AdamW%20%E4%BC%98%E5%8C%96%E5%99%A8%EF%BC%8C%E8%80%8C%E4%B8%94%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B%E9%87%87%E7%94%A8%E4%BA%86%E4%B8%80%E4%B8%AA%E8%BE%83%E5%A4%A7%E7%9A%84%20batch%20size%EF%BC%9A32768%E3%80%82))

- > 对于 ViT-L/14，还在 336 的分辨率下额外 finetune 了一个 epoch 来增强性能，论文发现这个模型效果最好，记为 **ViT-L/14@336**，论文中进行对比实验的 CLIP 模型也采用这个。  ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/493489688#js_content:~:text=%E5%AF%B9%E4%BA%8E%20ViT-L/14%EF%BC%8C%E8%BF%98%E5%9C%A8%20336%20%E7%9A%84%E5%88%86%E8%BE%A8%E7%8E%87%E4%B8%8B%E9%A2%9D%E5%A4%96%20finetune%20%E4%BA%86%E4%B8%80%E4%B8%AA%20epoch%20%E6%9D%A5%E5%A2%9E%E5%BC%BA%E6%80%A7%E8%83%BD%EF%BC%8C%E8%AE%BA%E6%96%87%E5%8F%91%E7%8E%B0%E8%BF%99%E4%B8%AA%E6%A8%A1%E5%9E%8B%E6%95%88%E6%9E%9C%E6%9C%80%E5%A5%BD%EF%BC%8C%E8%AE%B0%E4%B8%BA%20ViT-L/14@336%EF%BC%8C%E8%AE%BA%E6%96%87%E4%B8%AD%E8%BF%9B%E8%A1%8C%E5%AF%B9%E6%AF%94%E5%AE%9E%E9%AA%8C%E7%9A%84%20CLIP%20%E6%A8%A1%E5%9E%8B%E4%B9%9F%E9%87%87%E7%94%A8%E8%BF%99%E4%B8%AA%E3%80%82))

- > **与 CV 中常用的先预训练然后微调不同，CLIP 可以直接实现 zero-shot 的图像分类，即不需要任何训练数据，就能在某个具体下游任务上实现分类，**这也是 CLIP 亮点和强大之处。  ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/493489688#js_content:~:text=%E4%B8%8E%20CV%20%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%E5%85%88%E9%A2%84%E8%AE%AD%E7%BB%83%E7%84%B6%E5%90%8E%E5%BE%AE%E8%B0%83%E4%B8%8D%E5%90%8C%EF%BC%8CCLIP%20%E5%8F%AF%E4%BB%A5%E7%9B%B4%E6%8E%A5%E5%AE%9E%E7%8E%B0%20zero-shot%20%E7%9A%84%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%EF%BC%8C%E5%8D%B3%E4%B8%8D%E9%9C%80%E8%A6%81%E4%BB%BB%E4%BD%95%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE%EF%BC%8C%E5%B0%B1%E8%83%BD%E5%9C%A8%E6%9F%90%E4%B8%AA%E5%85%B7%E4%BD%93%E4%B8%8B%E6%B8%B8%E4%BB%BB%E5%8A%A1%E4%B8%8A%E5%AE%9E%E7%8E%B0%E5%88%86%E7%B1%BB%EF%BC%8C%E8%BF%99%E4%B9%9F%E6%98%AF%20CLIP%20%E4%BA%AE%E7%82%B9%E5%92%8C%E5%BC%BA%E5%A4%A7%E4%B9%8B%E5%A4%84%E3%80%82))

- > 用 CLIP 实现 zero-shot 分类很简单，只需要简单的两步：

1.  根据任务的分类标签构建每个类别的描述文本：`A photo of {label}`，然后将这些文本送入 Text Encoder 得到对应的文本特征，如果类别数目为
    
    ![](https://www.zhihu.com/equation?tex=N)
    
    ，那么将得到
    
    ![](https://www.zhihu.com/equation?tex=N)
    
    个文本特征；
2.  将要预测的图像送入 Image Encoder 得到图像特征，然后与
    
    ![](https://www.zhihu.com/equation?tex=N)
    
    个文本特征计算缩放的余弦相似度（和训练过程一致），然后选择相似度最大的文本对应的类别作为图像分类预测结果，进一步地，可以将这些相似度看成 logits，送入 softmax 后可以到每个类别的预测概率。  ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/493489688#js_content:~:text=%E7%94%A8%20CLIP%20%E5%AE%9E%E7%8E%B0%20zero-shot%20%E5%88%86%E7%B1%BB%E5%BE%88%E7%AE%80%E5%8D%95%EF%BC%8C%E5%8F%AA%E9%9C%80%E8%A6%81%E7%AE%80%E5%8D%95%E7%9A%84%E4%B8%A4%E6%AD%A5%EF%BC%9A%E6%A0%B9%E6%8D%AE%E4%BB%BB%E5%8A%A1%E7%9A%84%E5%88%86%E7%B1%BB%E6%A0%87%E7%AD%BE%E6%9E%84%E5%BB%BA%E6%AF%8F%E4%B8%AA%E7%B1%BB%E5%88%AB%E7%9A%84%E6%8F%8F%E8%BF%B0%E6%96%87%E6%9C%AC%EF%BC%9A%EF%BC%8C%E7%84%B6%E5%90%8E%E5%B0%86%E8%BF%99%E4%BA%9B%E6%96%87%E6%9C%AC%E9%80%81%E5%85%A5%20Text%20Encoder%20%E5%BE%97%E5%88%B0%E5%AF%B9%E5%BA%94%E7%9A%84%E6%96%87%E6%9C%AC%E7%89%B9%E5%BE%81%EF%BC%8C%E5%A6%82%E6%9E%9C%E7%B1%BB%E5%88%AB%E6%95%B0%E7%9B%AE%E4%B8%BAA%20photo%20of%20%7Blabel%7D%EF%BC%8C%E9%82%A3%E4%B9%88%E5%B0%86%E5%BE%97%E5%88%B0%E4%B8%AA%E6%96%87%E6%9C%AC%E7%89%B9%E5%BE%81%EF%BC%9B%E5%B0%86%E8%A6%81%E9%A2%84%E6%B5%8B%E7%9A%84%E5%9B%BE%E5%83%8F%E9%80%81%E5%85%A5%20Image%20Encoder%20%E5%BE%97%E5%88%B0%E5%9B%BE%E5%83%8F%E7%89%B9%E5%BE%81%EF%BC%8C%E7%84%B6%E5%90%8E%E4%B8%8E%E4%B8%AA%E6%96%87%E6%9C%AC%E7%89%B9%E5%BE%81%E8%AE%A1%E7%AE%97%E7%BC%A9%E6%94%BE%E7%9A%84%E4%BD%99%E5%BC%A6%E7%9B%B8%E4%BC%BC%E5%BA%A6%EF%BC%88%E5%92%8C%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B%E4%B8%80%E8%87%B4%EF%BC%89%EF%BC%8C%E7%84%B6%E5%90%8E%E9%80%89%E6%8B%A9%E7%9B%B8%E4%BC%BC%E5%BA%A6%E6%9C%80%E5%A4%A7%E7%9A%84%E6%96%87%E6%9C%AC%E5%AF%B9%E5%BA%94%E7%9A%84%E7%B1%BB%E5%88%AB%E4%BD%9C%E4%B8%BA%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E9%A2%84%E6%B5%8B%E7%BB%93%E6%9E%9C%EF%BC%8C%E8%BF%9B%E4%B8%80%E6%AD%A5%E5%9C%B0%EF%BC%8C%E5%8F%AF%E4%BB%A5%E5%B0%86%E8%BF%99%E4%BA%9B%E7%9B%B8%E4%BC%BC%E5%BA%A6%E7%9C%8B%E6%88%90%20logits%EF%BC%8C%E9%80%81%E5%85%A5%20softmax%20%E5%90%8E%E5%8F%AF%E4%BB%A5%E5%88%B0%E6%AF%8F%E4%B8%AA%E7%B1%BB%E5%88%AB%E7%9A%84%E9%A2%84%E6%B5%8B%E6%A6%82%E7%8E%87%E3%80%82))

- > ![](https://pic4.zhimg.com/v2-acd4b008007ca7de78bdab1c9042bbcb_r.jpg) [[系统流程]]  [[clip]]   ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/493489688#js_content:~:text=https://pic4.zhimg.com/v2-acd4b008007ca7de78bdab1c9042bbcb_r.jpg))

- > 可以看到，我们是利用 CLIP 的多模态特性为具体的任务**构建了动态的分类器**，**其中 Text Encoder 提取的文本特征可以看成分类器的 weights，而 Image Encoder 提取的图像特征是分类器的输入**。  ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/493489688#js_content:~:text=%E6%9E%84%E5%BB%BA%E4%BA%86%E5%8A%A8%E6%80%81%E7%9A%84%E5%88%86%E7%B1%BB%E5%99%A8%E5%8F%AF%E4%BB%A5%E7%9C%8B%E5%88%B0%EF%BC%8C%E6%88%91%E4%BB%AC%E6%98%AF%E5%88%A9%E7%94%A8%20CLIP%20%E7%9A%84%E5%A4%9A%E6%A8%A1%E6%80%81%E7%89%B9%E6%80%A7%E4%B8%BA%E5%85%B7%E4%BD%93%E7%9A%84%E4%BB%BB%E5%8A%A1%E5%85%B6%E4%B8%AD%20Text%20Encoder%20%E6%8F%90%E5%8F%96%E7%9A%84%E6%96%87%E6%9C%AC%E7%89%B9%E5%BE%81%E5%8F%AF%E4%BB%A5%E7%9C%8B%E6%88%90%E5%88%86%E7%B1%BB%E5%99%A8%E7%9A%84%20weights%EF%BC%8C%E8%80%8C%20Image%20Encoder%20%E6%8F%90%E5%8F%96%E7%9A%84%E5%9B%BE%E5%83%8F%E7%89%B9%E5%BE%81%E6%98%AF%E5%88%86%E7%B1%BB%E5%99%A8%E7%9A%84%E8%BE%93%E5%85%A5%EF%BC%8C%E3%80%82))

- > 简单来说，prompt learning 的核心是通过构建合适 prompt（提示）来使预训练模型能够直接应用到下游任务，这和之前的预训练 + 微调属于不同的范式。  ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/493489688#js_content:~:text=%E7%AE%80%E5%8D%95%E6%9D%A5%E8%AF%B4%EF%BC%8Cprompt%20learning%20%E7%9A%84%E6%A0%B8%E5%BF%83%E6%98%AF%E9%80%9A%E8%BF%87%E6%9E%84%E5%BB%BA%E5%90%88%E9%80%82%20prompt%EF%BC%88%E6%8F%90%E7%A4%BA%EF%BC%89%E6%9D%A5%E4%BD%BF%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E8%83%BD%E5%A4%9F%E7%9B%B4%E6%8E%A5%E5%BA%94%E7%94%A8%E5%88%B0%E4%B8%8B%E6%B8%B8%E4%BB%BB%E5%8A%A1%EF%BC%8C%E8%BF%99%E5%92%8C%E4%B9%8B%E5%89%8D%E7%9A%84%E9%A2%84%E8%AE%AD%E7%BB%83%20+%20%E5%BE%AE%E8%B0%83%E5%B1%9E%E4%BA%8E%E4%B8%8D%E5%90%8C%E7%9A%84%E8%8C%83%E5%BC%8F%E3%80%82))

- > ![](https://pic2.zhimg.com/v2-cbebe6dfb596bf0c93a4c22c441f17cd_r.jpg)  ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/493489688#js_content:~:text=https://pic2.zhimg.com/v2-cbebe6dfb596bf0c93a4c22c441f17cd_r.jpg))
  - 📝 prompt engineering 对 clip 的提升

- > CLIP 模型在效果上远远超过之前的模型，**其中在 ImageNet 数据集可以达到 76.2，这和全监督的 ResNet50 效果相当**  ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/493489688#js_content:~:text=CLIP%20%E6%A8%A1%E5%9E%8B%E5%9C%A8%E6%95%88%E6%9E%9C%E4%B8%8A%E8%BF%9C%E8%BF%9C%E8%B6%85%E8%BF%87%E4%B9%8B%E5%89%8D%E7%9A%84%E6%A8%A1%E5%9E%8B%EF%BC%8C%E5%85%B6%E4%B8%AD%E5%9C%A8%20ImageNet%20%E6%95%B0%E6%8D%AE%E9%9B%86%E5%8F%AF%E4%BB%A5%E8%BE%BE%E5%88%B0%2076.2%EF%BC%8C%E8%BF%99%E5%92%8C%E5%85%A8%E7%9B%91%E7%9D%A3%E7%9A%84%20ResNet50%20%E6%95%88%E6%9E%9C%E7%9B%B8%E5%BD%93))

- > 更进一步地，论文还对比了 zero-shot CLIP 和 ResNet50 linear probing（ImageNet 数据上预训练，在加上线性分类层进行 finetune）在 27 个数据集上表现，如下图所示，其中在 16 个数据集上 CLIP 可以超过 ResNet50。但是**在一些特别的，复杂的或者抽象的数据集上 CLIP 表现较差**  ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/493489688#js_content:~:text=%E6%9B%B4%E8%BF%9B%E4%B8%80%E6%AD%A5%E5%9C%B0%EF%BC%8C%E8%AE%BA%E6%96%87%E8%BF%98%E5%AF%B9%E6%AF%94%E4%BA%86%20zero-shot%20CLIP%20%E5%92%8C%20ResNet50%20linear%20probing%EF%BC%88ImageNet%20%E6%95%B0%E6%8D%AE%E4%B8%8A%E9%A2%84%E8%AE%AD%E7%BB%83%EF%BC%8C%E5%9C%A8%E5%8A%A0%E4%B8%8A%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB%E5%B1%82%E8%BF%9B%E8%A1%8C%20finetune%EF%BC%89%E5%9C%A8%2027%20%E4%B8%AA%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%8A%E8%A1%A8%E7%8E%B0%EF%BC%8C%E5%A6%82%E4%B8%8B%E5%9B%BE%E6%89%80%E7%A4%BA%EF%BC%8C%E5%85%B6%E4%B8%AD%E5%9C%A8%2016%20%E4%B8%AA%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%8A%20CLIP%20%E5%8F%AF%E4%BB%A5%E8%B6%85%E8%BF%87%20ResNet50%E3%80%82%E4%BD%86%E6%98%AF%E5%9C%A8%E4%B8%80%E4%BA%9B%E7%89%B9%E5%88%AB%E7%9A%84%EF%BC%8C%E5%A4%8D%E6%9D%82%E7%9A%84%E6%88%96%E8%80%85%E6%8A%BD%E8%B1%A1%E7%9A%84%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%8A%20CLIP%20%E8%A1%A8%E7%8E%B0%E8%BE%83%E5%B7%AE))

- > CLIP 表现较差的竟然还有 MNIST 数据集，分类准确度只有 88%，这是不可思议的，因为这个任务太简单了，通过对 CLIP 训练数据进行分析，作者发现 4 亿的训练数据中基本上没有和 MNIST 比较相似的数据，所以这对 CLIP 来说就属于**域外数据**了，表现较差就比较容易理解了。这也表明：**CLIP 依然无法解决域外泛化这个深度学习难题。**  ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/493489688#js_content:~:text=CLIP%20%E8%A1%A8%E7%8E%B0%E8%BE%83%E5%B7%AE%E7%9A%84%E7%AB%9F%E7%84%B6%E8%BF%98%E6%9C%89%20MNIST%20%E6%95%B0%E6%8D%AE%E9%9B%86%EF%BC%8C%E5%88%86%E7%B1%BB%E5%87%86%E7%A1%AE%E5%BA%A6%E5%8F%AA%E6%9C%89%2088%25%EF%BC%8C%E8%BF%99%E6%98%AF%E4%B8%8D%E5%8F%AF%E6%80%9D%E8%AE%AE%E7%9A%84%EF%BC%8C%E5%9B%A0%E4%B8%BA%E8%BF%99%E4%B8%AA%E4%BB%BB%E5%8A%A1%E5%A4%AA%E7%AE%80%E5%8D%95%E4%BA%86%EF%BC%8C%E9%80%9A%E8%BF%87%E5%AF%B9%20CLIP%20%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE%E8%BF%9B%E8%A1%8C%E5%88%86%E6%9E%90%EF%BC%8C%E4%BD%9C%E8%80%85%E5%8F%91%E7%8E%B0%204%20%E4%BA%BF%E7%9A%84%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE%E4%B8%AD%E5%9F%BA%E6%9C%AC%E4%B8%8A%E6%B2%A1%E6%9C%89%E5%92%8C%20MNIST%20%E6%AF%94%E8%BE%83%E7%9B%B8%E4%BC%BC%E7%9A%84%E6%95%B0%E6%8D%AE%EF%BC%8C%E6%89%80%E4%BB%A5%E8%BF%99%E5%AF%B9%20CLIP%20%E6%9D%A5%E8%AF%B4%E5%B0%B1%E5%B1%9E%E4%BA%8E%E5%9F%9F%E5%A4%96%E6%95%B0%E6%8D%AE%E4%BA%86%EF%BC%8C%E8%A1%A8%E7%8E%B0%E8%BE%83%E5%B7%AE%E5%B0%B1%E6%AF%94%E8%BE%83%E5%AE%B9%E6%98%93%E7%90%86%E8%A7%A3%E4%BA%86%E3%80%82%E8%BF%99%E4%B9%9F%E8%A1%A8%E6%98%8E%EF%BC%9ACLIP%20%E4%BE%9D%E7%84%B6%E6%97%A0%E6%B3%95%E8%A7%A3%E5%86%B3%E5%9F%9F%E5%A4%96%E6%B3%9B%E5%8C%96%E8%BF%99%E4%B8%AA%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E9%9A%BE%E9%A2%98%E3%80%82))

- > ![](https://pic1.zhimg.com/v2-dc8ba977117fdec5fd2f79961d7110d4_r.jpg)  ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/493489688#js_content:~:text=https://pic1.zhimg.com/v2-dc8ba977117fdec5fd2f79961d7110d4_r.jpg))
  - 📝 clip zero shot vs 全量训练的 resnet50

- > 虽然 CLIP 在 few-shot 实验中随着样本量增加性能有提升，但是 1-shot 和 2-shot 性能比 zero-shot 还差，这个作者认为主要是 CLIP 的训练和常规的有监督训练存在一定的差异造成的。  ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/493489688#js_content:~:text=%E8%99%BD%E7%84%B6%20CLIP%20%E5%9C%A8%20few-shot%20%E5%AE%9E%E9%AA%8C%E4%B8%AD%E9%9A%8F%E7%9D%80%E6%A0%B7%E6%9C%AC%E9%87%8F%E5%A2%9E%E5%8A%A0%E6%80%A7%E8%83%BD%E6%9C%89%E6%8F%90%E5%8D%87%EF%BC%8C%E4%BD%86%E6%98%AF%201-shot%20%E5%92%8C%202-shot%20%E6%80%A7%E8%83%BD%E6%AF%94%20zero-shot%20%E8%BF%98%E5%B7%AE%EF%BC%8C%E8%BF%99%E4%B8%AA%E4%BD%9C%E8%80%85%E8%AE%A4%E4%B8%BA%E4%B8%BB%E8%A6%81%E6%98%AF%20CLIP%20%E7%9A%84%E8%AE%AD%E7%BB%83%E5%92%8C%E5%B8%B8%E8%A7%84%E7%9A%84%E6%9C%89%E7%9B%91%E7%9D%A3%E8%AE%AD%E7%BB%83%E5%AD%98%E5%9C%A8%E4%B8%80%E5%AE%9A%E7%9A%84%E5%B7%AE%E5%BC%82%E9%80%A0%E6%88%90%E7%9A%84%E3%80%82))

- > 论文还进行了**表征学习**（**representation Learning**）实验，即自监督学习中常用的 **linear probe**：用训练好的模型先提取特征，然后用一个线性分类器来有监督训练。下图为不同模型在 27 个数据集上的 average linear probe score 对比，可以看到 CLIP 模型在性能上超过其它模型，而且计算更高效 [[clip]]   ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/493489688#js_content:~:text=%E8%A1%A8%E5%BE%81%E5%AD%A6%E4%B9%A0representation%20Learning%E8%AE%BA%E6%96%87%E8%BF%98%E8%BF%9B%E8%A1%8C%E4%BA%86%EF%BC%88%EF%BC%89%E5%AE%9E%E9%AA%8C%EF%BC%8C%E5%8D%B3%E8%87%AA%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E4%B8%AD%E5%B8%B8%E7%94%A8%E7%9A%84%20linear%20probe%EF%BC%9A%E7%94%A8%E8%AE%AD%E7%BB%83%E5%A5%BD%E7%9A%84%E6%A8%A1%E5%9E%8B%E5%85%88%E6%8F%90%E5%8F%96%E7%89%B9%E5%BE%81%EF%BC%8C%E7%84%B6%E5%90%8E%E7%94%A8%E4%B8%80%E4%B8%AA%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB%E5%99%A8%E6%9D%A5%E6%9C%89%E7%9B%91%E7%9D%A3%E8%AE%AD%E7%BB%83%E3%80%82%E4%B8%8B%E5%9B%BE%E4%B8%BA%E4%B8%8D%E5%90%8C%E6%A8%A1%E5%9E%8B%E5%9C%A8%2027%20%E4%B8%AA%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%8A%E7%9A%84%20average%20linear%20probe%20score%20%E5%AF%B9%E6%AF%94%EF%BC%8C%E5%8F%AF%E4%BB%A5%E7%9C%8B%E5%88%B0%20CLIP%20%E6%A8%A1%E5%9E%8B%E5%9C%A8%E6%80%A7%E8%83%BD%E4%B8%8A%E8%B6%85%E8%BF%87%E5%85%B6%E5%AE%83%E6%A8%A1%E5%9E%8B%EF%BC%8C%E8%80%8C%E4%B8%94%E8%AE%A1%E7%AE%97%E6%9B%B4%E9%AB%98%E6%95%88))
  - 📝 可以用作特征提取器，用于下游任务

- > ![](https://pic1.zhimg.com/v2-42d05e933b46aebf16ba11e2cfce6154_r.jpg)  ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/493489688#js_content:~:text=https://pic1.zhimg.com/v2-42d05e933b46aebf16ba11e2cfce6154_r.jpg))

- > 论文还发现 CLIP 在自然分布漂移上表现更鲁棒，比如 CLIP 和基于 ImageNet 上有监督训练的 ResNet101 在 ImageNet 验证集都能达到 76.2%，但是在 ImageNetV2 数据集上，CLIP 要超过 ResNet101。在另外的 4 个分布漂移的数据集上，ResNet101 性能下降得比较厉害，但是 CLIP 能依然保持较大的准确度，比如在 ImageNet-A 数据集上，ResNet101 性能只有 2.7%，而 CLIP 能达到 77.1%。  ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/493489688#js_content:~:text=%E8%AE%BA%E6%96%87%E8%BF%98%E5%8F%91%E7%8E%B0%20CLIP%20%E5%9C%A8%E8%87%AA%E7%84%B6%E5%88%86%E5%B8%83%E6%BC%82%E7%A7%BB%E4%B8%8A%E8%A1%A8%E7%8E%B0%E6%9B%B4%E9%B2%81%E6%A3%92%EF%BC%8C%E6%AF%94%E5%A6%82%20CLIP%20%E5%92%8C%E5%9F%BA%E4%BA%8E%20ImageNet%20%E4%B8%8A%E6%9C%89%E7%9B%91%E7%9D%A3%E8%AE%AD%E7%BB%83%E7%9A%84%20ResNet101%20%E5%9C%A8%20ImageNet%20%E9%AA%8C%E8%AF%81%E9%9B%86%E9%83%BD%E8%83%BD%E8%BE%BE%E5%88%B0%2076.2%25%EF%BC%8C%E4%BD%86%E6%98%AF%E5%9C%A8%20ImageNetV2%20%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%8A%EF%BC%8CCLIP%20%E8%A6%81%E8%B6%85%E8%BF%87%20ResNet101%E3%80%82%E5%9C%A8%E5%8F%A6%E5%A4%96%E7%9A%84%204%20%E4%B8%AA%E5%88%86%E5%B8%83%E6%BC%82%E7%A7%BB%E7%9A%84%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%8A%EF%BC%8CResNet101%20%E6%80%A7%E8%83%BD%E4%B8%8B%E9%99%8D%E5%BE%97%E6%AF%94%E8%BE%83%E5%8E%89%E5%AE%B3%EF%BC%8C%E4%BD%86%E6%98%AF%20CLIP%20%E8%83%BD%E4%BE%9D%E7%84%B6%E4%BF%9D%E6%8C%81%E8%BE%83%E5%A4%A7%E7%9A%84%E5%87%86%E7%A1%AE%E5%BA%A6%EF%BC%8C%E6%AF%94%E5%A6%82%E5%9C%A8%20ImageNet-A%20%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%8A%EF%BC%8CResNet101%20%E6%80%A7%E8%83%BD%E5%8F%AA%E6%9C%89%202.7%25%EF%BC%8C%E8%80%8C%20CLIP%20%E8%83%BD%E8%BE%BE%E5%88%B0%2077.1%25%E3%80%82))
  - 📝 可迁移能力更强

- > CLIP 能实现这么好的 zero-shot 性能，大家很可能质疑 CLIP 的训练数据集可能包含一些测试数据集中的样例，即所谓的数据泄漏。关于这点，论文也采用一个重复检测器对评测的数据集重合做了检查，发现重合率的中位数为 2.2%，而平均值在 3.2%，去重前后大部分数据集的性能没有太大的变化  ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/493489688#js_content:~:text=CLIP%20%E8%83%BD%E5%AE%9E%E7%8E%B0%E8%BF%99%E4%B9%88%E5%A5%BD%E7%9A%84%20zero-shot%20%E6%80%A7%E8%83%BD%EF%BC%8C%E5%A4%A7%E5%AE%B6%E5%BE%88%E5%8F%AF%E8%83%BD%E8%B4%A8%E7%96%91%20CLIP%20%E7%9A%84%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE%E9%9B%86%E5%8F%AF%E8%83%BD%E5%8C%85%E5%90%AB%E4%B8%80%E4%BA%9B%E6%B5%8B%E8%AF%95%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%AD%E7%9A%84%E6%A0%B7%E4%BE%8B%EF%BC%8C%E5%8D%B3%E6%89%80%E8%B0%93%E7%9A%84%E6%95%B0%E6%8D%AE%E6%B3%84%E6%BC%8F%E3%80%82%E5%85%B3%E4%BA%8E%E8%BF%99%E7%82%B9%EF%BC%8C%E8%AE%BA%E6%96%87%E4%B9%9F%E9%87%87%E7%94%A8%E4%B8%80%E4%B8%AA%E9%87%8D%E5%A4%8D%E6%A3%80%E6%B5%8B%E5%99%A8%E5%AF%B9%E8%AF%84%E6%B5%8B%E7%9A%84%E6%95%B0%E6%8D%AE%E9%9B%86%E9%87%8D%E5%90%88%E5%81%9A%E4%BA%86%E6%A3%80%E6%9F%A5%EF%BC%8C%E5%8F%91%E7%8E%B0%E9%87%8D%E5%90%88%E7%8E%87%E7%9A%84%E4%B8%AD%E4%BD%8D%E6%95%B0%E4%B8%BA%202.2%25%EF%BC%8C%E8%80%8C%E5%B9%B3%E5%9D%87%E5%80%BC%E5%9C%A8%203.2%25%EF%BC%8C%E5%8E%BB%E9%87%8D%E5%89%8D%E5%90%8E%E5%A4%A7%E9%83%A8%E5%88%86%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9A%84%E6%80%A7%E8%83%BD%E6%B2%A1%E6%9C%89%E5%A4%AA%E5%A4%A7%E7%9A%84%E5%8F%98%E5%8C%96))

- > CLIP 在自然分布漂移上表现鲁棒，但是依然存在域外泛化问题，即如果测试数据集的分布和训练集相差较大，CLIP 会表现较差；  ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/493489688#js_content:~:text=CLIP%20%E5%9C%A8%E8%87%AA%E7%84%B6%E5%88%86%E5%B8%83%E6%BC%82%E7%A7%BB%E4%B8%8A%E8%A1%A8%E7%8E%B0%E9%B2%81%E6%A3%92%EF%BC%8C%E4%BD%86%E6%98%AF%E4%BE%9D%E7%84%B6%E5%AD%98%E5%9C%A8%E5%9F%9F%E5%A4%96%E6%B3%9B%E5%8C%96%E9%97%AE%E9%A2%98%EF%BC%8C%E5%8D%B3%E5%A6%82%E6%9E%9C%E6%B5%8B%E8%AF%95%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9A%84%E5%88%86%E5%B8%83%E5%92%8C%E8%AE%AD%E7%BB%83%E9%9B%86%E7%9B%B8%E5%B7%AE%E8%BE%83%E5%A4%A7%EF%BC%8CCLIP%20%E4%BC%9A%E8%A1%A8%E7%8E%B0%E8%BE%83%E5%B7%AE%EF%BC%9B))

- > 无论是有监督还是自监督方法，它们在迁移到下游任务时，还是需要进行有监督微调，而无法实现 zero-shot。对于有监督模型，由于它们在预训练数据集上采用固定类别数的分类器，所以在新的数据集上需要定义新的分类器来重新训练。对于自监督模型，代理任务往往是辅助来进行表征学习，在迁移到其它数据集时也需要加上新的分类器来进行有监督训练。  ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/493489688#js_content:~:text=%E6%97%A0%E8%AE%BA%E6%98%AF%E6%9C%89%E7%9B%91%E7%9D%A3%E8%BF%98%E6%98%AF%E8%87%AA%E7%9B%91%E7%9D%A3%E6%96%B9%E6%B3%95%EF%BC%8C%E5%AE%83%E4%BB%AC%E5%9C%A8%E8%BF%81%E7%A7%BB%E5%88%B0%E4%B8%8B%E6%B8%B8%E4%BB%BB%E5%8A%A1%E6%97%B6%EF%BC%8C%E8%BF%98%E6%98%AF%E9%9C%80%E8%A6%81%E8%BF%9B%E8%A1%8C%E6%9C%89%E7%9B%91%E7%9D%A3%E5%BE%AE%E8%B0%83%EF%BC%8C%E8%80%8C%E6%97%A0%E6%B3%95%E5%AE%9E%E7%8E%B0%20zero-shot%E3%80%82%E5%AF%B9%E4%BA%8E%E6%9C%89%E7%9B%91%E7%9D%A3%E6%A8%A1%E5%9E%8B%EF%BC%8C%E7%94%B1%E4%BA%8E%E5%AE%83%E4%BB%AC%E5%9C%A8%E9%A2%84%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%8A%E9%87%87%E7%94%A8%E5%9B%BA%E5%AE%9A%E7%B1%BB%E5%88%AB%E6%95%B0%E7%9A%84%E5%88%86%E7%B1%BB%E5%99%A8%EF%BC%8C%E6%89%80%E4%BB%A5%E5%9C%A8%E6%96%B0%E7%9A%84%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%8A%E9%9C%80%E8%A6%81%E5%AE%9A%E4%B9%89%E6%96%B0%E7%9A%84%E5%88%86%E7%B1%BB%E5%99%A8%E6%9D%A5%E9%87%8D%E6%96%B0%E8%AE%AD%E7%BB%83%E3%80%82%E5%AF%B9%E4%BA%8E%E8%87%AA%E7%9B%91%E7%9D%A3%E6%A8%A1%E5%9E%8B%EF%BC%8C%E4%BB%A3%E7%90%86%E4%BB%BB%E5%8A%A1%E5%BE%80%E5%BE%80%E6%98%AF%E8%BE%85%E5%8A%A9%E6%9D%A5%E8%BF%9B%E8%A1%8C%E8%A1%A8%E5%BE%81%E5%AD%A6%E4%B9%A0%EF%BC%8C%E5%9C%A8%E8%BF%81%E7%A7%BB%E5%88%B0%E5%85%B6%E5%AE%83%E6%95%B0%E6%8D%AE%E9%9B%86%E6%97%B6%E4%B9%9F%E9%9C%80%E8%A6%81%E5%8A%A0%E4%B8%8A%E6%96%B0%E7%9A%84%E5%88%86%E7%B1%BB%E5%99%A8%E6%9D%A5%E8%BF%9B%E8%A1%8C%E6%9C%89%E7%9B%91%E7%9D%A3%E8%AE%AD%E7%BB%83%E3%80%82))

- > NLP 领域，基于自回归或者语言掩码的预训练方法已经取得相对成熟，而且预训练模型很容易直接 zero-shot 迁移到下游任务，比如 OpenAI 的 GPT-3。这种差异一方面是由于文本和图像属于两个完全不同的模态，另外一个原因就是 NLP 模型可以采用从互联网上收集的大量文本。  ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/493489688#js_content:~:text=NLP%20%E9%A2%86%E5%9F%9F%EF%BC%8C%E5%9F%BA%E4%BA%8E%E8%87%AA%E5%9B%9E%E5%BD%92%E6%88%96%E8%80%85%E8%AF%AD%E8%A8%80%E6%8E%A9%E7%A0%81%E7%9A%84%E9%A2%84%E8%AE%AD%E7%BB%83%E6%96%B9%E6%B3%95%E5%B7%B2%E7%BB%8F%E5%8F%96%E5%BE%97%E7%9B%B8%E5%AF%B9%E6%88%90%E7%86%9F%EF%BC%8C%E8%80%8C%E4%B8%94%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E5%BE%88%E5%AE%B9%E6%98%93%E7%9B%B4%E6%8E%A5%20zero-shot%20%E8%BF%81%E7%A7%BB%E5%88%B0%E4%B8%8B%E6%B8%B8%E4%BB%BB%E5%8A%A1%EF%BC%8C%E6%AF%94%E5%A6%82%20OpenAI%20%E7%9A%84%20GPT-3%E3%80%82%E8%BF%99%E7%A7%8D%E5%B7%AE%E5%BC%82%E4%B8%80%E6%96%B9%E9%9D%A2%E6%98%AF%E7%94%B1%E4%BA%8E%E6%96%87%E6%9C%AC%E5%92%8C%E5%9B%BE%E5%83%8F%E5%B1%9E%E4%BA%8E%E4%B8%A4%E4%B8%AA%E5%AE%8C%E5%85%A8%E4%B8%8D%E5%90%8C%E7%9A%84%E6%A8%A1%E6%80%81%EF%BC%8C%E5%8F%A6%E5%A4%96%E4%B8%80%E4%B8%AA%E5%8E%9F%E5%9B%A0%E5%B0%B1%E6%98%AF%20NLP%20%E6%A8%A1%E5%9E%8B%E5%8F%AF%E4%BB%A5%E9%87%87%E7%94%A8%E4%BB%8E%E4%BA%92%E8%81%94%E7%BD%91%E4%B8%8A%E6%94%B6%E9%9B%86%E7%9A%84%E5%A4%A7%E9%87%8F%E6%96%87%E6%9C%AC%E3%80%82))

- > 虽然谷歌基于 JFT-300M 数据集取得了较好的结果，但是这些模型依然采用固定类别的 softmax 分类器进行预训练，这大大限制了它的迁移能力和扩展性。  ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/493489688#js_content:~:text=%E8%99%BD%E7%84%B6%E8%B0%B7%E6%AD%8C%E5%9F%BA%E4%BA%8E%20JFT-300M%20%E6%95%B0%E6%8D%AE%E9%9B%86%E5%8F%96%E5%BE%97%E4%BA%86%E8%BE%83%E5%A5%BD%E7%9A%84%E7%BB%93%E6%9E%9C%EF%BC%8C%E4%BD%86%E6%98%AF%E8%BF%99%E4%BA%9B%E6%A8%A1%E5%9E%8B%E4%BE%9D%E7%84%B6%E9%87%87%E7%94%A8%E5%9B%BA%E5%AE%9A%E7%B1%BB%E5%88%AB%E7%9A%84%20softmax%20%E5%88%86%E7%B1%BB%E5%99%A8%E8%BF%9B%E8%A1%8C%E9%A2%84%E8%AE%AD%E7%BB%83%EF%BC%8C%E8%BF%99%E5%A4%A7%E5%A4%A7%E9%99%90%E5%88%B6%E4%BA%86%E5%AE%83%E7%9A%84%E8%BF%81%E7%A7%BB%E8%83%BD%E5%8A%9B%E5%92%8C%E6%89%A9%E5%B1%95%E6%80%A7%E3%80%82))

- > **作者认为谷歌的弱监督方法和之前的方法的一个重要的区别在于规模，或者说算力和数据的规模不同**。JFT-300M 数据量达到了上亿级别，而且谷歌用了强大的算力来进行预训练。而 VirTex，ICMLM 和 ConVIRT 只在 10 万级别的数据上训练了几天。  ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/493489688#js_content:~:text=%E4%BD%9C%E8%80%85%E8%AE%A4%E4%B8%BA%E8%B0%B7%E6%AD%8C%E7%9A%84%E5%BC%B1%E7%9B%91%E7%9D%A3%E6%96%B9%E6%B3%95%E5%92%8C%E4%B9%8B%E5%89%8D%E7%9A%84%E6%96%B9%E6%B3%95%E7%9A%84%E4%B8%80%E4%B8%AA%E9%87%8D%E8%A6%81%E7%9A%84%E5%8C%BA%E5%88%AB%E5%9C%A8%E4%BA%8E%E8%A7%84%E6%A8%A1%EF%BC%8C%E6%88%96%E8%80%85%E8%AF%B4%E7%AE%97%E5%8A%9B%E5%92%8C%E6%95%B0%E6%8D%AE%E7%9A%84%E8%A7%84%E6%A8%A1%E4%B8%8D%E5%90%8C%E3%80%82JFT-300M%20%E6%95%B0%E6%8D%AE%E9%87%8F%E8%BE%BE%E5%88%B0%E4%BA%86%E4%B8%8A%E4%BA%BF%E7%BA%A7%E5%88%AB%EF%BC%8C%E8%80%8C%E4%B8%94%E8%B0%B7%E6%AD%8C%E7%94%A8%E4%BA%86%E5%BC%BA%E5%A4%A7%E7%9A%84%E7%AE%97%E5%8A%9B%E6%9D%A5%E8%BF%9B%E8%A1%8C%E9%A2%84%E8%AE%AD%E7%BB%83%E3%80%82%E8%80%8C%20VirTex%EF%BC%8CICMLM%20%E5%92%8C%20ConVIRT%20%E5%8F%AA%E5%9C%A8%2010%20%E4%B8%87%E7%BA%A7%E5%88%AB%E7%9A%84%E6%95%B0%E6%8D%AE%E4%B8%8A%E8%AE%AD%E7%BB%83%E4%BA%86%E5%87%A0%E5%A4%A9%E3%80%82))

- > **从本质上来讲，CLIP 其实并没有太大的创新，它只是将** [ConVIRT](https://arxiv.org/abs/2010.00747) **方法进行简化，并采用更大规模的文本 - 图像对数据集来训练。**  ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/493489688#js_content:~:text=%E4%BB%8E%E6%9C%AC%E8%B4%A8%E4%B8%8A%E6%9D%A5%E8%AE%B2%EF%BC%8CCLIP%20%E5%85%B6%E5%AE%9E%E5%B9%B6%E6%B2%A1%E6%9C%89%E5%A4%AA%E5%A4%A7%E7%9A%84%E5%88%9B%E6%96%B0%EF%BC%8C%E5%AE%83%E5%8F%AA%E6%98%AF%E5%B0%86%20%20%E6%96%B9%E6%B3%95%E8%BF%9B%E8%A1%8C%E7%AE%80%E5%8C%96%EF%BC%8C%E5%B9%B6%E9%87%87%E7%94%A8%E6%9B%B4%E5%A4%A7%E8%A7%84%E6%A8%A1%E7%9A%84%E6%96%87%E6%9C%AC%20-%20%E5%9B%BE%E5%83%8F%E5%AF%B9%E6%95%B0%E6%8D%AE%E9%9B%86%E6%9D%A5%E8%AE%AD%E7%BB%83%E3%80%82ConVIRT))

- > 基于文本来搜索图像是 CLIP 最能直接实现的一个应用，其实 CLIP 也是作为 DALL-E 的排序模型，即从生成的图像中选择和文本相关性较高的。  ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/493489688#js_content:~:text=%E5%9F%BA%E4%BA%8E%E6%96%87%E6%9C%AC%E6%9D%A5%E6%90%9C%E7%B4%A2%E5%9B%BE%E5%83%8F%E6%98%AF%20CLIP%20%E6%9C%80%E8%83%BD%E7%9B%B4%E6%8E%A5%E5%AE%9E%E7%8E%B0%E7%9A%84%E4%B8%80%E4%B8%AA%E5%BA%94%E7%94%A8%EF%BC%8C%E5%85%B6%E5%AE%9E%20CLIP%20%E4%B9%9F%E6%98%AF%E4%BD%9C%E4%B8%BA%20DALL-E%20%E7%9A%84%E6%8E%92%E5%BA%8F%E6%A8%A1%E5%9E%8B%EF%BC%8C%E5%8D%B3%E4%BB%8E%E7%94%9F%E6%88%90%E7%9A%84%E5%9B%BE%E5%83%8F%E4%B8%AD%E9%80%89%E6%8B%A9%E5%92%8C%E6%96%87%E6%9C%AC%E7%9B%B8%E5%85%B3%E6%80%A7%E8%BE%83%E9%AB%98%E7%9A%84%E3%80%82))

- > CLIP 是基于文本 - 图像对来做的，但是它可以扩展到文本 - 视频，比如 [VideoCLIP](https://arxiv.org/abs/2109.14084) 就是将 CLIP 应用在视频领域来实现一些 zero-shot 视频理解任务。  ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/493489688#js_content:~:text=CLIP%20%E6%98%AF%E5%9F%BA%E4%BA%8E%E6%96%87%E6%9C%AC%20-%20%E5%9B%BE%E5%83%8F%E5%AF%B9%E6%9D%A5%E5%81%9A%E7%9A%84%EF%BC%8C%E4%BD%86%E6%98%AF%E5%AE%83%E5%8F%AF%E4%BB%A5%E6%89%A9%E5%B1%95%E5%88%B0%E6%96%87%E6%9C%AC%20-%20%E8%A7%86%E9%A2%91%EF%BC%8C%E6%AF%94%E5%A6%82%20VideoCLIP%20%E5%B0%B1%E6%98%AF%E5%B0%86%20CLIP%20%E5%BA%94%E7%94%A8%E5%9C%A8%E8%A7%86%E9%A2%91%E9%A2%86%E5%9F%9F%E6%9D%A5%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%BA%9B%20zero-shot%20%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3%E4%BB%BB%E5%8A%A1%E3%80%82))

