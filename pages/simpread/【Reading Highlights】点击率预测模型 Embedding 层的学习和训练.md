title:: 【Reading Highlights】点击率预测模型 Embedding 层的学习和训练
source:: https://zhuanlan.zhihu.com/p/509188349
tags:: [[简悦]] [[embedding]]  [[华为技术]]   [[reading_highlights]]
date:: 20220519  

- > ![](https://pic1.zhimg.com/v2-1b972b16334617ccc85a0c6e36539d9c_r.jpg)  ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/509188349#js_content:~:text=https://pic1.zhimg.com/v2-1b972b16334617ccc85a0c6e36539d9c_r.jpg))

- > ![](https://pic4.zhimg.com/v2-015a9245728ff37e8932d1f08a549dcf_r.jpg)  ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/509188349#js_content:~:text=https://pic4.zhimg.com/v2-015a9245728ff37e8932d1f08a549dcf_r.jpg))

- > 在 2021 年 IJCAI 上面有这样一篇 Survey 论文，是上海交通大学张伟楠老师和华为诺亚实验实的联合工作，将深度学习时代的点击率预测模型分为了三类：

*   第一类就是基于组合特征挖掘的模型；
*   第二类针对用户行为的模型；
*   第三类是自动架构搜索的模型。  ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/509188349#js_content:~:text=%E5%9C%A8%202021%20%E5%B9%B4%20IJCAI%20%E4%B8%8A%E9%9D%A2%E6%9C%89%E8%BF%99%E6%A0%B7%E4%B8%80%E7%AF%87%20Survey%20%E8%AE%BA%E6%96%87%EF%BC%8C%E6%98%AF%E4%B8%8A%E6%B5%B7%E4%BA%A4%E9%80%9A%E5%A4%A7%E5%AD%A6%E5%BC%A0%E4%BC%9F%E6%A5%A0%E8%80%81%E5%B8%88%E5%92%8C%E5%8D%8E%E4%B8%BA%E8%AF%BA%E4%BA%9A%E5%AE%9E%E9%AA%8C%E5%AE%9E%E7%9A%84%E8%81%94%E5%90%88%E5%B7%A5%E4%BD%9C%EF%BC%8C%E5%B0%86%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%97%B6%E4%BB%A3%E7%9A%84%E7%82%B9%E5%87%BB%E7%8E%87%E9%A2%84%E6%B5%8B%E6%A8%A1%E5%9E%8B%E5%88%86%E4%B8%BA%E4%BA%86%E4%B8%89%E7%B1%BB%EF%BC%9A%E7%AC%AC%E4%B8%80%E7%B1%BB%E5%B0%B1%E6%98%AF%E5%9F%BA%E4%BA%8E%E7%BB%84%E5%90%88%E7%89%B9%E5%BE%81%E6%8C%96%E6%8E%98%E7%9A%84%E6%A8%A1%E5%9E%8B%EF%BC%9B%E7%AC%AC%E4%BA%8C%E7%B1%BB%E9%92%88%E5%AF%B9%E7%94%A8%E6%88%B7%E8%A1%8C%E4%B8%BA%E7%9A%84%E6%A8%A1%E5%9E%8B%EF%BC%9B%E7%AC%AC%E4%B8%89%E7%B1%BB%E6%98%AF%E8%87%AA%E5%8A%A8%E6%9E%B6%E6%9E%84%E6%90%9C%E7%B4%A2%E7%9A%84%E6%A8%A1%E5%9E%8B%E3%80%82))
  - 📝 提及的 survey：Deep Learning for Click-Through Rate Estimation

- > ![](https://pic1.zhimg.com/v2-fa70ca1c0fe329e26e331401abdac828_r.jpg)  ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/509188349#js_content:~:text=https://pic1.zhimg.com/v2-fa70ca1c0fe329e26e331401abdac828_r.jpg))

- > ![](https://pic2.zhimg.com/v2-c902c0cbf0edbffa7ca1cc9a3c4229a9_r.jpg)  ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/509188349#js_content:~:text=https://pic2.zhimg.com/v2-c902c0cbf0edbffa7ca1cc9a3c4229a9_r.jpg))

- > 另一类是组合特征挖掘类模型。

我个人认为可以分为三类：

第一类就是像 wide&deep 模型，谷歌最先提出，他们在模型里面加入了显示的交叉，也就是特征之间笛卡尔相乘之后构建出来新特征，加入到线性部分，这样模型会记住这些特征，当下次组合特征出现的时候，会直接把它的权重取出来做预测。  ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/509188349#js_content:~:text=%E5%8F%A6%E4%B8%80%E7%B1%BB%E6%98%AF%E7%BB%84%E5%90%88%E7%89%B9%E5%BE%81%E6%8C%96%E6%8E%98%E7%B1%BB%E6%A8%A1%E5%9E%8B%E3%80%82%E6%88%91%E4%B8%AA%E4%BA%BA%E8%AE%A4%E4%B8%BA%E5%8F%AF%E4%BB%A5%E5%88%86%E4%B8%BA%E4%B8%89%E7%B1%BB%EF%BC%9A%E7%AC%AC%E4%B8%80%E7%B1%BB%E5%B0%B1%E6%98%AF%E5%83%8F%20wide&deep%20%E6%A8%A1%E5%9E%8B%EF%BC%8C%E8%B0%B7%E6%AD%8C%E6%9C%80%E5%85%88%E6%8F%90%E5%87%BA%EF%BC%8C%E4%BB%96%E4%BB%AC%E5%9C%A8%E6%A8%A1%E5%9E%8B%E9%87%8C%E9%9D%A2%E5%8A%A0%E5%85%A5%E4%BA%86%E6%98%BE%E7%A4%BA%E7%9A%84%E4%BA%A4%E5%8F%89%EF%BC%8C%E4%B9%9F%E5%B0%B1%E6%98%AF%E7%89%B9%E5%BE%81%E4%B9%8B%E9%97%B4%E7%AC%9B%E5%8D%A1%E5%B0%94%E7%9B%B8%E4%B9%98%E4%B9%8B%E5%90%8E%E6%9E%84%E5%BB%BA%E5%87%BA%E6%9D%A5%E6%96%B0%E7%89%B9%E5%BE%81%EF%BC%8C%E5%8A%A0%E5%85%A5%E5%88%B0%E7%BA%BF%E6%80%A7%E9%83%A8%E5%88%86%EF%BC%8C%E8%BF%99%E6%A0%B7%E6%A8%A1%E5%9E%8B%E4%BC%9A%E8%AE%B0%E4%BD%8F%E8%BF%99%E4%BA%9B%E7%89%B9%E5%BE%81%EF%BC%8C%E5%BD%93%E4%B8%8B%E6%AC%A1%E7%BB%84%E5%90%88%E7%89%B9%E5%BE%81%E5%87%BA%E7%8E%B0%E7%9A%84%E6%97%B6%E5%80%99%EF%BC%8C%E4%BC%9A%E7%9B%B4%E6%8E%A5%E6%8A%8A%E5%AE%83%E7%9A%84%E6%9D%83%E9%87%8D%E5%8F%96%E5%87%BA%E6%9D%A5%E5%81%9A%E9%A2%84%E6%B5%8B%E3%80%82))

- > 第二类模型是 DeepFM 这类的模型，可以称为双塔模型、双塔结构，像 DCN，xDeepFM 以及后边的很多模型，都属于这类模型，这类模型是在 dnn 之外以及线性之外，加了基于分解的模块，用来建模两个特征之间的组合关系。两个特征的组合关系，是用一个向量的乘法或者是一些复杂的结构来拟合的，建模完这个关系之后，会直接把输出喂到最终输出中，而不会去神经网络。  ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/509188349#js_content:~:text=%E7%AC%AC%E4%BA%8C%E7%B1%BB%E6%A8%A1%E5%9E%8B%E6%98%AF%20DeepFM%20%E8%BF%99%E7%B1%BB%E7%9A%84%E6%A8%A1%E5%9E%8B%EF%BC%8C%E5%8F%AF%E4%BB%A5%E7%A7%B0%E4%B8%BA%E5%8F%8C%E5%A1%94%E6%A8%A1%E5%9E%8B%E3%80%81%E5%8F%8C%E5%A1%94%E7%BB%93%E6%9E%84%EF%BC%8C%E5%83%8F%20DCN%EF%BC%8CxDeepFM%20%E4%BB%A5%E5%8F%8A%E5%90%8E%E8%BE%B9%E7%9A%84%E5%BE%88%E5%A4%9A%E6%A8%A1%E5%9E%8B%EF%BC%8C%E9%83%BD%E5%B1%9E%E4%BA%8E%E8%BF%99%E7%B1%BB%E6%A8%A1%E5%9E%8B%EF%BC%8C%E8%BF%99%E7%B1%BB%E6%A8%A1%E5%9E%8B%E6%98%AF%E5%9C%A8%20dnn%20%E4%B9%8B%E5%A4%96%E4%BB%A5%E5%8F%8A%E7%BA%BF%E6%80%A7%E4%B9%8B%E5%A4%96%EF%BC%8C%E5%8A%A0%E4%BA%86%E5%9F%BA%E4%BA%8E%E5%88%86%E8%A7%A3%E7%9A%84%E6%A8%A1%E5%9D%97%EF%BC%8C%E7%94%A8%E6%9D%A5%E5%BB%BA%E6%A8%A1%E4%B8%A4%E4%B8%AA%E7%89%B9%E5%BE%81%E4%B9%8B%E9%97%B4%E7%9A%84%E7%BB%84%E5%90%88%E5%85%B3%E7%B3%BB%E3%80%82%E4%B8%A4%E4%B8%AA%E7%89%B9%E5%BE%81%E7%9A%84%E7%BB%84%E5%90%88%E5%85%B3%E7%B3%BB%EF%BC%8C%E6%98%AF%E7%94%A8%E4%B8%80%E4%B8%AA%E5%90%91%E9%87%8F%E7%9A%84%E4%B9%98%E6%B3%95%E6%88%96%E8%80%85%E6%98%AF%E4%B8%80%E4%BA%9B%E5%A4%8D%E6%9D%82%E7%9A%84%E7%BB%93%E6%9E%84%E6%9D%A5%E6%8B%9F%E5%90%88%E7%9A%84%EF%BC%8C%E5%BB%BA%E6%A8%A1%E5%AE%8C%E8%BF%99%E4%B8%AA%E5%85%B3%E7%B3%BB%E4%B9%8B%E5%90%8E%EF%BC%8C%E4%BC%9A%E7%9B%B4%E6%8E%A5%E6%8A%8A%E8%BE%93%E5%87%BA%E5%96%82%E5%88%B0%E6%9C%80%E7%BB%88%E8%BE%93%E5%87%BA%E4%B8%AD%EF%BC%8C%E8%80%8C%E4%B8%8D%E4%BC%9A%E5%8E%BB%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E3%80%82))
  - 📝 第一类模型的方式称为“特征组合”的话，第二类的方式也许称为“特征向量组合”更容易理解和区分一些。

- > 与之相反的是第三类如 PNN 这种网络，也会利用分解模式构建特征之间的组合关系，但是它构建完组合关系之后，会再把输出喂入到模型 MLP 中，让 MLP 来再度拟和这些特征之间的关系。  ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/509188349#js_content:~:text=%E4%B8%8E%E4%B9%8B%E7%9B%B8%E5%8F%8D%E7%9A%84%E6%98%AF%E7%AC%AC%E4%B8%89%E7%B1%BB%E5%A6%82%20PNN%20%E8%BF%99%E7%A7%8D%E7%BD%91%E7%BB%9C%EF%BC%8C%E4%B9%9F%E4%BC%9A%E5%88%A9%E7%94%A8%E5%88%86%E8%A7%A3%E6%A8%A1%E5%BC%8F%E6%9E%84%E5%BB%BA%E7%89%B9%E5%BE%81%E4%B9%8B%E9%97%B4%E7%9A%84%E7%BB%84%E5%90%88%E5%85%B3%E7%B3%BB%EF%BC%8C%E4%BD%86%E6%98%AF%E5%AE%83%E6%9E%84%E5%BB%BA%E5%AE%8C%E7%BB%84%E5%90%88%E5%85%B3%E7%B3%BB%E4%B9%8B%E5%90%8E%EF%BC%8C%E4%BC%9A%E5%86%8D%E6%8A%8A%E8%BE%93%E5%87%BA%E5%96%82%E5%85%A5%E5%88%B0%E6%A8%A1%E5%9E%8B%20MLP%20%E4%B8%AD%EF%BC%8C%E8%AE%A9%20MLP%20%E6%9D%A5%E5%86%8D%E5%BA%A6%E6%8B%9F%E5%92%8C%E8%BF%99%E4%BA%9B%E7%89%B9%E5%BE%81%E4%B9%8B%E9%97%B4%E7%9A%84%E5%85%B3%E7%B3%BB%E3%80%82))
  - 📝 单纯地从特征组合的角度来看，该分类不必单独拎出来，和第二类的区别只是在于后续的层有所不同：双塔和 MLP

- > ![](https://pic2.zhimg.com/v2-baadda67104d650555919182064bf121_r.jpg)  ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/509188349#js_content:~:text=https://pic2.zhimg.com/v2-baadda67104d650555919182064bf121_r.jpg))

- > 像阿里的 CAN 模型，并没有使用显示的特征，而是将显示的交互特征（组合特征）喂入模型， 带来的提升也是很明显的。怎么设计特征或者说怎么选择哪些特征做显示的喂入，哪些做隐式的交叉也是一个研究方向。  ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/509188349#js_content:~:text=%E5%83%8F%E9%98%BF%E9%87%8C%E7%9A%84%20CAN%20%E6%A8%A1%E5%9E%8B%EF%BC%8C%E5%B9%B6%E6%B2%A1%E6%9C%89%E4%BD%BF%E7%94%A8%E6%98%BE%E7%A4%BA%E7%9A%84%E7%89%B9%E5%BE%81%EF%BC%8C%E8%80%8C%E6%98%AF%E5%B0%86%E6%98%BE%E7%A4%BA%E7%9A%84%E4%BA%A4%E4%BA%92%E7%89%B9%E5%BE%81%EF%BC%88%E7%BB%84%E5%90%88%E7%89%B9%E5%BE%81%EF%BC%89%E5%96%82%E5%85%A5%E6%A8%A1%E5%9E%8B%EF%BC%8C%20%E5%B8%A6%E6%9D%A5%E7%9A%84%E6%8F%90%E5%8D%87%E4%B9%9F%E6%98%AF%E5%BE%88%E6%98%8E%E6%98%BE%E7%9A%84%E3%80%82%E6%80%8E%E4%B9%88%E8%AE%BE%E8%AE%A1%E7%89%B9%E5%BE%81%E6%88%96%E8%80%85%E8%AF%B4%E6%80%8E%E4%B9%88%E9%80%89%E6%8B%A9%E5%93%AA%E4%BA%9B%E7%89%B9%E5%BE%81%E5%81%9A%E6%98%BE%E7%A4%BA%E7%9A%84%E5%96%82%E5%85%A5%EF%BC%8C%E5%93%AA%E4%BA%9B%E5%81%9A%E9%9A%90%E5%BC%8F%E7%9A%84%E4%BA%A4%E5%8F%89%E4%B9%9F%E6%98%AF%E4%B8%80%E4%B8%AA%E7%A0%94%E7%A9%B6%E6%96%B9%E5%90%91%E3%80%82))
  - 📝 先交叉再 embedding，而不是先 embedding 再交叉。
  - 📝 +[[CAN]]

- > 华为诺亚方舟实验室在 2020 年发表的 AutoFIS 模型，该模型针对交叉特征加了一组参数，用来自动去学哪些特征重要，哪些特征不重要。通过第一阶段的搜索，筛选出重要特征，把不重要的去掉，再重新输入到模型，这样做效果有明显提升。  ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/509188349#js_content:~:text=%E5%8D%8E%E4%B8%BA%E8%AF%BA%E4%BA%9A%E6%96%B9%E8%88%9F%E5%AE%9E%E9%AA%8C%E5%AE%A4%E5%9C%A8%202020%20%E5%B9%B4%E5%8F%91%E8%A1%A8%E7%9A%84%20AutoFIS%20%E6%A8%A1%E5%9E%8B%EF%BC%8C%E8%AF%A5%E6%A8%A1%E5%9E%8B%E9%92%88%E5%AF%B9%E4%BA%A4%E5%8F%89%E7%89%B9%E5%BE%81%E5%8A%A0%E4%BA%86%E4%B8%80%E7%BB%84%E5%8F%82%E6%95%B0%EF%BC%8C%E7%94%A8%E6%9D%A5%E8%87%AA%E5%8A%A8%E5%8E%BB%E5%AD%A6%E5%93%AA%E4%BA%9B%E7%89%B9%E5%BE%81%E9%87%8D%E8%A6%81%EF%BC%8C%E5%93%AA%E4%BA%9B%E7%89%B9%E5%BE%81%E4%B8%8D%E9%87%8D%E8%A6%81%E3%80%82%E9%80%9A%E8%BF%87%E7%AC%AC%E4%B8%80%E9%98%B6%E6%AE%B5%E7%9A%84%E6%90%9C%E7%B4%A2%EF%BC%8C%E7%AD%9B%E9%80%89%E5%87%BA%E9%87%8D%E8%A6%81%E7%89%B9%E5%BE%81%EF%BC%8C%E6%8A%8A%E4%B8%8D%E9%87%8D%E8%A6%81%E7%9A%84%E5%8E%BB%E6%8E%89%EF%BC%8C%E5%86%8D%E9%87%8D%E6%96%B0%E8%BE%93%E5%85%A5%E5%88%B0%E6%A8%A1%E5%9E%8B%EF%BC%8C%E8%BF%99%E6%A0%B7%E5%81%9A%E6%95%88%E6%9E%9C%E6%9C%89%E6%98%8E%E6%98%BE%E6%8F%90%E5%8D%87%E3%80%82))
  - 📝 +[[AutoFIS]]

- > 阿里以及上海交通大学张伟楠老师分别发表了类似的工作：SIM 和 UBR。这两个工作想法类似：在行为数据中加入检索模块。  ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/509188349#js_content:~:text=%E9%98%BF%E9%87%8C%E4%BB%A5%E5%8F%8A%E4%B8%8A%E6%B5%B7%E4%BA%A4%E9%80%9A%E5%A4%A7%E5%AD%A6%E5%BC%A0%E4%BC%9F%E6%A5%A0%E8%80%81%E5%B8%88%E5%88%86%E5%88%AB%E5%8F%91%E8%A1%A8%E4%BA%86%E7%B1%BB%E4%BC%BC%E7%9A%84%E5%B7%A5%E4%BD%9C%EF%BC%9ASIM%20%E5%92%8C%20UBR%E3%80%82%E8%BF%99%E4%B8%A4%E4%B8%AA%E5%B7%A5%E4%BD%9C%E6%83%B3%E6%B3%95%E7%B1%BB%E4%BC%BC%EF%BC%9A%E5%9C%A8%E8%A1%8C%E4%B8%BA%E6%95%B0%E6%8D%AE%E4%B8%AD%E5%8A%A0%E5%85%A5%E6%A3%80%E7%B4%A2%E6%A8%A1%E5%9D%97%E3%80%82))
  - 📝 在用户行为序列中进行检索。

- > 这里的检索基于一个 target，即预测目标，去对用户的行为做了一个筛选或者加权。基于这样的操作，模型会有很明显的提升。  ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/509188349#js_content:~:text=%E8%BF%99%E9%87%8C%E7%9A%84%E6%A3%80%E7%B4%A2%E5%9F%BA%E4%BA%8E%E4%B8%80%E4%B8%AA%20target%EF%BC%8C%E5%8D%B3%E9%A2%84%E6%B5%8B%E7%9B%AE%E6%A0%87%EF%BC%8C%E5%8E%BB%E5%AF%B9%E7%94%A8%E6%88%B7%E7%9A%84%E8%A1%8C%E4%B8%BA%E5%81%9A%E4%BA%86%E4%B8%80%E4%B8%AA%E7%AD%9B%E9%80%89%E6%88%96%E8%80%85%E5%8A%A0%E6%9D%83%E3%80%82%E5%9F%BA%E4%BA%8E%E8%BF%99%E6%A0%B7%E7%9A%84%E6%93%8D%E4%BD%9C%EF%BC%8C%E6%A8%A1%E5%9E%8B%E4%BC%9A%E6%9C%89%E5%BE%88%E6%98%8E%E6%98%BE%E7%9A%84%E6%8F%90%E5%8D%87%E3%80%82))
  - 📝 用户行为序列检索，关键是检索的目标，target/query 是什么？

- > 怎样去处理大 embedding。分两个方面来看，一方面就是怎样把 embedding 变小，也就是将 embedding 压缩；另一方面就是怎么用更新的分布式架构去更高效更低成本地去训练大 embedding。  ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/509188349#js_content:~:text=%E6%80%8E%E6%A0%B7%E5%8E%BB%E5%A4%84%E7%90%86%E5%A4%A7%20embedding%E3%80%82%E5%88%86%E4%B8%A4%E4%B8%AA%E6%96%B9%E9%9D%A2%E6%9D%A5%E7%9C%8B%EF%BC%8C%E4%B8%80%E6%96%B9%E9%9D%A2%E5%B0%B1%E6%98%AF%E6%80%8E%E6%A0%B7%E6%8A%8A%20embedding%20%E5%8F%98%E5%B0%8F%EF%BC%8C%E4%B9%9F%E5%B0%B1%E6%98%AF%E5%B0%86%20embedding%20%E5%8E%8B%E7%BC%A9%EF%BC%9B%E5%8F%A6%E4%B8%80%E6%96%B9%E9%9D%A2%E5%B0%B1%E6%98%AF%E6%80%8E%E4%B9%88%E7%94%A8%E6%9B%B4%E6%96%B0%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%9E%B6%E6%9E%84%E5%8E%BB%E6%9B%B4%E9%AB%98%E6%95%88%E6%9B%B4%E4%BD%8E%E6%88%90%E6%9C%AC%E5%9C%B0%E5%8E%BB%E8%AE%AD%E7%BB%83%E5%A4%A7%20embedding%E3%80%82))
  - 📝 1. 压缩 embedding
  - 📝 2. 更高效更低成本地训练大 embedding

- > twitter 在 Recsys 2021 发表的 Double hash 的方法。这种方法首先把特征分成了高频和低频，因为高频特征相对比例比较小，给每一个高频特征分配一个独立的 embedding，它所占的空间也不是很大。对于低频特征，使用 Double hash 方法进行压缩，该 hash 方法是为了尽可能地减少冲突。 [[推特技术]]   ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/509188349#js_content:~:text=twitter%20%E5%9C%A8%20Recsys%202021%20%E5%8F%91%E8%A1%A8%E7%9A%84%20Double%20hash%20%E7%9A%84%E6%96%B9%E6%B3%95%E3%80%82%E8%BF%99%E7%A7%8D%E6%96%B9%E6%B3%95%E9%A6%96%E5%85%88%E6%8A%8A%E7%89%B9%E5%BE%81%E5%88%86%E6%88%90%E4%BA%86%E9%AB%98%E9%A2%91%E5%92%8C%E4%BD%8E%E9%A2%91%EF%BC%8C%E5%9B%A0%E4%B8%BA%E9%AB%98%E9%A2%91%E7%89%B9%E5%BE%81%E7%9B%B8%E5%AF%B9%E6%AF%94%E4%BE%8B%E6%AF%94%E8%BE%83%E5%B0%8F%EF%BC%8C%E7%BB%99%E6%AF%8F%E4%B8%80%E4%B8%AA%E9%AB%98%E9%A2%91%E7%89%B9%E5%BE%81%E5%88%86%E9%85%8D%E4%B8%80%E4%B8%AA%E7%8B%AC%E7%AB%8B%E7%9A%84%20embedding%EF%BC%8C%E5%AE%83%E6%89%80%E5%8D%A0%E7%9A%84%E7%A9%BA%E9%97%B4%E4%B9%9F%E4%B8%8D%E6%98%AF%E5%BE%88%E5%A4%A7%E3%80%82%E5%AF%B9%E4%BA%8E%E4%BD%8E%E9%A2%91%E7%89%B9%E5%BE%81%EF%BC%8C%E4%BD%BF%E7%94%A8%20Double%20hash%20%E6%96%B9%E6%B3%95%E8%BF%9B%E8%A1%8C%E5%8E%8B%E7%BC%A9%EF%BC%8C%E8%AF%A5%20hash%20%E6%96%B9%E6%B3%95%E6%98%AF%E4%B8%BA%E4%BA%86%E5%B0%BD%E5%8F%AF%E8%83%BD%E5%9C%B0%E5%87%8F%E5%B0%91%E5%86%B2%E7%AA%81%E3%80%82))
  - 📝 所谓 double hashing，简单地说，是分层哈希
  - 📝 是 recsys2020 年的论文 [Model Size Reduction Using Frequency Based Double Hashing for Recommender Systems](https://arxiv.org/pdf/2007.14523.pdf)

- > 百度在 SIGMOD2021 发表的一篇基于 int16 训练 Embedding 参数。  ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/509188349#js_content:~:text=%E7%99%BE%E5%BA%A6%E5%9C%A8%20SIGMOD2021%20%E5%8F%91%E8%A1%A8%E7%9A%84%E4%B8%80%E7%AF%87%E5%9F%BA%E4%BA%8E%20int16%20%E8%AE%AD%E7%BB%83%20Embedding%20%E5%8F%82%E6%95%B0%E3%80%82))
  - 📝 是这篇论文 [Agile and Accurate CTR Prediction Model Training for Massive-Scale Online Advertising Systems](https://dl.acm.org/doi/pdf/10.1145/3448016.3457236)

- > Google 发表在 KDD2021 上的 DHE 模型，去掉了 Embedding Table。  ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/509188349#js_content:~:text=Google%20%E5%8F%91%E8%A1%A8%E5%9C%A8%20KDD2021%20%E4%B8%8A%E7%9A%84%20DHE%20%E6%A8%A1%E5%9E%8B%EF%BC%8C%E5%8E%BB%E6%8E%89%E4%BA%86%20Embedding%20Table%E3%80%82))
  - 📝 是这篇论文 [Learning to Embed Categorical Features without Embedding Tables for Recommendation](https://dl.acm.org/doi/10.1145/3447548.3467304)

- > ![](https://pic3.zhimg.com/v2-eb6fddd686c3efe7f3309674f97b9fbe_r.jpg)  ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/509188349#js_content:~:text=https://pic3.zhimg.com/v2-eb6fddd686c3efe7f3309674f97b9fbe_r.jpg))

- > 传统的 embedding 的处理方法，对一个特征进行编码，得到一个 ID，然后用 ID 去一个大的 Embedding table 里面查表，得到它对应的 Embedding  ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/509188349#js_content:~:text=%E4%BC%A0%E7%BB%9F%E7%9A%84%20embedding%20%E7%9A%84%E5%A4%84%E7%90%86%E6%96%B9%E6%B3%95%EF%BC%8C%E5%AF%B9%E4%B8%80%E4%B8%AA%E7%89%B9%E5%BE%81%E8%BF%9B%E8%A1%8C%E7%BC%96%E7%A0%81%EF%BC%8C%E5%BE%97%E5%88%B0%E4%B8%80%E4%B8%AA%20ID%EF%BC%8C%E7%84%B6%E5%90%8E%E7%94%A8%20ID%20%E5%8E%BB%E4%B8%80%E4%B8%AA%E5%A4%A7%E7%9A%84%20Embedding%20table%20%E9%87%8C%E9%9D%A2%E6%9F%A5%E8%A1%A8%EF%BC%8C%E5%BE%97%E5%88%B0%E5%AE%83%E5%AF%B9%E5%BA%94%E7%9A%84%20Embedding))
  - 📝 传统的 特征编码 + embedding 方式

- > 谷歌的 DHE 基于原始输入，用了 1024 个 hash 函数对数据做了一个硬编码，但函数怎么设计，没有提到，只是给了一个简要的指导，基于它硬编码之后的 1024 维输出，会再通过一个多层的网络去恢复出来一个 Embedding，也就是说他认为 1024 维的 hash 函数进行编码加上多层神经网络即可恢复出 Embedding table 的参数。  ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/509188349#js_content:~:text=%E8%B0%B7%E6%AD%8C%E7%9A%84%20DHE%20%E5%9F%BA%E4%BA%8E%E5%8E%9F%E5%A7%8B%E8%BE%93%E5%85%A5%EF%BC%8C%E7%94%A8%E4%BA%86%201024%20%E4%B8%AA%20hash%20%E5%87%BD%E6%95%B0%E5%AF%B9%E6%95%B0%E6%8D%AE%E5%81%9A%E4%BA%86%E4%B8%80%E4%B8%AA%E7%A1%AC%E7%BC%96%E7%A0%81%EF%BC%8C%E4%BD%86%E5%87%BD%E6%95%B0%E6%80%8E%E4%B9%88%E8%AE%BE%E8%AE%A1%EF%BC%8C%E6%B2%A1%E6%9C%89%E6%8F%90%E5%88%B0%EF%BC%8C%E5%8F%AA%E6%98%AF%E7%BB%99%E4%BA%86%E4%B8%80%E4%B8%AA%E7%AE%80%E8%A6%81%E7%9A%84%E6%8C%87%E5%AF%BC%EF%BC%8C%E5%9F%BA%E4%BA%8E%E5%AE%83%E7%A1%AC%E7%BC%96%E7%A0%81%E4%B9%8B%E5%90%8E%E7%9A%84%201024%20%E7%BB%B4%E8%BE%93%E5%87%BA%EF%BC%8C%E4%BC%9A%E5%86%8D%E9%80%9A%E8%BF%87%E4%B8%80%E4%B8%AA%E5%A4%9A%E5%B1%82%E7%9A%84%E7%BD%91%E7%BB%9C%E5%8E%BB%E6%81%A2%E5%A4%8D%E5%87%BA%E6%9D%A5%E4%B8%80%E4%B8%AA%20Embedding%EF%BC%8C%E4%B9%9F%E5%B0%B1%E6%98%AF%E8%AF%B4%E4%BB%96%E8%AE%A4%E4%B8%BA%201024%20%E7%BB%B4%E7%9A%84%20hash%20%E5%87%BD%E6%95%B0%E8%BF%9B%E8%A1%8C%E7%BC%96%E7%A0%81%E5%8A%A0%E4%B8%8A%E5%A4%9A%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8D%B3%E5%8F%AF%E6%81%A2%E5%A4%8D%E5%87%BA%20Embedding%20table%20%E7%9A%84%E5%8F%82%E6%95%B0%E3%80%82))
  - 📝 最开始接触深度学习的时候，我对深度神经网络的理解，它就是一个加密路由器：每一层都是对前一层特征的加解密，再路由到下一层。这里的 DHE 可能更接近我最初的设想：hash 作为加密路由。
  - 📝 但是==恢复==，暂时不解

- > 腾讯发表于 SIGIR2020 的 DES 通过模型结合硬件设计了一个分布式的方案。英伟达提出基于 cude 直接写了一个 HugeCTR，当然还有很多其他工作，后面的第四部分会介绍华为诺亚方舟实验室的 ScaleFreeCTR 模型  ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/509188349#js_content:~:text=%E8%85%BE%E8%AE%AF%E5%8F%91%E8%A1%A8%E4%BA%8E%20SIGIR2020%20%E7%9A%84%20DES%20%E9%80%9A%E8%BF%87%E6%A8%A1%E5%9E%8B%E7%BB%93%E5%90%88%E7%A1%AC%E4%BB%B6%E8%AE%BE%E8%AE%A1%E4%BA%86%E4%B8%80%E4%B8%AA%E5%88%86%E5%B8%83%E5%BC%8F%E7%9A%84%E6%96%B9%E6%A1%88%E3%80%82%E8%8B%B1%E4%BC%9F%E8%BE%BE%E6%8F%90%E5%87%BA%E5%9F%BA%E4%BA%8E%20cude%20%E7%9B%B4%E6%8E%A5%E5%86%99%E4%BA%86%E4%B8%80%E4%B8%AA%20HugeCTR%EF%BC%8C%E5%BD%93%E7%84%B6%E8%BF%98%E6%9C%89%E5%BE%88%E5%A4%9A%E5%85%B6%E4%BB%96%E5%B7%A5%E4%BD%9C%EF%BC%8C%E5%90%8E%E9%9D%A2%E7%9A%84%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%E4%BC%9A%E4%BB%8B%E7%BB%8D%E5%8D%8E%E4%B8%BA%E8%AF%BA%E4%BA%9A%E6%96%B9%E8%88%9F%E5%AE%9E%E9%AA%8C%E5%AE%A4%E7%9A%84%20ScaleFreeCTR%20%E6%A8%A1%E5%9E%8B))
  - 📝 几种分布式训练大 embedding 的方法：
  - 📝 1. 腾讯的 DES
  - 📝 2. 英伟达的 HugeCTR
  - 📝 3. 华为的 ScaleFreeCTR

