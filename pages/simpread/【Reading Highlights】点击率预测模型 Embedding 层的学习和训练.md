title:: 【Reading Highlights】点击率预测模型 Embedding 层的学习和训练
source:: https://zhuanlan.zhihu.com/p/509188349
summary:: 
tags:: [[简悦]] [[embedding]]  [[华为技术]]   [[reading_highlights]]
date:: 20220520  

- > ![](https://pic1.zhimg.com/v2-1b972b16334617ccc85a0c6e36539d9c_r.jpg)  ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/509188349#js_content:~:text=https://pic1.zhimg.com/v2-1b972b16334617ccc85a0c6e36539d9c_r.jpg))

- > ![](https://pic4.zhimg.com/v2-015a9245728ff37e8932d1f08a549dcf_r.jpg)  ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/509188349#js_content:~:text=https://pic4.zhimg.com/v2-015a9245728ff37e8932d1f08a549dcf_r.jpg))

- > 在 2021 年 IJCAI 上面有这样一篇 Survey 论文，是上海交通大学张伟楠老师和华为诺亚实验实的联合工作，将深度学习时代的点击率预测模型分为了三类：

*   第一类就是基于组合特征挖掘的模型；
*   第二类针对用户行为的模型；
*   第三类是自动架构搜索的模型。  ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/509188349#js_content:~:text=%E5%9C%A8%202021%20%E5%B9%B4%20IJCAI%20%E4%B8%8A%E9%9D%A2%E6%9C%89%E8%BF%99%E6%A0%B7%E4%B8%80%E7%AF%87%20Survey%20%E8%AE%BA%E6%96%87%EF%BC%8C%E6%98%AF%E4%B8%8A%E6%B5%B7%E4%BA%A4%E9%80%9A%E5%A4%A7%E5%AD%A6%E5%BC%A0%E4%BC%9F%E6%A5%A0%E8%80%81%E5%B8%88%E5%92%8C%E5%8D%8E%E4%B8%BA%E8%AF%BA%E4%BA%9A%E5%AE%9E%E9%AA%8C%E5%AE%9E%E7%9A%84%E8%81%94%E5%90%88%E5%B7%A5%E4%BD%9C%EF%BC%8C%E5%B0%86%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%97%B6%E4%BB%A3%E7%9A%84%E7%82%B9%E5%87%BB%E7%8E%87%E9%A2%84%E6%B5%8B%E6%A8%A1%E5%9E%8B%E5%88%86%E4%B8%BA%E4%BA%86%E4%B8%89%E7%B1%BB%EF%BC%9A%E7%AC%AC%E4%B8%80%E7%B1%BB%E5%B0%B1%E6%98%AF%E5%9F%BA%E4%BA%8E%E7%BB%84%E5%90%88%E7%89%B9%E5%BE%81%E6%8C%96%E6%8E%98%E7%9A%84%E6%A8%A1%E5%9E%8B%EF%BC%9B%E7%AC%AC%E4%BA%8C%E7%B1%BB%E9%92%88%E5%AF%B9%E7%94%A8%E6%88%B7%E8%A1%8C%E4%B8%BA%E7%9A%84%E6%A8%A1%E5%9E%8B%EF%BC%9B%E7%AC%AC%E4%B8%89%E7%B1%BB%E6%98%AF%E8%87%AA%E5%8A%A8%E6%9E%B6%E6%9E%84%E6%90%9C%E7%B4%A2%E7%9A%84%E6%A8%A1%E5%9E%8B%E3%80%82))
  - 📝 提及的 survey：Deep Learning for Click-Through Rate Estimation

- > ![](https://pic1.zhimg.com/v2-fa70ca1c0fe329e26e331401abdac828_r.jpg)  ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/509188349#js_content:~:text=https://pic1.zhimg.com/v2-fa70ca1c0fe329e26e331401abdac828_r.jpg))

- > ![](https://pic2.zhimg.com/v2-c902c0cbf0edbffa7ca1cc9a3c4229a9_r.jpg)  ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/509188349#js_content:~:text=https://pic2.zhimg.com/v2-c902c0cbf0edbffa7ca1cc9a3c4229a9_r.jpg))

- > 另一类是组合特征挖掘类模型。

我个人认为可以分为三类：

第一类就是像 wide&deep 模型，谷歌最先提出，他们在模型里面加入了显示的交叉，也就是特征之间笛卡尔相乘之后构建出来新特征，加入到线性部分，这样模型会记住这些特征，当下次组合特征出现的时候，会直接把它的权重取出来做预测。  ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/509188349#js_content:~:text=%E5%8F%A6%E4%B8%80%E7%B1%BB%E6%98%AF%E7%BB%84%E5%90%88%E7%89%B9%E5%BE%81%E6%8C%96%E6%8E%98%E7%B1%BB%E6%A8%A1%E5%9E%8B%E3%80%82%E6%88%91%E4%B8%AA%E4%BA%BA%E8%AE%A4%E4%B8%BA%E5%8F%AF%E4%BB%A5%E5%88%86%E4%B8%BA%E4%B8%89%E7%B1%BB%EF%BC%9A%E7%AC%AC%E4%B8%80%E7%B1%BB%E5%B0%B1%E6%98%AF%E5%83%8F%20wide&deep%20%E6%A8%A1%E5%9E%8B%EF%BC%8C%E8%B0%B7%E6%AD%8C%E6%9C%80%E5%85%88%E6%8F%90%E5%87%BA%EF%BC%8C%E4%BB%96%E4%BB%AC%E5%9C%A8%E6%A8%A1%E5%9E%8B%E9%87%8C%E9%9D%A2%E5%8A%A0%E5%85%A5%E4%BA%86%E6%98%BE%E7%A4%BA%E7%9A%84%E4%BA%A4%E5%8F%89%EF%BC%8C%E4%B9%9F%E5%B0%B1%E6%98%AF%E7%89%B9%E5%BE%81%E4%B9%8B%E9%97%B4%E7%AC%9B%E5%8D%A1%E5%B0%94%E7%9B%B8%E4%B9%98%E4%B9%8B%E5%90%8E%E6%9E%84%E5%BB%BA%E5%87%BA%E6%9D%A5%E6%96%B0%E7%89%B9%E5%BE%81%EF%BC%8C%E5%8A%A0%E5%85%A5%E5%88%B0%E7%BA%BF%E6%80%A7%E9%83%A8%E5%88%86%EF%BC%8C%E8%BF%99%E6%A0%B7%E6%A8%A1%E5%9E%8B%E4%BC%9A%E8%AE%B0%E4%BD%8F%E8%BF%99%E4%BA%9B%E7%89%B9%E5%BE%81%EF%BC%8C%E5%BD%93%E4%B8%8B%E6%AC%A1%E7%BB%84%E5%90%88%E7%89%B9%E5%BE%81%E5%87%BA%E7%8E%B0%E7%9A%84%E6%97%B6%E5%80%99%EF%BC%8C%E4%BC%9A%E7%9B%B4%E6%8E%A5%E6%8A%8A%E5%AE%83%E7%9A%84%E6%9D%83%E9%87%8D%E5%8F%96%E5%87%BA%E6%9D%A5%E5%81%9A%E9%A2%84%E6%B5%8B%E3%80%82))

- > 第二类模型是 DeepFM 这类的模型，可以称为双塔模型、双塔结构，像 DCN，xDeepFM 以及后边的很多模型，都属于这类模型，这类模型是在 dnn 之外以及线性之外，加了基于分解的模块，用来建模两个特征之间的组合关系。两个特征的组合关系，是用一个向量的乘法或者是一些复杂的结构来拟合的，建模完这个关系之后，会直接把输出喂到最终输出中，而不会去神经网络。  ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/509188349#js_content:~:text=%E7%AC%AC%E4%BA%8C%E7%B1%BB%E6%A8%A1%E5%9E%8B%E6%98%AF%20DeepFM%20%E8%BF%99%E7%B1%BB%E7%9A%84%E6%A8%A1%E5%9E%8B%EF%BC%8C%E5%8F%AF%E4%BB%A5%E7%A7%B0%E4%B8%BA%E5%8F%8C%E5%A1%94%E6%A8%A1%E5%9E%8B%E3%80%81%E5%8F%8C%E5%A1%94%E7%BB%93%E6%9E%84%EF%BC%8C%E5%83%8F%20DCN%EF%BC%8CxDeepFM%20%E4%BB%A5%E5%8F%8A%E5%90%8E%E8%BE%B9%E7%9A%84%E5%BE%88%E5%A4%9A%E6%A8%A1%E5%9E%8B%EF%BC%8C%E9%83%BD%E5%B1%9E%E4%BA%8E%E8%BF%99%E7%B1%BB%E6%A8%A1%E5%9E%8B%EF%BC%8C%E8%BF%99%E7%B1%BB%E6%A8%A1%E5%9E%8B%E6%98%AF%E5%9C%A8%20dnn%20%E4%B9%8B%E5%A4%96%E4%BB%A5%E5%8F%8A%E7%BA%BF%E6%80%A7%E4%B9%8B%E5%A4%96%EF%BC%8C%E5%8A%A0%E4%BA%86%E5%9F%BA%E4%BA%8E%E5%88%86%E8%A7%A3%E7%9A%84%E6%A8%A1%E5%9D%97%EF%BC%8C%E7%94%A8%E6%9D%A5%E5%BB%BA%E6%A8%A1%E4%B8%A4%E4%B8%AA%E7%89%B9%E5%BE%81%E4%B9%8B%E9%97%B4%E7%9A%84%E7%BB%84%E5%90%88%E5%85%B3%E7%B3%BB%E3%80%82%E4%B8%A4%E4%B8%AA%E7%89%B9%E5%BE%81%E7%9A%84%E7%BB%84%E5%90%88%E5%85%B3%E7%B3%BB%EF%BC%8C%E6%98%AF%E7%94%A8%E4%B8%80%E4%B8%AA%E5%90%91%E9%87%8F%E7%9A%84%E4%B9%98%E6%B3%95%E6%88%96%E8%80%85%E6%98%AF%E4%B8%80%E4%BA%9B%E5%A4%8D%E6%9D%82%E7%9A%84%E7%BB%93%E6%9E%84%E6%9D%A5%E6%8B%9F%E5%90%88%E7%9A%84%EF%BC%8C%E5%BB%BA%E6%A8%A1%E5%AE%8C%E8%BF%99%E4%B8%AA%E5%85%B3%E7%B3%BB%E4%B9%8B%E5%90%8E%EF%BC%8C%E4%BC%9A%E7%9B%B4%E6%8E%A5%E6%8A%8A%E8%BE%93%E5%87%BA%E5%96%82%E5%88%B0%E6%9C%80%E7%BB%88%E8%BE%93%E5%87%BA%E4%B8%AD%EF%BC%8C%E8%80%8C%E4%B8%8D%E4%BC%9A%E5%8E%BB%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E3%80%82))
  - 📝 第一类模型的方式称为“特征组合”的话，第二类的方式也许称为“特征向量组合”更容易理解和区分一些。

- > 与之相反的是第三类如 PNN 这种网络，也会利用分解模式构建特征之间的组合关系，但是它构建完组合关系之后，会再把输出喂入到模型 MLP 中，让 MLP 来再度拟和这些特征之间的关系。  ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/509188349#js_content:~:text=%E4%B8%8E%E4%B9%8B%E7%9B%B8%E5%8F%8D%E7%9A%84%E6%98%AF%E7%AC%AC%E4%B8%89%E7%B1%BB%E5%A6%82%20PNN%20%E8%BF%99%E7%A7%8D%E7%BD%91%E7%BB%9C%EF%BC%8C%E4%B9%9F%E4%BC%9A%E5%88%A9%E7%94%A8%E5%88%86%E8%A7%A3%E6%A8%A1%E5%BC%8F%E6%9E%84%E5%BB%BA%E7%89%B9%E5%BE%81%E4%B9%8B%E9%97%B4%E7%9A%84%E7%BB%84%E5%90%88%E5%85%B3%E7%B3%BB%EF%BC%8C%E4%BD%86%E6%98%AF%E5%AE%83%E6%9E%84%E5%BB%BA%E5%AE%8C%E7%BB%84%E5%90%88%E5%85%B3%E7%B3%BB%E4%B9%8B%E5%90%8E%EF%BC%8C%E4%BC%9A%E5%86%8D%E6%8A%8A%E8%BE%93%E5%87%BA%E5%96%82%E5%85%A5%E5%88%B0%E6%A8%A1%E5%9E%8B%20MLP%20%E4%B8%AD%EF%BC%8C%E8%AE%A9%20MLP%20%E6%9D%A5%E5%86%8D%E5%BA%A6%E6%8B%9F%E5%92%8C%E8%BF%99%E4%BA%9B%E7%89%B9%E5%BE%81%E4%B9%8B%E9%97%B4%E7%9A%84%E5%85%B3%E7%B3%BB%E3%80%82))
  - 📝 单纯地从特征组合的角度来看，该分类不必单独拎出来，和第二类的区别只是在于后续的层有所不同：双塔和 MLP

- > ![](https://pic2.zhimg.com/v2-baadda67104d650555919182064bf121_r.jpg)  ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/509188349#js_content:~:text=https://pic2.zhimg.com/v2-baadda67104d650555919182064bf121_r.jpg))

- > 像阿里的 CAN 模型，并没有使用显示的特征，而是将显示的交互特征（组合特征）喂入模型， 带来的提升也是很明显的。怎么设计特征或者说怎么选择哪些特征做显示的喂入，哪些做隐式的交叉也是一个研究方向。  ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/509188349#js_content:~:text=%E5%83%8F%E9%98%BF%E9%87%8C%E7%9A%84%20CAN%20%E6%A8%A1%E5%9E%8B%EF%BC%8C%E5%B9%B6%E6%B2%A1%E6%9C%89%E4%BD%BF%E7%94%A8%E6%98%BE%E7%A4%BA%E7%9A%84%E7%89%B9%E5%BE%81%EF%BC%8C%E8%80%8C%E6%98%AF%E5%B0%86%E6%98%BE%E7%A4%BA%E7%9A%84%E4%BA%A4%E4%BA%92%E7%89%B9%E5%BE%81%EF%BC%88%E7%BB%84%E5%90%88%E7%89%B9%E5%BE%81%EF%BC%89%E5%96%82%E5%85%A5%E6%A8%A1%E5%9E%8B%EF%BC%8C%20%E5%B8%A6%E6%9D%A5%E7%9A%84%E6%8F%90%E5%8D%87%E4%B9%9F%E6%98%AF%E5%BE%88%E6%98%8E%E6%98%BE%E7%9A%84%E3%80%82%E6%80%8E%E4%B9%88%E8%AE%BE%E8%AE%A1%E7%89%B9%E5%BE%81%E6%88%96%E8%80%85%E8%AF%B4%E6%80%8E%E4%B9%88%E9%80%89%E6%8B%A9%E5%93%AA%E4%BA%9B%E7%89%B9%E5%BE%81%E5%81%9A%E6%98%BE%E7%A4%BA%E7%9A%84%E5%96%82%E5%85%A5%EF%BC%8C%E5%93%AA%E4%BA%9B%E5%81%9A%E9%9A%90%E5%BC%8F%E7%9A%84%E4%BA%A4%E5%8F%89%E4%B9%9F%E6%98%AF%E4%B8%80%E4%B8%AA%E7%A0%94%E7%A9%B6%E6%96%B9%E5%90%91%E3%80%82))
  - 📝 先交叉再 embedding，而不是先 embedding 再交叉。
  - 📝 +[[CAN]]

- > 华为诺亚方舟实验室在 2020 年发表的 AutoFIS 模型，该模型针对交叉特征加了一组参数，用来自动去学哪些特征重要，哪些特征不重要。通过第一阶段的搜索，筛选出重要特征，把不重要的去掉，再重新输入到模型，这样做效果有明显提升。  ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/509188349#js_content:~:text=%E5%8D%8E%E4%B8%BA%E8%AF%BA%E4%BA%9A%E6%96%B9%E8%88%9F%E5%AE%9E%E9%AA%8C%E5%AE%A4%E5%9C%A8%202020%20%E5%B9%B4%E5%8F%91%E8%A1%A8%E7%9A%84%20AutoFIS%20%E6%A8%A1%E5%9E%8B%EF%BC%8C%E8%AF%A5%E6%A8%A1%E5%9E%8B%E9%92%88%E5%AF%B9%E4%BA%A4%E5%8F%89%E7%89%B9%E5%BE%81%E5%8A%A0%E4%BA%86%E4%B8%80%E7%BB%84%E5%8F%82%E6%95%B0%EF%BC%8C%E7%94%A8%E6%9D%A5%E8%87%AA%E5%8A%A8%E5%8E%BB%E5%AD%A6%E5%93%AA%E4%BA%9B%E7%89%B9%E5%BE%81%E9%87%8D%E8%A6%81%EF%BC%8C%E5%93%AA%E4%BA%9B%E7%89%B9%E5%BE%81%E4%B8%8D%E9%87%8D%E8%A6%81%E3%80%82%E9%80%9A%E8%BF%87%E7%AC%AC%E4%B8%80%E9%98%B6%E6%AE%B5%E7%9A%84%E6%90%9C%E7%B4%A2%EF%BC%8C%E7%AD%9B%E9%80%89%E5%87%BA%E9%87%8D%E8%A6%81%E7%89%B9%E5%BE%81%EF%BC%8C%E6%8A%8A%E4%B8%8D%E9%87%8D%E8%A6%81%E7%9A%84%E5%8E%BB%E6%8E%89%EF%BC%8C%E5%86%8D%E9%87%8D%E6%96%B0%E8%BE%93%E5%85%A5%E5%88%B0%E6%A8%A1%E5%9E%8B%EF%BC%8C%E8%BF%99%E6%A0%B7%E5%81%9A%E6%95%88%E6%9E%9C%E6%9C%89%E6%98%8E%E6%98%BE%E6%8F%90%E5%8D%87%E3%80%82))
  - 📝 +[[AutoFIS]]

- > 阿里以及上海交通大学张伟楠老师分别发表了类似的工作：SIM 和 UBR。这两个工作想法类似：在行为数据中加入检索模块。  ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/509188349#js_content:~:text=%E9%98%BF%E9%87%8C%E4%BB%A5%E5%8F%8A%E4%B8%8A%E6%B5%B7%E4%BA%A4%E9%80%9A%E5%A4%A7%E5%AD%A6%E5%BC%A0%E4%BC%9F%E6%A5%A0%E8%80%81%E5%B8%88%E5%88%86%E5%88%AB%E5%8F%91%E8%A1%A8%E4%BA%86%E7%B1%BB%E4%BC%BC%E7%9A%84%E5%B7%A5%E4%BD%9C%EF%BC%9ASIM%20%E5%92%8C%20UBR%E3%80%82%E8%BF%99%E4%B8%A4%E4%B8%AA%E5%B7%A5%E4%BD%9C%E6%83%B3%E6%B3%95%E7%B1%BB%E4%BC%BC%EF%BC%9A%E5%9C%A8%E8%A1%8C%E4%B8%BA%E6%95%B0%E6%8D%AE%E4%B8%AD%E5%8A%A0%E5%85%A5%E6%A3%80%E7%B4%A2%E6%A8%A1%E5%9D%97%E3%80%82))
  - 📝 在用户行为序列中进行检索。

- > 这里的检索基于一个 target，即预测目标，去对用户的行为做了一个筛选或者加权。基于这样的操作，模型会有很明显的提升。  ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/509188349#js_content:~:text=%E8%BF%99%E9%87%8C%E7%9A%84%E6%A3%80%E7%B4%A2%E5%9F%BA%E4%BA%8E%E4%B8%80%E4%B8%AA%20target%EF%BC%8C%E5%8D%B3%E9%A2%84%E6%B5%8B%E7%9B%AE%E6%A0%87%EF%BC%8C%E5%8E%BB%E5%AF%B9%E7%94%A8%E6%88%B7%E7%9A%84%E8%A1%8C%E4%B8%BA%E5%81%9A%E4%BA%86%E4%B8%80%E4%B8%AA%E7%AD%9B%E9%80%89%E6%88%96%E8%80%85%E5%8A%A0%E6%9D%83%E3%80%82%E5%9F%BA%E4%BA%8E%E8%BF%99%E6%A0%B7%E7%9A%84%E6%93%8D%E4%BD%9C%EF%BC%8C%E6%A8%A1%E5%9E%8B%E4%BC%9A%E6%9C%89%E5%BE%88%E6%98%8E%E6%98%BE%E7%9A%84%E6%8F%90%E5%8D%87%E3%80%82))
  - 📝 用户行为序列检索，关键是检索的目标，target/query 是什么？

- > 怎样去处理大 embedding。分两个方面来看，一方面就是怎样把 embedding 变小，也就是将 embedding 压缩；另一方面就是怎么用更新的分布式架构去更高效更低成本地去训练大 embedding。  ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/509188349#js_content:~:text=%E6%80%8E%E6%A0%B7%E5%8E%BB%E5%A4%84%E7%90%86%E5%A4%A7%20embedding%E3%80%82%E5%88%86%E4%B8%A4%E4%B8%AA%E6%96%B9%E9%9D%A2%E6%9D%A5%E7%9C%8B%EF%BC%8C%E4%B8%80%E6%96%B9%E9%9D%A2%E5%B0%B1%E6%98%AF%E6%80%8E%E6%A0%B7%E6%8A%8A%20embedding%20%E5%8F%98%E5%B0%8F%EF%BC%8C%E4%B9%9F%E5%B0%B1%E6%98%AF%E5%B0%86%20embedding%20%E5%8E%8B%E7%BC%A9%EF%BC%9B%E5%8F%A6%E4%B8%80%E6%96%B9%E9%9D%A2%E5%B0%B1%E6%98%AF%E6%80%8E%E4%B9%88%E7%94%A8%E6%9B%B4%E6%96%B0%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%9E%B6%E6%9E%84%E5%8E%BB%E6%9B%B4%E9%AB%98%E6%95%88%E6%9B%B4%E4%BD%8E%E6%88%90%E6%9C%AC%E5%9C%B0%E5%8E%BB%E8%AE%AD%E7%BB%83%E5%A4%A7%20embedding%E3%80%82))
  - 📝 1. 压缩 embedding
  - 📝 2. 更高效更低成本地训练大 embedding

- > twitter 在 Recsys 2021 发表的 Double hash 的方法。这种方法首先把特征分成了高频和低频，因为高频特征相对比例比较小，给每一个高频特征分配一个独立的 embedding，它所占的空间也不是很大。对于低频特征，使用 Double hash 方法进行压缩，该 hash 方法是为了尽可能地减少冲突。 [[推特技术]]   ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/509188349#js_content:~:text=twitter%20%E5%9C%A8%20Recsys%202021%20%E5%8F%91%E8%A1%A8%E7%9A%84%20Double%20hash%20%E7%9A%84%E6%96%B9%E6%B3%95%E3%80%82%E8%BF%99%E7%A7%8D%E6%96%B9%E6%B3%95%E9%A6%96%E5%85%88%E6%8A%8A%E7%89%B9%E5%BE%81%E5%88%86%E6%88%90%E4%BA%86%E9%AB%98%E9%A2%91%E5%92%8C%E4%BD%8E%E9%A2%91%EF%BC%8C%E5%9B%A0%E4%B8%BA%E9%AB%98%E9%A2%91%E7%89%B9%E5%BE%81%E7%9B%B8%E5%AF%B9%E6%AF%94%E4%BE%8B%E6%AF%94%E8%BE%83%E5%B0%8F%EF%BC%8C%E7%BB%99%E6%AF%8F%E4%B8%80%E4%B8%AA%E9%AB%98%E9%A2%91%E7%89%B9%E5%BE%81%E5%88%86%E9%85%8D%E4%B8%80%E4%B8%AA%E7%8B%AC%E7%AB%8B%E7%9A%84%20embedding%EF%BC%8C%E5%AE%83%E6%89%80%E5%8D%A0%E7%9A%84%E7%A9%BA%E9%97%B4%E4%B9%9F%E4%B8%8D%E6%98%AF%E5%BE%88%E5%A4%A7%E3%80%82%E5%AF%B9%E4%BA%8E%E4%BD%8E%E9%A2%91%E7%89%B9%E5%BE%81%EF%BC%8C%E4%BD%BF%E7%94%A8%20Double%20hash%20%E6%96%B9%E6%B3%95%E8%BF%9B%E8%A1%8C%E5%8E%8B%E7%BC%A9%EF%BC%8C%E8%AF%A5%20hash%20%E6%96%B9%E6%B3%95%E6%98%AF%E4%B8%BA%E4%BA%86%E5%B0%BD%E5%8F%AF%E8%83%BD%E5%9C%B0%E5%87%8F%E5%B0%91%E5%86%B2%E7%AA%81%E3%80%82))
  - 📝 所谓 double hashing，简单地说，是分层哈希
  - 📝 是 recsys2020 年的论文 [Model Size Reduction Using Frequency Based Double Hashing for Recommender Systems](https://arxiv.org/pdf/2007.14523.pdf)

- > 百度在 SIGMOD2021 发表的一篇基于 int16 训练 Embedding 参数。  ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/509188349#js_content:~:text=%E7%99%BE%E5%BA%A6%E5%9C%A8%20SIGMOD2021%20%E5%8F%91%E8%A1%A8%E7%9A%84%E4%B8%80%E7%AF%87%E5%9F%BA%E4%BA%8E%20int16%20%E8%AE%AD%E7%BB%83%20Embedding%20%E5%8F%82%E6%95%B0%E3%80%82))
  - 📝 是这篇论文 [Agile and Accurate CTR Prediction Model Training for Massive-Scale Online Advertising Systems](https://dl.acm.org/doi/pdf/10.1145/3448016.3457236)

- > Google 发表在 KDD2021 上的 DHE 模型，去掉了 Embedding Table。  ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/509188349#js_content:~:text=Google%20%E5%8F%91%E8%A1%A8%E5%9C%A8%20KDD2021%20%E4%B8%8A%E7%9A%84%20DHE%20%E6%A8%A1%E5%9E%8B%EF%BC%8C%E5%8E%BB%E6%8E%89%E4%BA%86%20Embedding%20Table%E3%80%82))
  - 📝 是这篇论文 [Learning to Embed Categorical Features without Embedding Tables for Recommendation](https://dl.acm.org/doi/10.1145/3447548.3467304)

- > ![](https://pic3.zhimg.com/v2-eb6fddd686c3efe7f3309674f97b9fbe_r.jpg)  ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/509188349#js_content:~:text=https://pic3.zhimg.com/v2-eb6fddd686c3efe7f3309674f97b9fbe_r.jpg))

- > 传统的 embedding 的处理方法，对一个特征进行编码，得到一个 ID，然后用 ID 去一个大的 Embedding table 里面查表，得到它对应的 Embedding  ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/509188349#js_content:~:text=%E4%BC%A0%E7%BB%9F%E7%9A%84%20embedding%20%E7%9A%84%E5%A4%84%E7%90%86%E6%96%B9%E6%B3%95%EF%BC%8C%E5%AF%B9%E4%B8%80%E4%B8%AA%E7%89%B9%E5%BE%81%E8%BF%9B%E8%A1%8C%E7%BC%96%E7%A0%81%EF%BC%8C%E5%BE%97%E5%88%B0%E4%B8%80%E4%B8%AA%20ID%EF%BC%8C%E7%84%B6%E5%90%8E%E7%94%A8%20ID%20%E5%8E%BB%E4%B8%80%E4%B8%AA%E5%A4%A7%E7%9A%84%20Embedding%20table%20%E9%87%8C%E9%9D%A2%E6%9F%A5%E8%A1%A8%EF%BC%8C%E5%BE%97%E5%88%B0%E5%AE%83%E5%AF%B9%E5%BA%94%E7%9A%84%20Embedding))
  - 📝 传统的 特征编码 + embedding 方式

- > 谷歌的 DHE 基于原始输入，用了 1024 个 hash 函数对数据做了一个硬编码，但函数怎么设计，没有提到，只是给了一个简要的指导，基于它硬编码之后的 1024 维输出，会再通过一个多层的网络去恢复出来一个 Embedding，也就是说他认为 1024 维的 hash 函数进行编码加上多层神经网络即可恢复出 Embedding table 的参数。  ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/509188349#js_content:~:text=%E8%B0%B7%E6%AD%8C%E7%9A%84%20DHE%20%E5%9F%BA%E4%BA%8E%E5%8E%9F%E5%A7%8B%E8%BE%93%E5%85%A5%EF%BC%8C%E7%94%A8%E4%BA%86%201024%20%E4%B8%AA%20hash%20%E5%87%BD%E6%95%B0%E5%AF%B9%E6%95%B0%E6%8D%AE%E5%81%9A%E4%BA%86%E4%B8%80%E4%B8%AA%E7%A1%AC%E7%BC%96%E7%A0%81%EF%BC%8C%E4%BD%86%E5%87%BD%E6%95%B0%E6%80%8E%E4%B9%88%E8%AE%BE%E8%AE%A1%EF%BC%8C%E6%B2%A1%E6%9C%89%E6%8F%90%E5%88%B0%EF%BC%8C%E5%8F%AA%E6%98%AF%E7%BB%99%E4%BA%86%E4%B8%80%E4%B8%AA%E7%AE%80%E8%A6%81%E7%9A%84%E6%8C%87%E5%AF%BC%EF%BC%8C%E5%9F%BA%E4%BA%8E%E5%AE%83%E7%A1%AC%E7%BC%96%E7%A0%81%E4%B9%8B%E5%90%8E%E7%9A%84%201024%20%E7%BB%B4%E8%BE%93%E5%87%BA%EF%BC%8C%E4%BC%9A%E5%86%8D%E9%80%9A%E8%BF%87%E4%B8%80%E4%B8%AA%E5%A4%9A%E5%B1%82%E7%9A%84%E7%BD%91%E7%BB%9C%E5%8E%BB%E6%81%A2%E5%A4%8D%E5%87%BA%E6%9D%A5%E4%B8%80%E4%B8%AA%20Embedding%EF%BC%8C%E4%B9%9F%E5%B0%B1%E6%98%AF%E8%AF%B4%E4%BB%96%E8%AE%A4%E4%B8%BA%201024%20%E7%BB%B4%E7%9A%84%20hash%20%E5%87%BD%E6%95%B0%E8%BF%9B%E8%A1%8C%E7%BC%96%E7%A0%81%E5%8A%A0%E4%B8%8A%E5%A4%9A%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8D%B3%E5%8F%AF%E6%81%A2%E5%A4%8D%E5%87%BA%20Embedding%20table%20%E7%9A%84%E5%8F%82%E6%95%B0%E3%80%82))
  - 📝 最开始接触深度学习的时候，我对深度神经网络的理解，它就是一个加密路由器：每一层都是对前一层特征的加解密，再路由到下一层。这里的 DHE 可能更接近我最初的设想：hash 作为加密路由。
  - 📝 但是==恢复==，暂时不解

- > 腾讯发表于 SIGIR2020 的 DES 通过模型结合硬件设计了一个分布式的方案。英伟达提出基于 cude 直接写了一个 HugeCTR，当然还有很多其他工作，后面的第四部分会介绍华为诺亚方舟实验室的 ScaleFreeCTR 模型  ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/509188349#js_content:~:text=%E8%85%BE%E8%AE%AF%E5%8F%91%E8%A1%A8%E4%BA%8E%20SIGIR2020%20%E7%9A%84%20DES%20%E9%80%9A%E8%BF%87%E6%A8%A1%E5%9E%8B%E7%BB%93%E5%90%88%E7%A1%AC%E4%BB%B6%E8%AE%BE%E8%AE%A1%E4%BA%86%E4%B8%80%E4%B8%AA%E5%88%86%E5%B8%83%E5%BC%8F%E7%9A%84%E6%96%B9%E6%A1%88%E3%80%82%E8%8B%B1%E4%BC%9F%E8%BE%BE%E6%8F%90%E5%87%BA%E5%9F%BA%E4%BA%8E%20cude%20%E7%9B%B4%E6%8E%A5%E5%86%99%E4%BA%86%E4%B8%80%E4%B8%AA%20HugeCTR%EF%BC%8C%E5%BD%93%E7%84%B6%E8%BF%98%E6%9C%89%E5%BE%88%E5%A4%9A%E5%85%B6%E4%BB%96%E5%B7%A5%E4%BD%9C%EF%BC%8C%E5%90%8E%E9%9D%A2%E7%9A%84%E7%AC%AC%E5%9B%9B%E9%83%A8%E5%88%86%E4%BC%9A%E4%BB%8B%E7%BB%8D%E5%8D%8E%E4%B8%BA%E8%AF%BA%E4%BA%9A%E6%96%B9%E8%88%9F%E5%AE%9E%E9%AA%8C%E5%AE%A4%E7%9A%84%20ScaleFreeCTR%20%E6%A8%A1%E5%9E%8B))
  - 📝 几种分布式训练大 embedding 的方法：
  - 📝 1. 腾讯的 DES
  - 📝 2. 英伟达的 HugeCTR
  - 📝 3. 华为的 ScaleFreeCTR

- > ![](https://pic1.zhimg.com/v2-4086838761a50ecf0c95b971be413e4c_r.jpg)  ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/509188349#js_content:~:text=https://pic1.zhimg.com/v2-4086838761a50ecf0c95b971be413e4c_r.jpg))

- > 对离散特征的处理方法，它就是做了 one-hot 的编码，然后去做 Embedding lookup。  ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/509188349#js_content:~:text=%E5%AF%B9%E7%A6%BB%E6%95%A3%E7%89%B9%E5%BE%81%E7%9A%84%E5%A4%84%E7%90%86%E6%96%B9%E6%B3%95%EF%BC%8C%E5%AE%83%E5%B0%B1%E6%98%AF%E5%81%9A%E4%BA%86%20one-hot%20%E7%9A%84%E7%BC%96%E7%A0%81%EF%BC%8C%E7%84%B6%E5%90%8E%E5%8E%BB%E5%81%9A%20Embedding%20lookup%E3%80%82))
  - 📝 离散特征的 embedding 比较简单，通常是 one-hot+embedding

- > 散特征的 embedding 比较简单，通常是 one-hot+embedding对于连续特征的处理方法，调研发现主要分为三类，第一类就是不使用 Embedding，把原始值做一些变化，或者是归一化后输入到模型。第二类是 Field Embedding，是给每个域一个 Embedding。第三类是把连续特征离散化，之后把它当成离散特征来处理。  ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/509188349#js_content:~:text=%E6%95%A3%E7%89%B9%E5%BE%81%E7%9A%84%20embedding%20%E6%AF%94%E8%BE%83%E7%AE%80%E5%8D%95%EF%BC%8C%E9%80%9A%E5%B8%B8%E6%98%AF%20one-hot+embedding%E5%AF%B9%E4%BA%8E%E8%BF%9E%E7%BB%AD%E7%89%B9%E5%BE%81%E7%9A%84%E5%A4%84%E7%90%86%E6%96%B9%E6%B3%95%EF%BC%8C%E8%B0%83%E7%A0%94%E5%8F%91%E7%8E%B0%E4%B8%BB%E8%A6%81%E5%88%86%E4%B8%BA%E4%B8%89%E7%B1%BB%EF%BC%8C%E7%AC%AC%E4%B8%80%E7%B1%BB%E5%B0%B1%E6%98%AF%E4%B8%8D%E4%BD%BF%E7%94%A8%20Embedding%EF%BC%8C%E6%8A%8A%E5%8E%9F%E5%A7%8B%E5%80%BC%E5%81%9A%E4%B8%80%E4%BA%9B%E5%8F%98%E5%8C%96%EF%BC%8C%E6%88%96%E8%80%85%E6%98%AF%E5%BD%92%E4%B8%80%E5%8C%96%E5%90%8E%E8%BE%93%E5%85%A5%E5%88%B0%E6%A8%A1%E5%9E%8B%E3%80%82%E7%AC%AC%E4%BA%8C%E7%B1%BB%E6%98%AF%20Field%20Embedding%EF%BC%8C%E6%98%AF%E7%BB%99%E6%AF%8F%E4%B8%AA%E5%9F%9F%E4%B8%80%E4%B8%AA%20Embedding%E3%80%82%E7%AC%AC%E4%B8%89%E7%B1%BB%E6%98%AF%E6%8A%8A%E8%BF%9E%E7%BB%AD%E7%89%B9%E5%BE%81%E7%A6%BB%E6%95%A3%E5%8C%96%EF%BC%8C%E4%B9%8B%E5%90%8E%E6%8A%8A%E5%AE%83%E5%BD%93%E6%88%90%E7%A6%BB%E6%95%A3%E7%89%B9%E5%BE%81%E6%9D%A5%E5%A4%84%E7%90%86%E3%80%82))
  - 📝 连续特征的 embedding 的花样比较多。
  - 📝 此处的 field embedding 不知道是不是我之前理解的：特征列 embedding，特征值作为系数乘以整个 embedding

- > wide&Deep，在它的介绍里面，使用的是原始值，另外一个是谷歌的 YouTubeNet，它会对原始值做平方开根号这些变换。另一个是 facebook DLRM 模型，对连续值的处理方式是把所有的连续值输入到一个神经网络，然后通过神经网络把它压缩到一个 embedding 维度大小的一个向量上，然后将 Embedding 和其他离散特征 Embedding Concat 起来，再做后面根据它的模型去做不同的计算。京东的 DMT 模型，他们的网络是使用了归一化的输出，这种方法表示能力比较弱，因为它这里其实没有对原始的延续特征做一个很好的表示。  ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/509188349#js_content:~:text=wide&Deep%EF%BC%8C%E5%9C%A8%E5%AE%83%E7%9A%84%E4%BB%8B%E7%BB%8D%E9%87%8C%E9%9D%A2%EF%BC%8C%E4%BD%BF%E7%94%A8%E7%9A%84%E6%98%AF%E5%8E%9F%E5%A7%8B%E5%80%BC%EF%BC%8C%E5%8F%A6%E5%A4%96%E4%B8%80%E4%B8%AA%E6%98%AF%E8%B0%B7%E6%AD%8C%E7%9A%84%20YouTubeNet%EF%BC%8C%E5%AE%83%E4%BC%9A%E5%AF%B9%E5%8E%9F%E5%A7%8B%E5%80%BC%E5%81%9A%E5%B9%B3%E6%96%B9%E5%BC%80%E6%A0%B9%E5%8F%B7%E8%BF%99%E4%BA%9B%E5%8F%98%E6%8D%A2%E3%80%82%E5%8F%A6%E4%B8%80%E4%B8%AA%E6%98%AF%20facebook%20DLRM%20%E6%A8%A1%E5%9E%8B%EF%BC%8C%E5%AF%B9%E8%BF%9E%E7%BB%AD%E5%80%BC%E7%9A%84%E5%A4%84%E7%90%86%E6%96%B9%E5%BC%8F%E6%98%AF%E6%8A%8A%E6%89%80%E6%9C%89%E7%9A%84%E8%BF%9E%E7%BB%AD%E5%80%BC%E8%BE%93%E5%85%A5%E5%88%B0%E4%B8%80%E4%B8%AA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%8C%E7%84%B6%E5%90%8E%E9%80%9A%E8%BF%87%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%8A%8A%E5%AE%83%E5%8E%8B%E7%BC%A9%E5%88%B0%E4%B8%80%E4%B8%AA%20embedding%20%E7%BB%B4%E5%BA%A6%E5%A4%A7%E5%B0%8F%E7%9A%84%E4%B8%80%E4%B8%AA%E5%90%91%E9%87%8F%E4%B8%8A%EF%BC%8C%E7%84%B6%E5%90%8E%E5%B0%86%20Embedding%20%E5%92%8C%E5%85%B6%E4%BB%96%E7%A6%BB%E6%95%A3%E7%89%B9%E5%BE%81%20Embedding%20Concat%20%E8%B5%B7%E6%9D%A5%EF%BC%8C%E5%86%8D%E5%81%9A%E5%90%8E%E9%9D%A2%E6%A0%B9%E6%8D%AE%E5%AE%83%E7%9A%84%E6%A8%A1%E5%9E%8B%E5%8E%BB%E5%81%9A%E4%B8%8D%E5%90%8C%E7%9A%84%E8%AE%A1%E7%AE%97%E3%80%82%E4%BA%AC%E4%B8%9C%E7%9A%84%20DMT%20%E6%A8%A1%E5%9E%8B%EF%BC%8C%E4%BB%96%E4%BB%AC%E7%9A%84%E7%BD%91%E7%BB%9C%E6%98%AF%E4%BD%BF%E7%94%A8%E4%BA%86%E5%BD%92%E4%B8%80%E5%8C%96%E7%9A%84%E8%BE%93%E5%87%BA%EF%BC%8C%E8%BF%99%E7%A7%8D%E6%96%B9%E6%B3%95%E8%A1%A8%E7%A4%BA%E8%83%BD%E5%8A%9B%E6%AF%94%E8%BE%83%E5%BC%B1%EF%BC%8C%E5%9B%A0%E4%B8%BA%E5%AE%83%E8%BF%99%E9%87%8C%E5%85%B6%E5%AE%9E%E6%B2%A1%E6%9C%89%E5%AF%B9%E5%8E%9F%E5%A7%8B%E7%9A%84%E5%BB%B6%E7%BB%AD%E7%89%B9%E5%BE%81%E5%81%9A%E4%B8%80%E4%B8%AA%E5%BE%88%E5%A5%BD%E7%9A%84%E8%A1%A8%E7%A4%BA%E3%80%82))
  - 📝 no embedding 的几种做法：
  - 📝 1. 原始值（以 wide and deep 为代表）
  - 📝 2. 原始值的数值变换（以 YoutubeNet、京东的 DMT 为代表）
  - 📝 3. 所有的连续值拼接成一个向量，再经过神经网络压缩为 embedding（以 Meta 的 DLRM 为代表）（输入到压缩用的网络之前，处理无非上述两种方式）

- > 第二种处理连续值的方法是 Field Embedding，每个域有一个 Embedding。某个域的 Embedding 是该域的一个连续值，乘上它的域的 Embedding。这类方法的问题是表示能力比较弱，然后不同值之间是一个线性的关系。  ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/509188349#js_content:~:text=%E7%AC%AC%E4%BA%8C%E7%A7%8D%E5%A4%84%E7%90%86%E8%BF%9E%E7%BB%AD%E5%80%BC%E7%9A%84%E6%96%B9%E6%B3%95%E6%98%AF%20Field%20Embedding%EF%BC%8C%E6%AF%8F%E4%B8%AA%E5%9F%9F%E6%9C%89%E4%B8%80%E4%B8%AA%20Embedding%E3%80%82%E6%9F%90%E4%B8%AA%E5%9F%9F%E7%9A%84%20Embedding%20%E6%98%AF%E8%AF%A5%E5%9F%9F%E7%9A%84%E4%B8%80%E4%B8%AA%E8%BF%9E%E7%BB%AD%E5%80%BC%EF%BC%8C%E4%B9%98%E4%B8%8A%E5%AE%83%E7%9A%84%E5%9F%9F%E7%9A%84%20Embedding%E3%80%82%E8%BF%99%E7%B1%BB%E6%96%B9%E6%B3%95%E7%9A%84%E9%97%AE%E9%A2%98%E6%98%AF%E8%A1%A8%E7%A4%BA%E8%83%BD%E5%8A%9B%E6%AF%94%E8%BE%83%E5%BC%B1%EF%BC%8C%E7%84%B6%E5%90%8E%E4%B8%8D%E5%90%8C%E5%80%BC%E4%B9%8B%E9%97%B4%E6%98%AF%E4%B8%80%E4%B8%AA%E7%BA%BF%E6%80%A7%E7%9A%84%E5%85%B3%E7%B3%BB%E3%80%82))
  - 📝 佐证了我上述的猜想，应该是在 [[AutoFIS]] 中看到过这样的用法。

- > 离散化可以有很多方法，比方说等频、等距和取 log，或者基于树的模型去做一个预训练。 [[特征工程]]   ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/509188349#js_content:~:text=%E7%A6%BB%E6%95%A3%E5%8C%96%E5%8F%AF%E4%BB%A5%E6%9C%89%E5%BE%88%E5%A4%9A%E6%96%B9%E6%B3%95%EF%BC%8C%E6%AF%94%E6%96%B9%E8%AF%B4%E7%AD%89%E9%A2%91%E3%80%81%E7%AD%89%E8%B7%9D%E5%92%8C%E5%8F%96%20log%EF%BC%8C%E6%88%96%E8%80%85%E5%9F%BA%E4%BA%8E%E6%A0%91%E7%9A%84%E6%A8%A1%E5%9E%8B%E5%8E%BB%E5%81%9A%E4%B8%80%E4%B8%AA%E9%A2%84%E8%AE%AD%E7%BB%83%E3%80%82))
  - 📝 连续特征离散化的几种方式方式：
  - 📝 1. 等频分桶
  - 📝 2. 等距分桶
  - 📝 3. 取 log 分桶
  - 📝 4. 使用树模型

- > 这类方法有两个问题：首先，就是它是两阶段的，离散化的过程不能端到端优化；另外，有一些边界的问题，如下图所示的例子，一个年龄特征，假设我们按 40，41 来分，40 以下的我们称之为青年，41 以上的成为中年，其实 40 和 41，它们是很接近的年龄，但是因为我们的离散化的方法，把它分到两个不同的桶里面，可能学到的 Embedding 是差异比较大的 Embedding。  ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/509188349#js_content:~:text=%E8%BF%99%E7%B1%BB%E6%96%B9%E6%B3%95%E6%9C%89%E4%B8%A4%E4%B8%AA%E9%97%AE%E9%A2%98%EF%BC%9A%E9%A6%96%E5%85%88%EF%BC%8C%E5%B0%B1%E6%98%AF%E5%AE%83%E6%98%AF%E4%B8%A4%E9%98%B6%E6%AE%B5%E7%9A%84%EF%BC%8C%E7%A6%BB%E6%95%A3%E5%8C%96%E7%9A%84%E8%BF%87%E7%A8%8B%E4%B8%8D%E8%83%BD%E7%AB%AF%E5%88%B0%E7%AB%AF%E4%BC%98%E5%8C%96%EF%BC%9B%E5%8F%A6%E5%A4%96%EF%BC%8C%E6%9C%89%E4%B8%80%E4%BA%9B%E8%BE%B9%E7%95%8C%E7%9A%84%E9%97%AE%E9%A2%98%EF%BC%8C%E5%A6%82%E4%B8%8B%E5%9B%BE%E6%89%80%E7%A4%BA%E7%9A%84%E4%BE%8B%E5%AD%90%EF%BC%8C%E4%B8%80%E4%B8%AA%E5%B9%B4%E9%BE%84%E7%89%B9%E5%BE%81%EF%BC%8C%E5%81%87%E8%AE%BE%E6%88%91%E4%BB%AC%E6%8C%89%2040%EF%BC%8C41%20%E6%9D%A5%E5%88%86%EF%BC%8C40%20%E4%BB%A5%E4%B8%8B%E7%9A%84%E6%88%91%E4%BB%AC%E7%A7%B0%E4%B9%8B%E4%B8%BA%E9%9D%92%E5%B9%B4%EF%BC%8C41%20%E4%BB%A5%E4%B8%8A%E7%9A%84%E6%88%90%E4%B8%BA%E4%B8%AD%E5%B9%B4%EF%BC%8C%E5%85%B6%E5%AE%9E%2040%20%E5%92%8C%2041%EF%BC%8C%E5%AE%83%E4%BB%AC%E6%98%AF%E5%BE%88%E6%8E%A5%E8%BF%91%E7%9A%84%E5%B9%B4%E9%BE%84%EF%BC%8C%E4%BD%86%E6%98%AF%E5%9B%A0%E4%B8%BA%E6%88%91%E4%BB%AC%E7%9A%84%E7%A6%BB%E6%95%A3%E5%8C%96%E7%9A%84%E6%96%B9%E6%B3%95%EF%BC%8C%E6%8A%8A%E5%AE%83%E5%88%86%E5%88%B0%E4%B8%A4%E4%B8%AA%E4%B8%8D%E5%90%8C%E7%9A%84%E6%A1%B6%E9%87%8C%E9%9D%A2%EF%BC%8C%E5%8F%AF%E8%83%BD%E5%AD%A6%E5%88%B0%E7%9A%84%20Embedding%20%E6%98%AF%E5%B7%AE%E5%BC%82%E6%AF%94%E8%BE%83%E5%A4%A7%E7%9A%84%20Embedding%E3%80%82))
  - 📝 连续特征离散化的问题：
  - 📝 1. 无法端到端的学习
  - 📝 2. 边界值的问题（我觉得不是问题，只是没有太好的处理方式）

- > ![](https://pic4.zhimg.com/v2-e0c9bb40ad71c676508250a0c4f3df63_r.jpg)  ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/509188349#js_content:~:text=https://pic4.zhimg.com/v2-e0c9bb40ad71c676508250a0c4f3df63_r.jpg))

- > 华为提出了一个连续值 Embedding 的方法 AutoDis，它分为三个模块，第一个模块是 Meta-embedding，就是假设有若干个桶，每个桶有个 Field Embedding，这个 embedding 是可以去学习的，还有一个模块叫做 Automatic Discretization，这个模块就是将原始值映射到一个 H 维的向量上。这里函数是去学习连续值分配到 H 个桶上的概率，然后基于分配概率和 Meta-embedding，就会得到最终的连续值的 embedding。  ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/509188349#js_content:~:text=%E5%8D%8E%E4%B8%BA%E6%8F%90%E5%87%BA%E4%BA%86%E4%B8%80%E4%B8%AA%E8%BF%9E%E7%BB%AD%E5%80%BC%20Embedding%20%E7%9A%84%E6%96%B9%E6%B3%95%20AutoDis%EF%BC%8C%E5%AE%83%E5%88%86%E4%B8%BA%E4%B8%89%E4%B8%AA%E6%A8%A1%E5%9D%97%EF%BC%8C%E7%AC%AC%E4%B8%80%E4%B8%AA%E6%A8%A1%E5%9D%97%E6%98%AF%20Meta-embedding%EF%BC%8C%E5%B0%B1%E6%98%AF%E5%81%87%E8%AE%BE%E6%9C%89%E8%8B%A5%E5%B9%B2%E4%B8%AA%E6%A1%B6%EF%BC%8C%E6%AF%8F%E4%B8%AA%E6%A1%B6%E6%9C%89%E4%B8%AA%20Field%20Embedding%EF%BC%8C%E8%BF%99%E4%B8%AA%20embedding%20%E6%98%AF%E5%8F%AF%E4%BB%A5%E5%8E%BB%E5%AD%A6%E4%B9%A0%E7%9A%84%EF%BC%8C%E8%BF%98%E6%9C%89%E4%B8%80%E4%B8%AA%E6%A8%A1%E5%9D%97%E5%8F%AB%E5%81%9A%20Automatic%20Discretization%EF%BC%8C%E8%BF%99%E4%B8%AA%E6%A8%A1%E5%9D%97%E5%B0%B1%E6%98%AF%E5%B0%86%E5%8E%9F%E5%A7%8B%E5%80%BC%E6%98%A0%E5%B0%84%E5%88%B0%E4%B8%80%E4%B8%AA%20H%20%E7%BB%B4%E7%9A%84%E5%90%91%E9%87%8F%E4%B8%8A%E3%80%82%E8%BF%99%E9%87%8C%E5%87%BD%E6%95%B0%E6%98%AF%E5%8E%BB%E5%AD%A6%E4%B9%A0%E8%BF%9E%E7%BB%AD%E5%80%BC%E5%88%86%E9%85%8D%E5%88%B0%20H%20%E4%B8%AA%E6%A1%B6%E4%B8%8A%E7%9A%84%E6%A6%82%E7%8E%87%EF%BC%8C%E7%84%B6%E5%90%8E%E5%9F%BA%E4%BA%8E%E5%88%86%E9%85%8D%E6%A6%82%E7%8E%87%E5%92%8C%20Meta-embedding%EF%BC%8C%E5%B0%B1%E4%BC%9A%E5%BE%97%E5%88%B0%E6%9C%80%E7%BB%88%E7%9A%84%E8%BF%9E%E7%BB%AD%E5%80%BC%E7%9A%84%20embedding%E3%80%82))
  - 📝 和 DME 的思想很像，一个 item （此处是连续特征）会有多个 embedding，再对不同的 embedding 做动态加权
  - 📝 +[[DME]]：[Dynamic Meta-Embeddings for Improved Sentence Representations](https://aclanthology.org/D18-1176/)

- > ![](https://pic4.zhimg.com/v2-3ee46447e7e37ab1396cfc92da31cebb_r.jpg)  ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/509188349#js_content:~:text=https://pic4.zhimg.com/v2-3ee46447e7e37ab1396cfc92da31cebb_r.jpg))
  - 📝 AutoDis 示意图

- > ![](https://pic4.zhimg.com/v2-5175b3370396fd51e1037c764117ae23_r.jpg)  ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/509188349#js_content:~:text=https://pic4.zhimg.com/v2-5175b3370396fd51e1037c764117ae23_r.jpg))
  - 📝 三种常规的特征组合方式：
  - 📝 1. naive
  - 📝 2. memorized
  - 📝 3. factorized

- > 第一类像 FNN 模型，即不建模，每个特征有一个 embedding，所有的特征 embedding 后 concat 拼接输入网络，后面网络自己去学，想学到什么就是什么。  ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/509188349#js_content:~:text=%E7%AC%AC%E4%B8%80%E7%B1%BB%E5%83%8F%20FNN%20%E6%A8%A1%E5%9E%8B%EF%BC%8C%E5%8D%B3%E4%B8%8D%E5%BB%BA%E6%A8%A1%EF%BC%8C%E6%AF%8F%E4%B8%AA%E7%89%B9%E5%BE%81%E6%9C%89%E4%B8%80%E4%B8%AA%20embedding%EF%BC%8C%E6%89%80%E6%9C%89%E7%9A%84%E7%89%B9%E5%BE%81%20embedding%20%E5%90%8E%20concat%20%E6%8B%BC%E6%8E%A5%E8%BE%93%E5%85%A5%E7%BD%91%E7%BB%9C%EF%BC%8C%E5%90%8E%E9%9D%A2%E7%BD%91%E7%BB%9C%E8%87%AA%E5%B7%B1%E5%8E%BB%E5%AD%A6%EF%BC%8C%E6%83%B3%E5%AD%A6%E5%88%B0%E4%BB%80%E4%B9%88%E5%B0%B1%E6%98%AF%E4%BB%80%E4%B9%88%E3%80%82))

- > 第二类像 wide&deep 模型，这里统称为基于记忆的方法，就是去显示的构造组合特征，特征做交叉做笛卡尔积，然后把新构造的特征输入模型。模型就会记住这个特征，这个信号就比较强。  ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/509188349#js_content:~:text=%E7%AC%AC%E4%BA%8C%E7%B1%BB%E5%83%8F%20wide&deep%20%E6%A8%A1%E5%9E%8B%EF%BC%8C%E8%BF%99%E9%87%8C%E7%BB%9F%E7%A7%B0%E4%B8%BA%E5%9F%BA%E4%BA%8E%E8%AE%B0%E5%BF%86%E7%9A%84%E6%96%B9%E6%B3%95%EF%BC%8C%E5%B0%B1%E6%98%AF%E5%8E%BB%E6%98%BE%E7%A4%BA%E7%9A%84%E6%9E%84%E9%80%A0%E7%BB%84%E5%90%88%E7%89%B9%E5%BE%81%EF%BC%8C%E7%89%B9%E5%BE%81%E5%81%9A%E4%BA%A4%E5%8F%89%E5%81%9A%E7%AC%9B%E5%8D%A1%E5%B0%94%E7%A7%AF%EF%BC%8C%E7%84%B6%E5%90%8E%E6%8A%8A%E6%96%B0%E6%9E%84%E9%80%A0%E7%9A%84%E7%89%B9%E5%BE%81%E8%BE%93%E5%85%A5%E6%A8%A1%E5%9E%8B%E3%80%82%E6%A8%A1%E5%9E%8B%E5%B0%B1%E4%BC%9A%E8%AE%B0%E4%BD%8F%E8%BF%99%E4%B8%AA%E7%89%B9%E5%BE%81%EF%BC%8C%E8%BF%99%E4%B8%AA%E4%BF%A1%E5%8F%B7%E5%B0%B1%E6%AF%94%E8%BE%83%E5%BC%BA%E3%80%82))
  - 📝 很适合 ID 类特征的

- > 第三类方法就是基于分解的方法，例如 IPNN 模型，对不同的域之间的交叉关系，通过乘法的方式去做建模，得到的乘法结果会和原始 embedding 一起喂入到后面的 MLP，然后来再次去做一个组合。不同的特征之间是不是都应该组合，或者说怎么去组合，如果我们去试的话，需要去做很多实验，能不能自动判断特征是不是要组合，以及它们之间应该用哪种组合这种关系去学到呢，这个就是我们这个工作的一个动机。  ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/509188349#js_content:~:text=%E7%AC%AC%E4%B8%89%E7%B1%BB%E6%96%B9%E6%B3%95%E5%B0%B1%E6%98%AF%E5%9F%BA%E4%BA%8E%E5%88%86%E8%A7%A3%E7%9A%84%E6%96%B9%E6%B3%95%EF%BC%8C%E4%BE%8B%E5%A6%82%20IPNN%20%E6%A8%A1%E5%9E%8B%EF%BC%8C%E5%AF%B9%E4%B8%8D%E5%90%8C%E7%9A%84%E5%9F%9F%E4%B9%8B%E9%97%B4%E7%9A%84%E4%BA%A4%E5%8F%89%E5%85%B3%E7%B3%BB%EF%BC%8C%E9%80%9A%E8%BF%87%E4%B9%98%E6%B3%95%E7%9A%84%E6%96%B9%E5%BC%8F%E5%8E%BB%E5%81%9A%E5%BB%BA%E6%A8%A1%EF%BC%8C%E5%BE%97%E5%88%B0%E7%9A%84%E4%B9%98%E6%B3%95%E7%BB%93%E6%9E%9C%E4%BC%9A%E5%92%8C%E5%8E%9F%E5%A7%8B%20embedding%20%E4%B8%80%E8%B5%B7%E5%96%82%E5%85%A5%E5%88%B0%E5%90%8E%E9%9D%A2%E7%9A%84%20MLP%EF%BC%8C%E7%84%B6%E5%90%8E%E6%9D%A5%E5%86%8D%E6%AC%A1%E5%8E%BB%E5%81%9A%E4%B8%80%E4%B8%AA%E7%BB%84%E5%90%88%E3%80%82%E4%B8%8D%E5%90%8C%E7%9A%84%E7%89%B9%E5%BE%81%E4%B9%8B%E9%97%B4%E6%98%AF%E4%B8%8D%E6%98%AF%E9%83%BD%E5%BA%94%E8%AF%A5%E7%BB%84%E5%90%88%EF%BC%8C%E6%88%96%E8%80%85%E8%AF%B4%E6%80%8E%E4%B9%88%E5%8E%BB%E7%BB%84%E5%90%88%EF%BC%8C%E5%A6%82%E6%9E%9C%E6%88%91%E4%BB%AC%E5%8E%BB%E8%AF%95%E7%9A%84%E8%AF%9D%EF%BC%8C%E9%9C%80%E8%A6%81%E5%8E%BB%E5%81%9A%E5%BE%88%E5%A4%9A%E5%AE%9E%E9%AA%8C%EF%BC%8C%E8%83%BD%E4%B8%8D%E8%83%BD%E8%87%AA%E5%8A%A8%E5%88%A4%E6%96%AD%E7%89%B9%E5%BE%81%E6%98%AF%E4%B8%8D%E6%98%AF%E8%A6%81%E7%BB%84%E5%90%88%EF%BC%8C%E4%BB%A5%E5%8F%8A%E5%AE%83%E4%BB%AC%E4%B9%8B%E9%97%B4%E5%BA%94%E8%AF%A5%E7%94%A8%E5%93%AA%E7%A7%8D%E7%BB%84%E5%90%88%E8%BF%99%E7%A7%8D%E5%85%B3%E7%B3%BB%E5%8E%BB%E5%AD%A6%E5%88%B0%E5%91%A2%EF%BC%8C%E8%BF%99%E4%B8%AA%E5%B0%B1%E6%98%AF%E6%88%91%E4%BB%AC%E8%BF%99%E4%B8%AA%E5%B7%A5%E4%BD%9C%E7%9A%84%E4%B8%80%E4%B8%AA%E5%8A%A8%E6%9C%BA%E3%80%82))

- > ![](https://pic2.zhimg.com/v2-3b69298aeee488768729f282893da04d_r.jpg)  ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/509188349#js_content:~:text=https://pic2.zhimg.com/v2-3b69298aeee488768729f282893da04d_r.jpg))

- > 最上面有一个分类器，然后中间是一个 Feature Interaction 层，再下面是一个 embedding 层，这一层一方面会为每个特征用原始方法去构建出来它的 embedding 输出（Origin Embedding table），还有一个的话就是通过一个 Cross-product transformation 模块，将交叉特征的 embedding 学到  ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/509188349#js_content:~:text=%E6%9C%80%E4%B8%8A%E9%9D%A2%E6%9C%89%E4%B8%80%E4%B8%AA%E5%88%86%E7%B1%BB%E5%99%A8%EF%BC%8C%E7%84%B6%E5%90%8E%E4%B8%AD%E9%97%B4%E6%98%AF%E4%B8%80%E4%B8%AA%20Feature%20Interaction%20%E5%B1%82%EF%BC%8C%E5%86%8D%E4%B8%8B%E9%9D%A2%E6%98%AF%E4%B8%80%E4%B8%AA%20embedding%20%E5%B1%82%EF%BC%8C%E8%BF%99%E4%B8%80%E5%B1%82%E4%B8%80%E6%96%B9%E9%9D%A2%E4%BC%9A%E4%B8%BA%E6%AF%8F%E4%B8%AA%E7%89%B9%E5%BE%81%E7%94%A8%E5%8E%9F%E5%A7%8B%E6%96%B9%E6%B3%95%E5%8E%BB%E6%9E%84%E5%BB%BA%E5%87%BA%E6%9D%A5%E5%AE%83%E7%9A%84%20embedding%20%E8%BE%93%E5%87%BA%EF%BC%88Origin%20Embedding%20table%EF%BC%89%EF%BC%8C%E8%BF%98%E6%9C%89%E4%B8%80%E4%B8%AA%E7%9A%84%E8%AF%9D%E5%B0%B1%E6%98%AF%E9%80%9A%E8%BF%87%E4%B8%80%E4%B8%AA%20Cross-product%20transformation%20%E6%A8%A1%E5%9D%97%EF%BC%8C%E5%B0%86%E4%BA%A4%E5%8F%89%E7%89%B9%E5%BE%81%E7%9A%84%20embedding%20%E5%AD%A6%E5%88%B0))

- > 选择模块的输入有三个：第一个是使用分解方式去构建的一个 embedding，基于这两个特征的一个 embedding 做一个乘法，然后得到的一个输出；然后第二个输入的话就是拿小白的方法 - 根据业务选择的特征直接输入，不做特征交叉，即我们认为两个特征之间关系不强，不去构建它，用了一个空的 embedding。然后第三个输入，是通过交叉或者笛卡尔去构建出来特征，为这个特征分配一个独立的 embedding。有这样三个输入，进入选择模块，选择模块会最终选出来一个 embedding。  ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/509188349#js_content:~:text=%E9%80%89%E6%8B%A9%E6%A8%A1%E5%9D%97%E7%9A%84%E8%BE%93%E5%85%A5%E6%9C%89%E4%B8%89%E4%B8%AA%EF%BC%9A%E7%AC%AC%E4%B8%80%E4%B8%AA%E6%98%AF%E4%BD%BF%E7%94%A8%E5%88%86%E8%A7%A3%E6%96%B9%E5%BC%8F%E5%8E%BB%E6%9E%84%E5%BB%BA%E7%9A%84%E4%B8%80%E4%B8%AA%20embedding%EF%BC%8C%E5%9F%BA%E4%BA%8E%E8%BF%99%E4%B8%A4%E4%B8%AA%E7%89%B9%E5%BE%81%E7%9A%84%E4%B8%80%E4%B8%AA%20embedding%20%E5%81%9A%E4%B8%80%E4%B8%AA%E4%B9%98%E6%B3%95%EF%BC%8C%E7%84%B6%E5%90%8E%E5%BE%97%E5%88%B0%E7%9A%84%E4%B8%80%E4%B8%AA%E8%BE%93%E5%87%BA%EF%BC%9B%E7%84%B6%E5%90%8E%E7%AC%AC%E4%BA%8C%E4%B8%AA%E8%BE%93%E5%85%A5%E7%9A%84%E8%AF%9D%E5%B0%B1%E6%98%AF%E6%8B%BF%E5%B0%8F%E7%99%BD%E7%9A%84%E6%96%B9%E6%B3%95%20-%20%E6%A0%B9%E6%8D%AE%E4%B8%9A%E5%8A%A1%E9%80%89%E6%8B%A9%E7%9A%84%E7%89%B9%E5%BE%81%E7%9B%B4%E6%8E%A5%E8%BE%93%E5%85%A5%EF%BC%8C%E4%B8%8D%E5%81%9A%E7%89%B9%E5%BE%81%E4%BA%A4%E5%8F%89%EF%BC%8C%E5%8D%B3%E6%88%91%E4%BB%AC%E8%AE%A4%E4%B8%BA%E4%B8%A4%E4%B8%AA%E7%89%B9%E5%BE%81%E4%B9%8B%E9%97%B4%E5%85%B3%E7%B3%BB%E4%B8%8D%E5%BC%BA%EF%BC%8C%E4%B8%8D%E5%8E%BB%E6%9E%84%E5%BB%BA%E5%AE%83%EF%BC%8C%E7%94%A8%E4%BA%86%E4%B8%80%E4%B8%AA%E7%A9%BA%E7%9A%84%20embedding%E3%80%82%E7%84%B6%E5%90%8E%E7%AC%AC%E4%B8%89%E4%B8%AA%E8%BE%93%E5%85%A5%EF%BC%8C%E6%98%AF%E9%80%9A%E8%BF%87%E4%BA%A4%E5%8F%89%E6%88%96%E8%80%85%E7%AC%9B%E5%8D%A1%E5%B0%94%E5%8E%BB%E6%9E%84%E5%BB%BA%E5%87%BA%E6%9D%A5%E7%89%B9%E5%BE%81%EF%BC%8C%E4%B8%BA%E8%BF%99%E4%B8%AA%E7%89%B9%E5%BE%81%E5%88%86%E9%85%8D%E4%B8%80%E4%B8%AA%E7%8B%AC%E7%AB%8B%E7%9A%84%20embedding%E3%80%82%E6%9C%89%E8%BF%99%E6%A0%B7%E4%B8%89%E4%B8%AA%E8%BE%93%E5%85%A5%EF%BC%8C%E8%BF%9B%E5%85%A5%E9%80%89%E6%8B%A9%E6%A8%A1%E5%9D%97%EF%BC%8C%E9%80%89%E6%8B%A9%E6%A8%A1%E5%9D%97%E4%BC%9A%E6%9C%80%E7%BB%88%E9%80%89%E5%87%BA%E6%9D%A5%E4%B8%80%E4%B8%AA%20embedding%E3%80%82))
  - 📝 看描述，似乎是硬性的选择；看图片，其实是三个 embedding 的加权，还是归结到了 dynamic embedding。
  - 📝 +[[DME]]

- > ![](https://pic3.zhimg.com/v2-6d47a9c5470cc17557a0847e0de70c1a_r.jpg)  ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/509188349#js_content:~:text=https://pic3.zhimg.com/v2-6d47a9c5470cc17557a0847e0de70c1a_r.jpg))

- > 推荐模型一般都包含两部分，一部分是参数 embedding，一部分是 MLP。两部分在数据和存储上有不同的特点。embedding 参数量很大，计算量相对比较少。  ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/509188349#js_content:~:text=%E6%8E%A8%E8%8D%90%E6%A8%A1%E5%9E%8B%E4%B8%80%E8%88%AC%E9%83%BD%E5%8C%85%E5%90%AB%E4%B8%A4%E9%83%A8%E5%88%86%EF%BC%8C%E4%B8%80%E9%83%A8%E5%88%86%E6%98%AF%E5%8F%82%E6%95%B0%20embedding%EF%BC%8C%E4%B8%80%E9%83%A8%E5%88%86%E6%98%AF%20MLP%E3%80%82%E4%B8%A4%E9%83%A8%E5%88%86%E5%9C%A8%E6%95%B0%E6%8D%AE%E5%92%8C%E5%AD%98%E5%82%A8%E4%B8%8A%E6%9C%89%E4%B8%8D%E5%90%8C%E7%9A%84%E7%89%B9%E7%82%B9%E3%80%82embedding%20%E5%8F%82%E6%95%B0%E9%87%8F%E5%BE%88%E5%A4%A7%EF%BC%8C%E8%AE%A1%E7%AE%97%E9%87%8F%E7%9B%B8%E5%AF%B9%E6%AF%94%E8%BE%83%E5%B0%91%E3%80%82))

- > 第二部分 MLP，它的参数量相对来说比较少，但计算量会相对比较大。训练 MLP 的话，使用 CPU 它的效率相对于 GPU 来说是低很多的。  ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/509188349#js_content:~:text=%E7%AC%AC%E4%BA%8C%E9%83%A8%E5%88%86%20MLP%EF%BC%8C%E5%AE%83%E7%9A%84%E5%8F%82%E6%95%B0%E9%87%8F%E7%9B%B8%E5%AF%B9%E6%9D%A5%E8%AF%B4%E6%AF%94%E8%BE%83%E5%B0%91%EF%BC%8C%E4%BD%86%E8%AE%A1%E7%AE%97%E9%87%8F%E4%BC%9A%E7%9B%B8%E5%AF%B9%E6%AF%94%E8%BE%83%E5%A4%A7%E3%80%82%E8%AE%AD%E7%BB%83%20MLP%20%E7%9A%84%E8%AF%9D%EF%BC%8C%E4%BD%BF%E7%94%A8%20CPU%20%E5%AE%83%E7%9A%84%E6%95%88%E7%8E%87%E7%9B%B8%E5%AF%B9%E4%BA%8E%20GPU%20%E6%9D%A5%E8%AF%B4%E6%98%AF%E4%BD%8E%E5%BE%88%E5%A4%9A%E7%9A%84%E3%80%82))

- > ![](https://pic2.zhimg.com/v2-a5ffcf0381a9dd5bdcbf5bb30d2dc4e9_r.jpg)  ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/509188349#js_content:~:text=https://pic2.zhimg.com/v2-a5ffcf0381a9dd5bdcbf5bb30d2dc4e9_r.jpg))

- > 在推荐里面因为用了 embedding，就是将高维稀疏的输入映射到了一个低维稠密的一个向量上，当我们稀疏的特征变得很大，大到几十亿甚至是几百亿上千亿规模之后，它的 embedding 的 table 也会变得很大。  ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/509188349#js_content:~:text=%E5%9C%A8%E6%8E%A8%E8%8D%90%E9%87%8C%E9%9D%A2%E5%9B%A0%E4%B8%BA%E7%94%A8%E4%BA%86%20embedding%EF%BC%8C%E5%B0%B1%E6%98%AF%E5%B0%86%E9%AB%98%E7%BB%B4%E7%A8%80%E7%96%8F%E7%9A%84%E8%BE%93%E5%85%A5%E6%98%A0%E5%B0%84%E5%88%B0%E4%BA%86%E4%B8%80%E4%B8%AA%E4%BD%8E%E7%BB%B4%E7%A8%A0%E5%AF%86%E7%9A%84%E4%B8%80%E4%B8%AA%E5%90%91%E9%87%8F%E4%B8%8A%EF%BC%8C%E5%BD%93%E6%88%91%E4%BB%AC%E7%A8%80%E7%96%8F%E7%9A%84%E7%89%B9%E5%BE%81%E5%8F%98%E5%BE%97%E5%BE%88%E5%A4%A7%EF%BC%8C%E5%A4%A7%E5%88%B0%E5%87%A0%E5%8D%81%E4%BA%BF%E7%94%9A%E8%87%B3%E6%98%AF%E5%87%A0%E7%99%BE%E4%BA%BF%E4%B8%8A%E5%8D%83%E4%BA%BF%E8%A7%84%E6%A8%A1%E4%B9%8B%E5%90%8E%EF%BC%8C%E5%AE%83%E7%9A%84%20embedding%20%E7%9A%84%20table%20%E4%B9%9F%E4%BC%9A%E5%8F%98%E5%BE%97%E5%BE%88%E5%A4%A7%E3%80%82))

- > ![](https://pic1.zhimg.com/v2-c23bd3a621d4b9547f9916cba13a97b8_r.jpg)  ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/509188349#js_content:~:text=https://pic1.zhimg.com/v2-c23bd3a621d4b9547f9916cba13a97b8_r.jpg))

- > 已有的几种并行训练的方法：

第一类是数据并行，例如基于 all-reduce 的 Horovod，这种方式在每个 GPU 卡中存一份完整的模型副本，需要把模型都能存得下  ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/509188349#js_content:~:text=%E5%B7%B2%E6%9C%89%E7%9A%84%E5%87%A0%E7%A7%8D%E5%B9%B6%E8%A1%8C%E8%AE%AD%E7%BB%83%E7%9A%84%E6%96%B9%E6%B3%95%EF%BC%9A%E7%AC%AC%E4%B8%80%E7%B1%BB%E6%98%AF%E6%95%B0%E6%8D%AE%E5%B9%B6%E8%A1%8C%EF%BC%8C%E4%BE%8B%E5%A6%82%E5%9F%BA%E4%BA%8E%20all-reduce%20%E7%9A%84%20Horovod%EF%BC%8C%E8%BF%99%E7%A7%8D%E6%96%B9%E5%BC%8F%E5%9C%A8%E6%AF%8F%E4%B8%AA%20GPU%20%E5%8D%A1%E4%B8%AD%E5%AD%98%E4%B8%80%E4%BB%BD%E5%AE%8C%E6%95%B4%E7%9A%84%E6%A8%A1%E5%9E%8B%E5%89%AF%E6%9C%AC%EF%BC%8C%E9%9C%80%E8%A6%81%E6%8A%8A%E6%A8%A1%E5%9E%8B%E9%83%BD%E8%83%BD%E5%AD%98%E5%BE%97%E4%B8%8B))

- > 第二类是 NVIDIA 提出的，之前他们的方案还是一个多卡切分的方案，但现在已经支持了一个 CPU 的 embedding 的一个存储，他们这个方法把 embedding 切成多份，然后在每个卡的显存里面存一部分，MLP 在每个节点都存一个完整的模型。embeding 通过一个 all to all 的通信， MLP 通过 all-reduce 通信  ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/509188349#js_content:~:text=%E7%AC%AC%E4%BA%8C%E7%B1%BB%E6%98%AF%20NVIDIA%20%E6%8F%90%E5%87%BA%E7%9A%84%EF%BC%8C%E4%B9%8B%E5%89%8D%E4%BB%96%E4%BB%AC%E7%9A%84%E6%96%B9%E6%A1%88%E8%BF%98%E6%98%AF%E4%B8%80%E4%B8%AA%E5%A4%9A%E5%8D%A1%E5%88%87%E5%88%86%E7%9A%84%E6%96%B9%E6%A1%88%EF%BC%8C%E4%BD%86%E7%8E%B0%E5%9C%A8%E5%B7%B2%E7%BB%8F%E6%94%AF%E6%8C%81%E4%BA%86%E4%B8%80%E4%B8%AA%20CPU%20%E7%9A%84%20embedding%20%E7%9A%84%E4%B8%80%E4%B8%AA%E5%AD%98%E5%82%A8%EF%BC%8C%E4%BB%96%E4%BB%AC%E8%BF%99%E4%B8%AA%E6%96%B9%E6%B3%95%E6%8A%8A%20embedding%20%E5%88%87%E6%88%90%E5%A4%9A%E4%BB%BD%EF%BC%8C%E7%84%B6%E5%90%8E%E5%9C%A8%E6%AF%8F%E4%B8%AA%E5%8D%A1%E7%9A%84%E6%98%BE%E5%AD%98%E9%87%8C%E9%9D%A2%E5%AD%98%E4%B8%80%E9%83%A8%E5%88%86%EF%BC%8CMLP%20%E5%9C%A8%E6%AF%8F%E4%B8%AA%E8%8A%82%E7%82%B9%E9%83%BD%E5%AD%98%E4%B8%80%E4%B8%AA%E5%AE%8C%E6%95%B4%E7%9A%84%E6%A8%A1%E5%9E%8B%E3%80%82embeding%20%E9%80%9A%E8%BF%87%E4%B8%80%E4%B8%AA%20all%20to%20all%20%E7%9A%84%E9%80%9A%E4%BF%A1%EF%BC%8C%20MLP%20%E9%80%9A%E8%BF%87%20all-reduce%20%E9%80%9A%E4%BF%A1))

- > 第三类方法是使用 CPU 的内存来存 embedding，然后用 GPU 来存 MLP。CPU 负责存储，MLP 来负责前项以及反向的梯度的计算。对于这种方法，如果我们采用同步训练的话，它有一个问题就是因为 embedding 是存在 CPU 侧的，需要从 CPU 去传输到 GPU，梯度需要从 GPU 回传到 CPU，他们之间通信的时延是很高的。  ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/509188349#js_content:~:text=%E7%AC%AC%E4%B8%89%E7%B1%BB%E6%96%B9%E6%B3%95%E6%98%AF%E4%BD%BF%E7%94%A8%20CPU%20%E7%9A%84%E5%86%85%E5%AD%98%E6%9D%A5%E5%AD%98%20embedding%EF%BC%8C%E7%84%B6%E5%90%8E%E7%94%A8%20GPU%20%E6%9D%A5%E5%AD%98%20MLP%E3%80%82CPU%20%E8%B4%9F%E8%B4%A3%E5%AD%98%E5%82%A8%EF%BC%8CMLP%20%E6%9D%A5%E8%B4%9F%E8%B4%A3%E5%89%8D%E9%A1%B9%E4%BB%A5%E5%8F%8A%E5%8F%8D%E5%90%91%E7%9A%84%E6%A2%AF%E5%BA%A6%E7%9A%84%E8%AE%A1%E7%AE%97%E3%80%82%E5%AF%B9%E4%BA%8E%E8%BF%99%E7%A7%8D%E6%96%B9%E6%B3%95%EF%BC%8C%E5%A6%82%E6%9E%9C%E6%88%91%E4%BB%AC%E9%87%87%E7%94%A8%E5%90%8C%E6%AD%A5%E8%AE%AD%E7%BB%83%E7%9A%84%E8%AF%9D%EF%BC%8C%E5%AE%83%E6%9C%89%E4%B8%80%E4%B8%AA%E9%97%AE%E9%A2%98%E5%B0%B1%E6%98%AF%E5%9B%A0%E4%B8%BA%20embedding%20%E6%98%AF%E5%AD%98%E5%9C%A8%20CPU%20%E4%BE%A7%E7%9A%84%EF%BC%8C%E9%9C%80%E8%A6%81%E4%BB%8E%20CPU%20%E5%8E%BB%E4%BC%A0%E8%BE%93%E5%88%B0%20GPU%EF%BC%8C%E6%A2%AF%E5%BA%A6%E9%9C%80%E8%A6%81%E4%BB%8E%20GPU%20%E5%9B%9E%E4%BC%A0%E5%88%B0%20CPU%EF%BC%8C%E4%BB%96%E4%BB%AC%E4%B9%8B%E9%97%B4%E9%80%9A%E4%BF%A1%E7%9A%84%E6%97%B6%E5%BB%B6%E6%98%AF%E5%BE%88%E9%AB%98%E7%9A%84%E3%80%82))

- > ![](https://pic1.zhimg.com/v2-bc352c3504bdeb818fb7cb67669fd490_r.jpg)  ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/509188349#js_content:~:text=https://pic1.zhimg.com/v2-bc352c3504bdeb818fb7cb67669fd490_r.jpg))

- > ScalefreeCTR，这个框架分成三个部分，第一个部分是有一个 Host manager，它是用来负责 embedding 单元以及缓存的一个维护，另一个模块的话是 dataloader，负责将数据从硬盘读到内存，以及做一些去重之类的操作，第三个部分是 GPU worker，它负责从缓存里面去把对应的 embedding 取到，然后去做一个前向计算以及反向的训练，然后再将梯度更新到缓存的 embedding 中。这里由 host-manager 来负责 embedding 参数的下发，GPU 缓存的维护，以及 embedding 参数的更新，因为有了缓存，所以我们可以做到数据读取，参数下发以及模型训练的三阶段的流水，尽可能的提升了资源的利用效率，从而提升了最终的吞吐。  ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/509188349#js_content:~:text=ScalefreeCTR%EF%BC%8C%E8%BF%99%E4%B8%AA%E6%A1%86%E6%9E%B6%E5%88%86%E6%88%90%E4%B8%89%E4%B8%AA%E9%83%A8%E5%88%86%EF%BC%8C%E7%AC%AC%E4%B8%80%E4%B8%AA%E9%83%A8%E5%88%86%E6%98%AF%E6%9C%89%E4%B8%80%E4%B8%AA%20Host%20manager%EF%BC%8C%E5%AE%83%E6%98%AF%E7%94%A8%E6%9D%A5%E8%B4%9F%E8%B4%A3%20embedding%20%E5%8D%95%E5%85%83%E4%BB%A5%E5%8F%8A%E7%BC%93%E5%AD%98%E7%9A%84%E4%B8%80%E4%B8%AA%E7%BB%B4%E6%8A%A4%EF%BC%8C%E5%8F%A6%E4%B8%80%E4%B8%AA%E6%A8%A1%E5%9D%97%E7%9A%84%E8%AF%9D%E6%98%AF%20dataloader%EF%BC%8C%E8%B4%9F%E8%B4%A3%E5%B0%86%E6%95%B0%E6%8D%AE%E4%BB%8E%E7%A1%AC%E7%9B%98%E8%AF%BB%E5%88%B0%E5%86%85%E5%AD%98%EF%BC%8C%E4%BB%A5%E5%8F%8A%E5%81%9A%E4%B8%80%E4%BA%9B%E5%8E%BB%E9%87%8D%E4%B9%8B%E7%B1%BB%E7%9A%84%E6%93%8D%E4%BD%9C%EF%BC%8C%E7%AC%AC%E4%B8%89%E4%B8%AA%E9%83%A8%E5%88%86%E6%98%AF%20GPU%20worker%EF%BC%8C%E5%AE%83%E8%B4%9F%E8%B4%A3%E4%BB%8E%E7%BC%93%E5%AD%98%E9%87%8C%E9%9D%A2%E5%8E%BB%E6%8A%8A%E5%AF%B9%E5%BA%94%E7%9A%84%20embedding%20%E5%8F%96%E5%88%B0%EF%BC%8C%E7%84%B6%E5%90%8E%E5%8E%BB%E5%81%9A%E4%B8%80%E4%B8%AA%E5%89%8D%E5%90%91%E8%AE%A1%E7%AE%97%E4%BB%A5%E5%8F%8A%E5%8F%8D%E5%90%91%E7%9A%84%E8%AE%AD%E7%BB%83%EF%BC%8C%E7%84%B6%E5%90%8E%E5%86%8D%E5%B0%86%E6%A2%AF%E5%BA%A6%E6%9B%B4%E6%96%B0%E5%88%B0%E7%BC%93%E5%AD%98%E7%9A%84%20embedding%20%E4%B8%AD%E3%80%82%E8%BF%99%E9%87%8C%E7%94%B1%20host-manager%20%E6%9D%A5%E8%B4%9F%E8%B4%A3%20embedding%20%E5%8F%82%E6%95%B0%E7%9A%84%E4%B8%8B%E5%8F%91%EF%BC%8CGPU%20%E7%BC%93%E5%AD%98%E7%9A%84%E7%BB%B4%E6%8A%A4%EF%BC%8C%E4%BB%A5%E5%8F%8A%20embedding%20%E5%8F%82%E6%95%B0%E7%9A%84%E6%9B%B4%E6%96%B0%EF%BC%8C%E5%9B%A0%E4%B8%BA%E6%9C%89%E4%BA%86%E7%BC%93%E5%AD%98%EF%BC%8C%E6%89%80%E4%BB%A5%E6%88%91%E4%BB%AC%E5%8F%AF%E4%BB%A5%E5%81%9A%E5%88%B0%E6%95%B0%E6%8D%AE%E8%AF%BB%E5%8F%96%EF%BC%8C%E5%8F%82%E6%95%B0%E4%B8%8B%E5%8F%91%E4%BB%A5%E5%8F%8A%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E7%9A%84%E4%B8%89%E9%98%B6%E6%AE%B5%E7%9A%84%E6%B5%81%E6%B0%B4%EF%BC%8C%E5%B0%BD%E5%8F%AF%E8%83%BD%E7%9A%84%E6%8F%90%E5%8D%87%E4%BA%86%E8%B5%84%E6%BA%90%E7%9A%84%E5%88%A9%E7%94%A8%E6%95%88%E7%8E%87%EF%BC%8C%E4%BB%8E%E8%80%8C%E6%8F%90%E5%8D%87%E4%BA%86%E6%9C%80%E7%BB%88%E7%9A%84%E5%90%9E%E5%90%90%E3%80%82))
  - 📝 是对上述第三类方法——CPU 存储 embedding 的优化。

