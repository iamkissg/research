title:: 【Reading Highlights】特征工程完全手册 - 从预处理、构造、选择、降维、不平衡处理，到放弃
source:: https://zhuanlan.zhihu.com/p/94994902
summary:: 
tags:: [[简悦]] [[特征工程]]   [[reading_highlights]]
date:: 20220529  

- > 特征构造主要是产生衍生变量，所谓衍生变量是指对原始数据进行加工、特征组合，生成有商业意义的新变量 (新特征)  ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/94994902#js_content:~:text=%E7%89%B9%E5%BE%81%E6%9E%84%E9%80%A0%E4%B8%BB%E8%A6%81%E6%98%AF%E4%BA%A7%E7%94%9F%E8%A1%8D%E7%94%9F%E5%8F%98%E9%87%8F%EF%BC%8C%E6%89%80%E8%B0%93%E8%A1%8D%E7%94%9F%E5%8F%98%E9%87%8F%E6%98%AF%E6%8C%87%E5%AF%B9%E5%8E%9F%E5%A7%8B%E6%95%B0%E6%8D%AE%E8%BF%9B%E8%A1%8C%E5%8A%A0%E5%B7%A5%E3%80%81%E7%89%B9%E5%BE%81%E7%BB%84%E5%90%88%EF%BC%8C%E7%94%9F%E6%88%90%E6%9C%89%E5%95%86%E4%B8%9A%E6%84%8F%E4%B9%89%E7%9A%84%E6%96%B0%E5%8F%98%E9%87%8F%20(%E6%96%B0%E7%89%B9%E5%BE%81)))

- > 新特征设计应与目标高度相关，要考虑的问题：

1.  这个特征是否对目标有实际意义？
2.  如果有用，这个特征重要性如何？
3.  这个特征的信息是否在其他特征上体现过？  ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/94994902#js_content:~:text=%E6%96%B0%E7%89%B9%E5%BE%81%E8%AE%BE%E8%AE%A1%E5%BA%94%E4%B8%8E%E7%9B%AE%E6%A0%87%E9%AB%98%E5%BA%A6%E7%9B%B8%E5%85%B3%EF%BC%8C%E8%A6%81%E8%80%83%E8%99%91%E7%9A%84%E9%97%AE%E9%A2%98%EF%BC%9A%E8%BF%99%E4%B8%AA%E7%89%B9%E5%BE%81%E6%98%AF%E5%90%A6%E5%AF%B9%E7%9B%AE%E6%A0%87%E6%9C%89%E5%AE%9E%E9%99%85%E6%84%8F%E4%B9%89%EF%BC%9F%E5%A6%82%E6%9E%9C%E6%9C%89%E7%94%A8%EF%BC%8C%E8%BF%99%E4%B8%AA%E7%89%B9%E5%BE%81%E9%87%8D%E8%A6%81%E6%80%A7%E5%A6%82%E4%BD%95%EF%BC%9F%E8%BF%99%E4%B8%AA%E7%89%B9%E5%BE%81%E7%9A%84%E4%BF%A1%E6%81%AF%E6%98%AF%E5%90%A6%E5%9C%A8%E5%85%B6%E4%BB%96%E7%89%B9%E5%BE%81%E4%B8%8A%E4%BD%93%E7%8E%B0%E8%BF%87%EF%BC%9F))
  - 📝 构造新特征的考虑点

- > ### **统计值构造法**

指通过统计单个或者多个变量的统计值 (max,min,count,mean) 等而形成新的特征。  ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/94994902#js_content:~:text=%E7%BB%9F%E8%AE%A1%E5%80%BC%E6%9E%84%E9%80%A0%E6%B3%95%E6%8C%87%E9%80%9A%E8%BF%87%E7%BB%9F%E8%AE%A1%E5%8D%95%E4%B8%AA%E6%88%96%E8%80%85%E5%A4%9A%E4%B8%AA%E5%8F%98%E9%87%8F%E7%9A%84%E7%BB%9F%E8%AE%A1%E5%80%BC%20(max,min,count,mean)%20%E7%AD%89%E8%80%8C%E5%BD%A2%E6%88%90%E6%96%B0%E7%9A%84%E7%89%B9%E5%BE%81%E3%80%82))

- > 分箱法：等宽 (宽度) 分箱法、等频 (频数) 分箱法 聚类划分：使用聚类算法将数据聚成几类，每一个类为一个划分  ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/94994902#js_content:~:text=%E5%88%86%E7%AE%B1%E6%B3%95%EF%BC%9A%E7%AD%89%E5%AE%BD%20(%E5%AE%BD%E5%BA%A6)%20%E5%88%86%E7%AE%B1%E6%B3%95%E3%80%81%E7%AD%89%E9%A2%91%20(%E9%A2%91%E6%95%B0)%20%E5%88%86%E7%AE%B1%E6%B3%95%20%E8%81%9A%E7%B1%BB%E5%88%92%E5%88%86%EF%BC%9A%E4%BD%BF%E7%94%A8%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%E5%B0%86%E6%95%B0%E6%8D%AE%E8%81%9A%E6%88%90%E5%87%A0%E7%B1%BB%EF%BC%8C%E6%AF%8F%E4%B8%80%E4%B8%AA%E7%B1%BB%E4%B8%BA%E4%B8%80%E4%B8%AA%E5%88%92%E5%88%86))
  - 📝 常见的特征分桶方法

- > 简单常用的函数变换法 (一般针对于连续数据)：平方(小数值—> 大数值)、开平方 (大数值—> 小数值)、指数、对数、差分 [[特征工程]]   ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/94994902#js_content:~:text=%E7%AE%80%E5%8D%95%E5%B8%B8%E7%94%A8%E7%9A%84%E5%87%BD%E6%95%B0%E5%8F%98%E6%8D%A2%E6%B3%95%20(%E4%B8%80%E8%88%AC%E9%92%88%E5%AF%B9%E4%BA%8E%E8%BF%9E%E7%BB%AD%E6%95%B0%E6%8D%AE)%EF%BC%9A%E5%B9%B3%E6%96%B9(%E5%B0%8F%E6%95%B0%E5%80%BC%E2%80%94%3E%20%E5%A4%A7%E6%95%B0%E5%80%BC)%E3%80%81%E5%BC%80%E5%B9%B3%E6%96%B9%20(%E5%A4%A7%E6%95%B0%E5%80%BC%E2%80%94%3E%20%E5%B0%8F%E6%95%B0%E5%80%BC)%E3%80%81%E6%8C%87%E6%95%B0%E3%80%81%E5%AF%B9%E6%95%B0%E3%80%81%E5%B7%AE%E5%88%86))
  - 📝 对数值特征的常用函数变换

- > 如果一个特征不发散，例如方差接近于 0，也就是说样本在这个特征上基本上没有差异，这个特征对于样本的区分并没有什么用。  ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/94994902#js_content:~:text=%E5%A6%82%E6%9E%9C%E4%B8%80%E4%B8%AA%E7%89%B9%E5%BE%81%E4%B8%8D%E5%8F%91%E6%95%A3%EF%BC%8C%E4%BE%8B%E5%A6%82%E6%96%B9%E5%B7%AE%E6%8E%A5%E8%BF%91%E4%BA%8E%200%EF%BC%8C%E4%B9%9F%E5%B0%B1%E6%98%AF%E8%AF%B4%E6%A0%B7%E6%9C%AC%E5%9C%A8%E8%BF%99%E4%B8%AA%E7%89%B9%E5%BE%81%E4%B8%8A%E5%9F%BA%E6%9C%AC%E4%B8%8A%E6%B2%A1%E6%9C%89%E5%B7%AE%E5%BC%82%EF%BC%8C%E8%BF%99%E4%B8%AA%E7%89%B9%E5%BE%81%E5%AF%B9%E4%BA%8E%E6%A0%B7%E6%9C%AC%E7%9A%84%E5%8C%BA%E5%88%86%E5%B9%B6%E6%B2%A1%E6%9C%89%E4%BB%80%E4%B9%88%E7%94%A8%E3%80%82))
  - 📝 不发散的特征，没有区分性

- > 与目标相关性高的特征，应当优先选择。  ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/94994902#js_content:~:text=%E4%B8%8E%E7%9B%AE%E6%A0%87%E7%9B%B8%E5%85%B3%E6%80%A7%E9%AB%98%E7%9A%84%E7%89%B9%E5%BE%81%EF%BC%8C%E5%BA%94%E5%BD%93%E4%BC%98%E5%85%88%E9%80%89%E6%8B%A9%E3%80%82))

- > **为什么要进行特征选择？**

1.  减轻维数灾难问题; 2. 降低学习任务的难度  ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/94994902#js_content:~:text=%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E8%BF%9B%E8%A1%8C%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%EF%BC%9F%E5%87%8F%E8%BD%BB%E7%BB%B4%E6%95%B0%E7%81%BE%E9%9A%BE%E9%97%AE%E9%A2%98;%202.%20%E9%99%8D%E4%BD%8E%E5%AD%A6%E4%B9%A0%E4%BB%BB%E5%8A%A1%E7%9A%84%E9%9A%BE%E5%BA%A6))
  - 📝 [[特征选择]] 两个目的：1、降低维数灾难的风险；2、降低任务的学习难度

- > **特征选择有哪些方法呢？**

1.  Filter 过滤法
2.  Wrapper 包装法
3.  Embedded 嵌入法  ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/94994902#js_content:~:text=%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%E6%9C%89%E5%93%AA%E4%BA%9B%E6%96%B9%E6%B3%95%E5%91%A2%EF%BC%9FFilter%20%E8%BF%87%E6%BB%A4%E6%B3%95Wrapper%20%E5%8C%85%E8%A3%85%E6%B3%95Embedded%20%E5%B5%8C%E5%85%A5%E6%B3%95))

- > **Filter 过滤法**
--------------

它主要侧重于**单个特征**跟**目标变量**的相关性。

优点是计算时间上较高效, 对于过拟合问题也具有较高的鲁棒性。  ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/94994902#js_content:~:text=Filter%20%E8%BF%87%E6%BB%A4%E6%B3%95%E5%8D%95%E4%B8%AA%E7%89%B9%E5%BE%81%E7%9B%AE%E6%A0%87%E5%8F%98%E9%87%8F%E5%AE%83%E4%B8%BB%E8%A6%81%E4%BE%A7%E9%87%8D%E4%BA%8E%E8%B7%9F%E7%9A%84%E7%9B%B8%E5%85%B3%E6%80%A7%E3%80%82%E4%BC%98%E7%82%B9%E6%98%AF%E8%AE%A1%E7%AE%97%E6%97%B6%E9%97%B4%E4%B8%8A%E8%BE%83%E9%AB%98%E6%95%88,%20%E5%AF%B9%E4%BA%8E%E8%BF%87%E6%8B%9F%E5%90%88%E9%97%AE%E9%A2%98%E4%B9%9F%E5%85%B7%E6%9C%89%E8%BE%83%E9%AB%98%E7%9A%84%E9%B2%81%E6%A3%92%E6%80%A7%E3%80%82))

- > 变量的方差越大，我们就可以认为它的离散程度越大，也就是意味着这个变量对模型的贡献和作用会更明显，因此要保留方差较大的变量，反之，要剔除掉无意义的特征。  ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/94994902#js_content:~:text=%E5%8F%98%E9%87%8F%E7%9A%84%E6%96%B9%E5%B7%AE%E8%B6%8A%E5%A4%A7%EF%BC%8C%E6%88%91%E4%BB%AC%E5%B0%B1%E5%8F%AF%E4%BB%A5%E8%AE%A4%E4%B8%BA%E5%AE%83%E7%9A%84%E7%A6%BB%E6%95%A3%E7%A8%8B%E5%BA%A6%E8%B6%8A%E5%A4%A7%EF%BC%8C%E4%B9%9F%E5%B0%B1%E6%98%AF%E6%84%8F%E5%91%B3%E7%9D%80%E8%BF%99%E4%B8%AA%E5%8F%98%E9%87%8F%E5%AF%B9%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%B4%A1%E7%8C%AE%E5%92%8C%E4%BD%9C%E7%94%A8%E4%BC%9A%E6%9B%B4%E6%98%8E%E6%98%BE%EF%BC%8C%E5%9B%A0%E6%AD%A4%E8%A6%81%E4%BF%9D%E7%95%99%E6%96%B9%E5%B7%AE%E8%BE%83%E5%A4%A7%E7%9A%84%E5%8F%98%E9%87%8F%EF%BC%8C%E5%8F%8D%E4%B9%8B%EF%BC%8C%E8%A6%81%E5%89%94%E9%99%A4%E6%8E%89%E6%97%A0%E6%84%8F%E4%B9%89%E7%9A%84%E7%89%B9%E5%BE%81%E3%80%82))

- > 只是根据特征与特征之间的相关度来筛选特征，但并没有结合与目标的相关度来衡量  ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/94994902#js_content:~:text=%E5%8F%AA%E6%98%AF%E6%A0%B9%E6%8D%AE%E7%89%B9%E5%BE%81%E4%B8%8E%E7%89%B9%E5%BE%81%E4%B9%8B%E9%97%B4%E7%9A%84%E7%9B%B8%E5%85%B3%E5%BA%A6%E6%9D%A5%E7%AD%9B%E9%80%89%E7%89%B9%E5%BE%81%EF%BC%8C%E4%BD%86%E5%B9%B6%E6%B2%A1%E6%9C%89%E7%BB%93%E5%90%88%E4%B8%8E%E7%9B%AE%E6%A0%87%E7%9A%84%E7%9B%B8%E5%85%B3%E5%BA%A6%E6%9D%A5%E8%A1%A1%E9%87%8F))
  - 📝 根据特征间相关性进行[[特征选择]]（剔除）的缺点是，没有考虑到特征与目标的相关度

- > 相关性的强度确实是用相关系数的大小来衡量的，但相关大小的评价要以相关系数显著性的评价为前提

因此，要先检验相关系数的显著性，如果显著，证明相关系数有统计学意义，下一步再来看相关系数大小；  ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/94994902#js_content:~:text=%E7%9B%B8%E5%85%B3%E6%80%A7%E7%9A%84%E5%BC%BA%E5%BA%A6%E7%A1%AE%E5%AE%9E%E6%98%AF%E7%94%A8%E7%9B%B8%E5%85%B3%E7%B3%BB%E6%95%B0%E7%9A%84%E5%A4%A7%E5%B0%8F%E6%9D%A5%E8%A1%A1%E9%87%8F%E7%9A%84%EF%BC%8C%E4%BD%86%E7%9B%B8%E5%85%B3%E5%A4%A7%E5%B0%8F%E7%9A%84%E8%AF%84%E4%BB%B7%E8%A6%81%E4%BB%A5%E7%9B%B8%E5%85%B3%E7%B3%BB%E6%95%B0%E6%98%BE%E8%91%97%E6%80%A7%E7%9A%84%E8%AF%84%E4%BB%B7%E4%B8%BA%E5%89%8D%E6%8F%90%E5%9B%A0%E6%AD%A4%EF%BC%8C%E8%A6%81%E5%85%88%E6%A3%80%E9%AA%8C%E7%9B%B8%E5%85%B3%E7%B3%BB%E6%95%B0%E7%9A%84%E6%98%BE%E8%91%97%E6%80%A7%EF%BC%8C%E5%A6%82%E6%9E%9C%E6%98%BE%E8%91%97%EF%BC%8C%E8%AF%81%E6%98%8E%E7%9B%B8%E5%85%B3%E7%B3%BB%E6%95%B0%E6%9C%89%E7%BB%9F%E8%AE%A1%E5%AD%A6%E6%84%8F%E4%B9%89%EF%BC%8C%E4%B8%8B%E4%B8%80%E6%AD%A5%E5%86%8D%E6%9D%A5%E7%9C%8B%E7%9B%B8%E5%85%B3%E7%B3%BB%E6%95%B0%E5%A4%A7%E5%B0%8F%EF%BC%9B))
  - 📝 查看特征相关性的时候，需要结合相关系数的显著性水平来评价

- > 封装器用选取的**特征子集**对**样本 (标签) 集**进行训练学习，**训练的精度 (准确率) 作为衡量特征子集好坏的标准**, 经过比较选出最好的特征子集。  
常用的有逐步回归（Stepwise regression）、向前选择（Forward selection）和向后选择（Backward selection）。  ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/94994902#js_content:~:text=%E7%89%B9%E5%BE%81%E5%AD%90%E9%9B%86%E6%A0%B7%E6%9C%AC%20(%E6%A0%87%E7%AD%BE)%20%E9%9B%86%E8%AE%AD%E7%BB%83%E7%9A%84%E7%B2%BE%E5%BA%A6%20(%E5%87%86%E7%A1%AE%E7%8E%87)%20%E4%BD%9C%E4%B8%BA%E8%A1%A1%E9%87%8F%E7%89%B9%E5%BE%81%E5%AD%90%E9%9B%86%E5%A5%BD%E5%9D%8F%E7%9A%84%E6%A0%87%E5%87%86%E5%B0%81%E8%A3%85%E5%99%A8%E7%94%A8%E9%80%89%E5%8F%96%E7%9A%84%E5%AF%B9%E8%BF%9B%E8%A1%8C%E8%AE%AD%E7%BB%83%E5%AD%A6%E4%B9%A0%EF%BC%8C,%20%E7%BB%8F%E8%BF%87%E6%AF%94%E8%BE%83%E9%80%89%E5%87%BA%E6%9C%80%E5%A5%BD%E7%9A%84%E7%89%B9%E5%BE%81%E5%AD%90%E9%9B%86%E3%80%82%E5%B8%B8%E7%94%A8%E7%9A%84%E6%9C%89%E9%80%90%E6%AD%A5%E5%9B%9E%E5%BD%92%EF%BC%88Stepwise%20regression%EF%BC%89%E3%80%81%E5%90%91%E5%89%8D%E9%80%89%E6%8B%A9%EF%BC%88Forward%20selection%EF%BC%89%E5%92%8C%E5%90%91%E5%90%8E%E9%80%89%E6%8B%A9%EF%BC%88Backward%20selection%EF%BC%89%E3%80%82))
  - 📝 wrapper 方法挑选了一个特征子集进行学习，既考虑了特征之间的关系，又考虑到特征与目标的关系。

- > 稳定性选择是一种基于**二次抽样和选择算法 (训练模型)** 相结合的方法，选择算法可以是回归、分类 SVM 或者类似算法。

**原理实现**

在不同的特征子集上运行训练模型，不断地重复，最终汇总特征选择的结果。比如可以统计某个特征被认为是重要特征的频率 （被选为重要特征的次数除以它所在的子集被测试的次数）。理想情况下，重要特征的得分会接近 100%。稍微弱一点的特征得分会是非 0 的数， 而最无用的特征得分将会接近于 0。  ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/94994902#js_content:~:text=%E4%BA%8C%E6%AC%A1%E6%8A%BD%E6%A0%B7%E5%92%8C%E9%80%89%E6%8B%A9%E7%AE%97%E6%B3%95%20(%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B)%E7%A8%B3%E5%AE%9A%E6%80%A7%E9%80%89%E6%8B%A9%E6%98%AF%E4%B8%80%E7%A7%8D%E5%9F%BA%E4%BA%8E%20%E7%9B%B8%E7%BB%93%E5%90%88%E7%9A%84%E6%96%B9%E6%B3%95%EF%BC%8C%E9%80%89%E6%8B%A9%E7%AE%97%E6%B3%95%E5%8F%AF%E4%BB%A5%E6%98%AF%E5%9B%9E%E5%BD%92%E3%80%81%E5%88%86%E7%B1%BB%20SVM%20%E6%88%96%E8%80%85%E7%B1%BB%E4%BC%BC%E7%AE%97%E6%B3%95%E3%80%82%E5%8E%9F%E7%90%86%E5%AE%9E%E7%8E%B0%E5%9C%A8%E4%B8%8D%E5%90%8C%E7%9A%84%E7%89%B9%E5%BE%81%E5%AD%90%E9%9B%86%E4%B8%8A%E8%BF%90%E8%A1%8C%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%EF%BC%8C%E4%B8%8D%E6%96%AD%E5%9C%B0%E9%87%8D%E5%A4%8D%EF%BC%8C%E6%9C%80%E7%BB%88%E6%B1%87%E6%80%BB%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%E7%9A%84%E7%BB%93%E6%9E%9C%E3%80%82%E6%AF%94%E5%A6%82%E5%8F%AF%E4%BB%A5%E7%BB%9F%E8%AE%A1%E6%9F%90%E4%B8%AA%E7%89%B9%E5%BE%81%E8%A2%AB%E8%AE%A4%E4%B8%BA%E6%98%AF%E9%87%8D%E8%A6%81%E7%89%B9%E5%BE%81%E7%9A%84%E9%A2%91%E7%8E%87%20%EF%BC%88%E8%A2%AB%E9%80%89%E4%B8%BA%E9%87%8D%E8%A6%81%E7%89%B9%E5%BE%81%E7%9A%84%E6%AC%A1%E6%95%B0%E9%99%A4%E4%BB%A5%E5%AE%83%E6%89%80%E5%9C%A8%E7%9A%84%E5%AD%90%E9%9B%86%E8%A2%AB%E6%B5%8B%E8%AF%95%E7%9A%84%E6%AC%A1%E6%95%B0%EF%BC%89%E3%80%82%E7%90%86%E6%83%B3%E6%83%85%E5%86%B5%E4%B8%8B%EF%BC%8C%E9%87%8D%E8%A6%81%E7%89%B9%E5%BE%81%E7%9A%84%E5%BE%97%E5%88%86%E4%BC%9A%E6%8E%A5%E8%BF%91%20100%25%E3%80%82%E7%A8%8D%E5%BE%AE%E5%BC%B1%E4%B8%80%E7%82%B9%E7%9A%84%E7%89%B9%E5%BE%81%E5%BE%97%E5%88%86%E4%BC%9A%E6%98%AF%E9%9D%9E%200%20%E7%9A%84%E6%95%B0%EF%BC%8C%20%E8%80%8C%E6%9C%80%E6%97%A0%E7%94%A8%E7%9A%84%E7%89%B9%E5%BE%81%E5%BE%97%E5%88%86%E5%B0%86%E4%BC%9A%E6%8E%A5%E8%BF%91%E4%BA%8E%200%E3%80%82))
  - 📝 稳定性选择：多次执行 wrapper 特征选择方法，考虑某个特征在所有包含它与所有不包含它的实验中的有效性：总是有效、偶然有效、完全无效。

- > 在许多数据集和环境下，稳定性选择往往是性能最好的方法之一。  ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/94994902#js_content:~:text=%E5%9C%A8%E8%AE%B8%E5%A4%9A%E6%95%B0%E6%8D%AE%E9%9B%86%E5%92%8C%E7%8E%AF%E5%A2%83%E4%B8%8B%EF%BC%8C%E7%A8%B3%E5%AE%9A%E6%80%A7%E9%80%89%E6%8B%A9%E5%BE%80%E5%BE%80%E6%98%AF%E6%80%A7%E8%83%BD%E6%9C%80%E5%A5%BD%E7%9A%84%E6%96%B9%E6%B3%95%E4%B9%8B%E4%B8%80%E3%80%82))

- > RFE 算法的主要思想就是使用一个基模型（这里是 S 模型 VM）来进行多轮训练，  
每轮训练后，根据每个特征的系数对特征打分，去掉得分最小的特征，  
然后用剩余的特征构建新的特征集，进行下一轮训练，直到所有的特征都遍历了。

这个过程中特征被消除的次序就是特征的排序，实际上这是一种寻找**最优特征子集**的贪心算法。  ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/94994902#js_content:~:text=RFE%20%E7%AE%97%E6%B3%95%E7%9A%84%E4%B8%BB%E8%A6%81%E6%80%9D%E6%83%B3%E5%B0%B1%E6%98%AF%E4%BD%BF%E7%94%A8%E4%B8%80%E4%B8%AA%E5%9F%BA%E6%A8%A1%E5%9E%8B%EF%BC%88%E8%BF%99%E9%87%8C%E6%98%AF%20S%20%E6%A8%A1%E5%9E%8B%20VM%EF%BC%89%E6%9D%A5%E8%BF%9B%E8%A1%8C%E5%A4%9A%E8%BD%AE%E8%AE%AD%E7%BB%83%EF%BC%8C%E6%AF%8F%E8%BD%AE%E8%AE%AD%E7%BB%83%E5%90%8E%EF%BC%8C%E6%A0%B9%E6%8D%AE%E6%AF%8F%E4%B8%AA%E7%89%B9%E5%BE%81%E7%9A%84%E7%B3%BB%E6%95%B0%E5%AF%B9%E7%89%B9%E5%BE%81%E6%89%93%E5%88%86%EF%BC%8C%E5%8E%BB%E6%8E%89%E5%BE%97%E5%88%86%E6%9C%80%E5%B0%8F%E7%9A%84%E7%89%B9%E5%BE%81%EF%BC%8C%E7%84%B6%E5%90%8E%E7%94%A8%E5%89%A9%E4%BD%99%E7%9A%84%E7%89%B9%E5%BE%81%E6%9E%84%E5%BB%BA%E6%96%B0%E7%9A%84%E7%89%B9%E5%BE%81%E9%9B%86%EF%BC%8C%E8%BF%9B%E8%A1%8C%E4%B8%8B%E4%B8%80%E8%BD%AE%E8%AE%AD%E7%BB%83%EF%BC%8C%E7%9B%B4%E5%88%B0%E6%89%80%E6%9C%89%E7%9A%84%E7%89%B9%E5%BE%81%E9%83%BD%E9%81%8D%E5%8E%86%E4%BA%86%E3%80%82%E6%9C%80%E4%BC%98%E7%89%B9%E5%BE%81%E5%AD%90%E9%9B%86%E8%BF%99%E4%B8%AA%E8%BF%87%E7%A8%8B%E4%B8%AD%E7%89%B9%E5%BE%81%E8%A2%AB%E6%B6%88%E9%99%A4%E7%9A%84%E6%AC%A1%E5%BA%8F%E5%B0%B1%E6%98%AF%E7%89%B9%E5%BE%81%E7%9A%84%E6%8E%92%E5%BA%8F%EF%BC%8C%E5%AE%9E%E9%99%85%E4%B8%8A%E8%BF%99%E6%98%AF%E4%B8%80%E7%A7%8D%E5%AF%BB%E6%89%BE%E7%9A%84%E8%B4%AA%E5%BF%83%E7%AE%97%E6%B3%95%E3%80%82))
  - 📝 递归特征削除：不断递归执行算法，剔除最差的特征
  - 📝 属于贪心算法。

- > 理论上来讲，如果某个特征进行排序或者打乱之后，会很明显的影响 (无论正向影响还是负向影响) 到模型 (预测评分) 效果评分，  
那么可以说明这个特征对模型来说是重要的；反之，说明这个特征存不存在并不会影响到模型的效能。  ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/94994902#js_content:~:text=%E7%90%86%E8%AE%BA%E4%B8%8A%E6%9D%A5%E8%AE%B2%EF%BC%8C%E5%A6%82%E6%9E%9C%E6%9F%90%E4%B8%AA%E7%89%B9%E5%BE%81%E8%BF%9B%E8%A1%8C%E6%8E%92%E5%BA%8F%E6%88%96%E8%80%85%E6%89%93%E4%B9%B1%E4%B9%8B%E5%90%8E%EF%BC%8C%E4%BC%9A%E5%BE%88%E6%98%8E%E6%98%BE%E7%9A%84%E5%BD%B1%E5%93%8D%20(%E6%97%A0%E8%AE%BA%E6%AD%A3%E5%90%91%E5%BD%B1%E5%93%8D%E8%BF%98%E6%98%AF%E8%B4%9F%E5%90%91%E5%BD%B1%E5%93%8D)%20%E5%88%B0%E6%A8%A1%E5%9E%8B%20(%E9%A2%84%E6%B5%8B%E8%AF%84%E5%88%86)%20%E6%95%88%E6%9E%9C%E8%AF%84%E5%88%86%EF%BC%8C%E9%82%A3%E4%B9%88%E5%8F%AF%E4%BB%A5%E8%AF%B4%E6%98%8E%E8%BF%99%E4%B8%AA%E7%89%B9%E5%BE%81%E5%AF%B9%E6%A8%A1%E5%9E%8B%E6%9D%A5%E8%AF%B4%E6%98%AF%E9%87%8D%E8%A6%81%E7%9A%84%EF%BC%9B%E5%8F%8D%E4%B9%8B%EF%BC%8C%E8%AF%B4%E6%98%8E%E8%BF%99%E4%B8%AA%E7%89%B9%E5%BE%81%E5%AD%98%E4%B8%8D%E5%AD%98%E5%9C%A8%E5%B9%B6%E4%B8%8D%E4%BC%9A%E5%BD%B1%E5%93%8D%E5%88%B0%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%95%88%E8%83%BD%E3%80%82))
  - 📝 特征值排序选择：为某一特征随机选择特征值（打乱顺序），查看对算法的影响
  - 📝 好处是，不会生造样本

- > 在噪音不多的数据上，或者是数据量远远大于特征数的数据上，如果特征之间相对来说是比较独立的，  
那么即便是运用最简单的线性回归模型也一样能取得非常好的效果  ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/94994902#js_content:~:text=%E5%9C%A8%E5%99%AA%E9%9F%B3%E4%B8%8D%E5%A4%9A%E7%9A%84%E6%95%B0%E6%8D%AE%E4%B8%8A%EF%BC%8C%E6%88%96%E8%80%85%E6%98%AF%E6%95%B0%E6%8D%AE%E9%87%8F%E8%BF%9C%E8%BF%9C%E5%A4%A7%E4%BA%8E%E7%89%B9%E5%BE%81%E6%95%B0%E7%9A%84%E6%95%B0%E6%8D%AE%E4%B8%8A%EF%BC%8C%E5%A6%82%E6%9E%9C%E7%89%B9%E5%BE%81%E4%B9%8B%E9%97%B4%E7%9B%B8%E5%AF%B9%E6%9D%A5%E8%AF%B4%E6%98%AF%E6%AF%94%E8%BE%83%E7%8B%AC%E7%AB%8B%E7%9A%84%EF%BC%8C%E9%82%A3%E4%B9%88%E5%8D%B3%E4%BE%BF%E6%98%AF%E8%BF%90%E7%94%A8%E6%9C%80%E7%AE%80%E5%8D%95%E7%9A%84%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B%E4%B9%9F%E4%B8%80%E6%A0%B7%E8%83%BD%E5%8F%96%E5%BE%97%E9%9D%9E%E5%B8%B8%E5%A5%BD%E7%9A%84%E6%95%88%E6%9E%9C))

- > L1 正则化 Lasso(least absolute shrinkage and selection operator) 将系数 w 的 l1 范数作为惩罚项加到损失函数上，由于正则项非零，这就迫使那些弱的特征所对应的系数变成 0。因此 L1 正则化往往会使学到的模型很稀疏（系数 w 经常为 0），这个特性使得 L1 正则化成为一种很好的特征选择方法。  ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/94994902#js_content:~:text=L1%20%E6%AD%A3%E5%88%99%E5%8C%96%20Lasso(least%20absolute%20shrinkage%20and%20selection%20operator)%20%E5%B0%86%E7%B3%BB%E6%95%B0%20w%20%E7%9A%84%20l1%20%E8%8C%83%E6%95%B0%E4%BD%9C%E4%B8%BA%E6%83%A9%E7%BD%9A%E9%A1%B9%E5%8A%A0%E5%88%B0%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E4%B8%8A%EF%BC%8C%E7%94%B1%E4%BA%8E%E6%AD%A3%E5%88%99%E9%A1%B9%E9%9D%9E%E9%9B%B6%EF%BC%8C%E8%BF%99%E5%B0%B1%E8%BF%AB%E4%BD%BF%E9%82%A3%E4%BA%9B%E5%BC%B1%E7%9A%84%E7%89%B9%E5%BE%81%E6%89%80%E5%AF%B9%E5%BA%94%E7%9A%84%E7%B3%BB%E6%95%B0%E5%8F%98%E6%88%90%200%E3%80%82%E5%9B%A0%E6%AD%A4%20L1%20%E6%AD%A3%E5%88%99%E5%8C%96%E5%BE%80%E5%BE%80%E4%BC%9A%E4%BD%BF%E5%AD%A6%E5%88%B0%E7%9A%84%E6%A8%A1%E5%9E%8B%E5%BE%88%E7%A8%80%E7%96%8F%EF%BC%88%E7%B3%BB%E6%95%B0%20w%20%E7%BB%8F%E5%B8%B8%E4%B8%BA%200%EF%BC%89%EF%BC%8C%E8%BF%99%E4%B8%AA%E7%89%B9%E6%80%A7%E4%BD%BF%E5%BE%97%20L1%20%E6%AD%A3%E5%88%99%E5%8C%96%E6%88%90%E4%B8%BA%E4%B8%80%E7%A7%8D%E5%BE%88%E5%A5%BD%E7%9A%84%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%E6%96%B9%E6%B3%95%E3%80%82))
  - 📝 L1 正则化

- > L2 正则化同样将系数向量的 L2 范数添加到了损失函数中。由于 L2 惩罚项中系数是二次方的，这使得 L2 和 L1 有着诸多差异，最明显的一点就是，L2 正则化会让系数的取值变得平均。  ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/94994902#js_content:~:text=L2%20%E6%AD%A3%E5%88%99%E5%8C%96%E5%90%8C%E6%A0%B7%E5%B0%86%E7%B3%BB%E6%95%B0%E5%90%91%E9%87%8F%E7%9A%84%20L2%20%E8%8C%83%E6%95%B0%E6%B7%BB%E5%8A%A0%E5%88%B0%E4%BA%86%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E4%B8%AD%E3%80%82%E7%94%B1%E4%BA%8E%20L2%20%E6%83%A9%E7%BD%9A%E9%A1%B9%E4%B8%AD%E7%B3%BB%E6%95%B0%E6%98%AF%E4%BA%8C%E6%AC%A1%E6%96%B9%E7%9A%84%EF%BC%8C%E8%BF%99%E4%BD%BF%E5%BE%97%20L2%20%E5%92%8C%20L1%20%E6%9C%89%E7%9D%80%E8%AF%B8%E5%A4%9A%E5%B7%AE%E5%BC%82%EF%BC%8C%E6%9C%80%E6%98%8E%E6%98%BE%E7%9A%84%E4%B8%80%E7%82%B9%E5%B0%B1%E6%98%AF%EF%BC%8CL2%20%E6%AD%A3%E5%88%99%E5%8C%96%E4%BC%9A%E8%AE%A9%E7%B3%BB%E6%95%B0%E7%9A%84%E5%8F%96%E5%80%BC%E5%8F%98%E5%BE%97%E5%B9%B3%E5%9D%87%E3%80%82))
  - 📝 L2 正则化

- > 随机森林具有准确率高、鲁棒性好、易于使用等优点，随机森林提供了两种特征选择的方法:  
1. 平均不纯度减少  
2. 平均精确率减少  ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/94994902#js_content:~:text=%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%E5%85%B7%E6%9C%89%E5%87%86%E7%A1%AE%E7%8E%87%E9%AB%98%E3%80%81%E9%B2%81%E6%A3%92%E6%80%A7%E5%A5%BD%E3%80%81%E6%98%93%E4%BA%8E%E4%BD%BF%E7%94%A8%E7%AD%89%E4%BC%98%E7%82%B9%EF%BC%8C%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%E6%8F%90%E4%BE%9B%E4%BA%86%E4%B8%A4%E7%A7%8D%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9%E7%9A%84%E6%96%B9%E6%B3%95:1.%20%E5%B9%B3%E5%9D%87%E4%B8%8D%E7%BA%AF%E5%BA%A6%E5%87%8F%E5%B0%912.%20%E5%B9%B3%E5%9D%87%E7%B2%BE%E7%A1%AE%E7%8E%87%E5%87%8F%E5%B0%91))
  - 📝 [[随机森林]] 用于 [[特征选择]] 的优点

