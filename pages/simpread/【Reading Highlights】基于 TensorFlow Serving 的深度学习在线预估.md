title:: 【Reading Highlights】基于 TensorFlow Serving 的深度学习在线预估
source:: https://zhuanlan.zhihu.com/p/46591057
summary:: 
tags:: [[简悦]] [[美团技术]]  [[tensorflow]]  [[tf-serving]]  [[工程优化]]   [[reading_highlights]]
date:: 20220622  

- > 离线数据方面，我们使用 Spark 生成 TensorFlow[5] 原生态的数据格式 tfrecord，加快数据读取。  ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/46591057#js_content:~:text=%E7%A6%BB%E7%BA%BF%E6%95%B0%E6%8D%AE%E6%96%B9%E9%9D%A2%EF%BC%8C%E6%88%91%E4%BB%AC%E4%BD%BF%E7%94%A8%20Spark%20%E7%94%9F%E6%88%90%20TensorFlow%5B5%5D%20%E5%8E%9F%E7%94%9F%E6%80%81%E7%9A%84%E6%95%B0%E6%8D%AE%E6%A0%BC%E5%BC%8F%20tfrecord%EF%BC%8C%E5%8A%A0%E5%BF%AB%E6%95%B0%E6%8D%AE%E8%AF%BB%E5%8F%96%E3%80%82))
  - 📝 离线数据生成：spark->tfrecord

- > 离线训练方面，使用 TensorFlow 同步 + Backup Workers[6] 的分布式框架，解决异步更新延迟和同步更新性能慢的问题。

在分布式 ps 参数分配方面，使用 GreedyLoadBalancing 方式，根据预估参数大小分配参数，取代 Round Robin 取模分配的方法，可以使各个 PS 负载均衡。  ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/46591057#js_content:~:text=%E7%A6%BB%E7%BA%BF%E8%AE%AD%E7%BB%83%E6%96%B9%E9%9D%A2%EF%BC%8C%E4%BD%BF%E7%94%A8%20TensorFlow%20%E5%90%8C%E6%AD%A5%20+%20Backup%20Workers%5B6%5D%20%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E6%A1%86%E6%9E%B6%EF%BC%8C%E8%A7%A3%E5%86%B3%E5%BC%82%E6%AD%A5%E6%9B%B4%E6%96%B0%E5%BB%B6%E8%BF%9F%E5%92%8C%E5%90%8C%E6%AD%A5%E6%9B%B4%E6%96%B0%E6%80%A7%E8%83%BD%E6%85%A2%E7%9A%84%E9%97%AE%E9%A2%98%E3%80%82%E5%9C%A8%E5%88%86%E5%B8%83%E5%BC%8F%20ps%20%E5%8F%82%E6%95%B0%E5%88%86%E9%85%8D%E6%96%B9%E9%9D%A2%EF%BC%8C%E4%BD%BF%E7%94%A8%20GreedyLoadBalancing%20%E6%96%B9%E5%BC%8F%EF%BC%8C%E6%A0%B9%E6%8D%AE%E9%A2%84%E4%BC%B0%E5%8F%82%E6%95%B0%E5%A4%A7%E5%B0%8F%E5%88%86%E9%85%8D%E5%8F%82%E6%95%B0%EF%BC%8C%E5%8F%96%E4%BB%A3%20Round%20Robin%20%E5%8F%96%E6%A8%A1%E5%88%86%E9%85%8D%E7%9A%84%E6%96%B9%E6%B3%95%EF%BC%8C%E5%8F%AF%E4%BB%A5%E4%BD%BF%E5%90%84%E4%B8%AA%20PS%20%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%E3%80%82))
  - 📝 采用 ps 架构的分布式训练

- > 计算设备方面，我们发现只使用 CPU 而不使用 GPU，训练速度会更快，这主要是因为尽管 GPU 计算上性能可能会提升，但是却增加了 CPU 与 GPU 之间数据传输的开销，当模型计算并不太复杂时，使用 CPU 效果会更好些。  ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/46591057#js_content:~:text=%E8%AE%A1%E7%AE%97%E8%AE%BE%E5%A4%87%E6%96%B9%E9%9D%A2%EF%BC%8C%E6%88%91%E4%BB%AC%E5%8F%91%E7%8E%B0%E5%8F%AA%E4%BD%BF%E7%94%A8%20CPU%20%E8%80%8C%E4%B8%8D%E4%BD%BF%E7%94%A8%20GPU%EF%BC%8C%E8%AE%AD%E7%BB%83%E9%80%9F%E5%BA%A6%E4%BC%9A%E6%9B%B4%E5%BF%AB%EF%BC%8C%E8%BF%99%E4%B8%BB%E8%A6%81%E6%98%AF%E5%9B%A0%E4%B8%BA%E5%B0%BD%E7%AE%A1%20GPU%20%E8%AE%A1%E7%AE%97%E4%B8%8A%E6%80%A7%E8%83%BD%E5%8F%AF%E8%83%BD%E4%BC%9A%E6%8F%90%E5%8D%87%EF%BC%8C%E4%BD%86%E6%98%AF%E5%8D%B4%E5%A2%9E%E5%8A%A0%E4%BA%86%20CPU%20%E4%B8%8E%20GPU%20%E4%B9%8B%E9%97%B4%E6%95%B0%E6%8D%AE%E4%BC%A0%E8%BE%93%E7%9A%84%E5%BC%80%E9%94%80%EF%BC%8C%E5%BD%93%E6%A8%A1%E5%9E%8B%E8%AE%A1%E7%AE%97%E5%B9%B6%E4%B8%8D%E5%A4%AA%E5%A4%8D%E6%9D%82%E6%97%B6%EF%BC%8C%E4%BD%BF%E7%94%A8%20CPU%20%E6%95%88%E6%9E%9C%E4%BC%9A%E6%9B%B4%E5%A5%BD%E4%BA%9B%E3%80%82))
  - 📝 模型不太复杂的情况下，CPU-GPU 的数据传输开销可能大于计算本身，那么只是用  CPU 训练更快。

- > 使用 Estimator 的主要好处在于：

1.  单机训练与分布式训练可以很简单的切换，而且在使用不同设备：CPU、GPU、TPU 时，无需修改过多的代码。
2.  Estimator 的框架十分清晰，便于开发者之间的交流。
3.  初学者还可以直接使用一些已经构建好的 Estimator 模型：DNN 模型、XGBoost 模型、线性模型等。 [[tensorflow]]   ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/46591057#js_content:~:text=%E4%BD%BF%E7%94%A8%20Estimator%20%E7%9A%84%E4%B8%BB%E8%A6%81%E5%A5%BD%E5%A4%84%E5%9C%A8%E4%BA%8E%EF%BC%9A%E5%8D%95%E6%9C%BA%E8%AE%AD%E7%BB%83%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83%E5%8F%AF%E4%BB%A5%E5%BE%88%E7%AE%80%E5%8D%95%E7%9A%84%E5%88%87%E6%8D%A2%EF%BC%8C%E8%80%8C%E4%B8%94%E5%9C%A8%E4%BD%BF%E7%94%A8%E4%B8%8D%E5%90%8C%E8%AE%BE%E5%A4%87%EF%BC%9ACPU%E3%80%81GPU%E3%80%81TPU%20%E6%97%B6%EF%BC%8C%E6%97%A0%E9%9C%80%E4%BF%AE%E6%94%B9%E8%BF%87%E5%A4%9A%E7%9A%84%E4%BB%A3%E7%A0%81%E3%80%82Estimator%20%E7%9A%84%E6%A1%86%E6%9E%B6%E5%8D%81%E5%88%86%E6%B8%85%E6%99%B0%EF%BC%8C%E4%BE%BF%E4%BA%8E%E5%BC%80%E5%8F%91%E8%80%85%E4%B9%8B%E9%97%B4%E7%9A%84%E4%BA%A4%E6%B5%81%E3%80%82%E5%88%9D%E5%AD%A6%E8%80%85%E8%BF%98%E5%8F%AF%E4%BB%A5%E7%9B%B4%E6%8E%A5%E4%BD%BF%E7%94%A8%E4%B8%80%E4%BA%9B%E5%B7%B2%E7%BB%8F%E6%9E%84%E5%BB%BA%E5%A5%BD%E7%9A%84%20Estimator%20%E6%A8%A1%E5%9E%8B%EF%BC%9ADNN%20%E6%A8%A1%E5%9E%8B%E3%80%81XGBoost%20%E6%A8%A1%E5%9E%8B%E3%80%81%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B%E7%AD%89%E3%80%82))
  - 📝 使用 Estimator 的好处

- > ![](https://pic2.zhimg.com/v2-c5284534bbf2605c373b9bc7f142a7dd_r.jpg) [[系统流程]]  [[tf-serving]]   ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/46591057#js_content:~:text=https://pic2.zhimg.com/v2-c5284534bbf2605c373b9bc7f142a7dd_r.jpg))
  - 📝 tf-serving 的架构图

- > 部署 TensorFlow Serving 的第一版时，QPS 大约 200 时，打包请求需要 5ms，网络开销需要固定 3ms 左右，仅模型预估计算需要 10ms，整个过程的 TP50 线大约 18ms，性能完全达不到线上的要求。  ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/46591057#js_content:~:text=%E9%83%A8%E7%BD%B2%20TensorFlow%20Serving%20%E7%9A%84%E7%AC%AC%E4%B8%80%E7%89%88%E6%97%B6%EF%BC%8CQPS%20%E5%A4%A7%E7%BA%A6%20200%20%E6%97%B6%EF%BC%8C%E6%89%93%E5%8C%85%E8%AF%B7%E6%B1%82%E9%9C%80%E8%A6%81%205ms%EF%BC%8C%E7%BD%91%E7%BB%9C%E5%BC%80%E9%94%80%E9%9C%80%E8%A6%81%E5%9B%BA%E5%AE%9A%203ms%20%E5%B7%A6%E5%8F%B3%EF%BC%8C%E4%BB%85%E6%A8%A1%E5%9E%8B%E9%A2%84%E4%BC%B0%E8%AE%A1%E7%AE%97%E9%9C%80%E8%A6%81%2010ms%EF%BC%8C%E6%95%B4%E4%B8%AA%E8%BF%87%E7%A8%8B%E7%9A%84%20TP50%20%E7%BA%BF%E5%A4%A7%E7%BA%A6%2018ms%EF%BC%8C%E6%80%A7%E8%83%BD%E5%AE%8C%E5%85%A8%E8%BE%BE%E4%B8%8D%E5%88%B0%E7%BA%BF%E4%B8%8A%E7%9A%84%E8%A6%81%E6%B1%82%E3%80%82))
  - 📝 优化前，美团基于 tf-serving 的性能，平响 18ms

- > 线上请求端优化主要是对一百个广告进行并行处理，我们使用 OpenMP 多线程并行处理数据，将请求时间性能从 5ms 降低到 2ms 左右。 [[工程优化]]   ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/46591057#js_content:~:text=%E7%BA%BF%E4%B8%8A%E8%AF%B7%E6%B1%82%E7%AB%AF%E4%BC%98%E5%8C%96%E4%B8%BB%E8%A6%81%E6%98%AF%E5%AF%B9%E4%B8%80%E7%99%BE%E4%B8%AA%E5%B9%BF%E5%91%8A%E8%BF%9B%E8%A1%8C%E5%B9%B6%E8%A1%8C%E5%A4%84%E7%90%86%EF%BC%8C%E6%88%91%E4%BB%AC%E4%BD%BF%E7%94%A8%20OpenMP%20%E5%A4%9A%E7%BA%BF%E7%A8%8B%E5%B9%B6%E8%A1%8C%E5%A4%84%E7%90%86%E6%95%B0%E6%8D%AE%EF%BC%8C%E5%B0%86%E8%AF%B7%E6%B1%82%E6%97%B6%E9%97%B4%E6%80%A7%E8%83%BD%E4%BB%8E%205ms%20%E9%99%8D%E4%BD%8E%E5%88%B0%202ms%20%E5%B7%A6%E5%8F%B3%E3%80%82))
  - 📝 请求端优化，多线程并行处理数据

- > 使用 tf.feature_column 的好处是，输入时不需要对原数据做任何处理，可以通过 feature_column API 在模型内部对特征做很多常用的处理，例如：tf.feature_column.bucketized_column 可以做分桶，tf.feature_column.crossed_column 可以对类别特征做特征交叉。但特征处理的压力就放在了模型里。  ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/46591057#js_content:~:text=%E4%BD%BF%E7%94%A8%20tf.feature_column%20%E7%9A%84%E5%A5%BD%E5%A4%84%E6%98%AF%EF%BC%8C%E8%BE%93%E5%85%A5%E6%97%B6%E4%B8%8D%E9%9C%80%E8%A6%81%E5%AF%B9%E5%8E%9F%E6%95%B0%E6%8D%AE%E5%81%9A%E4%BB%BB%E4%BD%95%E5%A4%84%E7%90%86%EF%BC%8C%E5%8F%AF%E4%BB%A5%E9%80%9A%E8%BF%87%20feature_column%20API%20%E5%9C%A8%E6%A8%A1%E5%9E%8B%E5%86%85%E9%83%A8%E5%AF%B9%E7%89%B9%E5%BE%81%E5%81%9A%E5%BE%88%E5%A4%9A%E5%B8%B8%E7%94%A8%E7%9A%84%E5%A4%84%E7%90%86%EF%BC%8C%E4%BE%8B%E5%A6%82%EF%BC%9Atf.feature_column.bucketized_column%20%E5%8F%AF%E4%BB%A5%E5%81%9A%E5%88%86%E6%A1%B6%EF%BC%8Ctf.feature_column.crossed_column%20%E5%8F%AF%E4%BB%A5%E5%AF%B9%E7%B1%BB%E5%88%AB%E7%89%B9%E5%BE%81%E5%81%9A%E7%89%B9%E5%BE%81%E4%BA%A4%E5%8F%89%E3%80%82%E4%BD%86%E7%89%B9%E5%BE%81%E5%A4%84%E7%90%86%E7%9A%84%E5%8E%8B%E5%8A%9B%E5%B0%B1%E6%94%BE%E5%9C%A8%E4%BA%86%E6%A8%A1%E5%9E%8B%E9%87%8C%E3%80%82))
  - 📝 使用 tf.feature_column 处理数据的优缺点，缺点更难以接受一些：数据处理的压力都压在了模型端

- > 为了进一步分析使用 feature_column 的耗时，我们使用 tf.profiler 工具，对整个离线训练流程耗时做了分析。  ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/46591057#js_content:~:text=%E4%B8%BA%E4%BA%86%E8%BF%9B%E4%B8%80%E6%AD%A5%E5%88%86%E6%9E%90%E4%BD%BF%E7%94%A8%20feature_column%20%E7%9A%84%E8%80%97%E6%97%B6%EF%BC%8C%E6%88%91%E4%BB%AC%E4%BD%BF%E7%94%A8%20tf.profiler%20%E5%B7%A5%E5%85%B7%EF%BC%8C%E5%AF%B9%E6%95%B4%E4%B8%AA%E7%A6%BB%E7%BA%BF%E8%AE%AD%E7%BB%83%E6%B5%81%E7%A8%8B%E8%80%97%E6%97%B6%E5%81%9A%E4%BA%86%E5%88%86%E6%9E%90%E3%80%82))
  - 📝 使用 tf.profiler 对流程耗时进行解析，搭配 estimator 使用
  - 📝 keras model 可以通过接口（tf.keras.estimator.model_to_estimator）较为方便地转为 estimator

- > ![](https://pic2.zhimg.com/v2-82c6ec6015b4f3c2463e80f456467da1_r.jpg)  ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/46591057#js_content:~:text=https://pic2.zhimg.com/v2-82c6ec6015b4f3c2463e80f456467da1_r.jpg))
  - 📝 tf.profiler 对 tensorflow 流程耗时的记录，50% 消耗在了 tf.feature_column  特征处理上

- > 为了解决特征在模型内做处理耗时大的问题，我们在处理离线数据时，把所有 string 格式的原生数据，提前做好 One Hot 的映射，并且把映射关系落到本地 feature_index 文件，进而供线上线下使用。 [[工程优化]]   ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/46591057#js_content:~:text=%E4%B8%BA%E4%BA%86%E8%A7%A3%E5%86%B3%E7%89%B9%E5%BE%81%E5%9C%A8%E6%A8%A1%E5%9E%8B%E5%86%85%E5%81%9A%E5%A4%84%E7%90%86%E8%80%97%E6%97%B6%E5%A4%A7%E7%9A%84%E9%97%AE%E9%A2%98%EF%BC%8C%E6%88%91%E4%BB%AC%E5%9C%A8%E5%A4%84%E7%90%86%E7%A6%BB%E7%BA%BF%E6%95%B0%E6%8D%AE%E6%97%B6%EF%BC%8C%E6%8A%8A%E6%89%80%E6%9C%89%20string%20%E6%A0%BC%E5%BC%8F%E7%9A%84%E5%8E%9F%E7%94%9F%E6%95%B0%E6%8D%AE%EF%BC%8C%E6%8F%90%E5%89%8D%E5%81%9A%E5%A5%BD%20One%20Hot%20%E7%9A%84%E6%98%A0%E5%B0%84%EF%BC%8C%E5%B9%B6%E4%B8%94%E6%8A%8A%E6%98%A0%E5%B0%84%E5%85%B3%E7%B3%BB%E8%90%BD%E5%88%B0%E6%9C%AC%E5%9C%B0%20feature_index%20%E6%96%87%E4%BB%B6%EF%BC%8C%E8%BF%9B%E8%80%8C%E4%BE%9B%E7%BA%BF%E4%B8%8A%E7%BA%BF%E4%B8%8B%E4%BD%BF%E7%94%A8%E3%80%82))
  - 📝 前置 one-hot encoding 的工作

- > 在构建模型时候，使用更多性能有保证的低阶 API 替代 feature_column 这样的高阶 API。下图为性能优化后，前向传播耗时在整个训练流程的占比。  ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/46591057#js_content:~:text=%E5%9C%A8%E6%9E%84%E5%BB%BA%E6%A8%A1%E5%9E%8B%E6%97%B6%E5%80%99%EF%BC%8C%E4%BD%BF%E7%94%A8%E6%9B%B4%E5%A4%9A%E6%80%A7%E8%83%BD%E6%9C%89%E4%BF%9D%E8%AF%81%E7%9A%84%E4%BD%8E%E9%98%B6%20API%20%E6%9B%BF%E4%BB%A3%20feature_column%20%E8%BF%99%E6%A0%B7%E7%9A%84%E9%AB%98%E9%98%B6%20API%E3%80%82%E4%B8%8B%E5%9B%BE%E4%B8%BA%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E5%90%8E%EF%BC%8C%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD%E8%80%97%E6%97%B6%E5%9C%A8%E6%95%B4%E4%B8%AA%E8%AE%AD%E7%BB%83%E6%B5%81%E7%A8%8B%E7%9A%84%E5%8D%A0%E6%AF%94%E3%80%82))
  - 📝 实践中，我知道 bucket 和 cross 的耗时会比较严重，所以把他们前置了。但是没有提前处理 string，所以我的模型看起来比同事的小的多，但是耗时比他的还要高。

- > TensorFlow 采用有向数据流图来表达整个计算过程，其中 Node 代表着操作（OPS），数据通过 Tensor 的方式来表达，不同 Node 间有向的边表示数据流动方向，整个图就是有向的数据流图。

XLA（Accelerated Linear Algebra）是一种专门对 TensorFlow 中线性代数运算进行优化的编译器，当打开 JIT（Just In Time）编译模式时，便会使用 XLA 编译器。  ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/46591057#js_content:~:text=TensorFlow%20%E9%87%87%E7%94%A8%E6%9C%89%E5%90%91%E6%95%B0%E6%8D%AE%E6%B5%81%E5%9B%BE%E6%9D%A5%E8%A1%A8%E8%BE%BE%E6%95%B4%E4%B8%AA%E8%AE%A1%E7%AE%97%E8%BF%87%E7%A8%8B%EF%BC%8C%E5%85%B6%E4%B8%AD%20Node%20%E4%BB%A3%E8%A1%A8%E7%9D%80%E6%93%8D%E4%BD%9C%EF%BC%88OPS%EF%BC%89%EF%BC%8C%E6%95%B0%E6%8D%AE%E9%80%9A%E8%BF%87%20Tensor%20%E7%9A%84%E6%96%B9%E5%BC%8F%E6%9D%A5%E8%A1%A8%E8%BE%BE%EF%BC%8C%E4%B8%8D%E5%90%8C%20Node%20%E9%97%B4%E6%9C%89%E5%90%91%E7%9A%84%E8%BE%B9%E8%A1%A8%E7%A4%BA%E6%95%B0%E6%8D%AE%E6%B5%81%E5%8A%A8%E6%96%B9%E5%90%91%EF%BC%8C%E6%95%B4%E4%B8%AA%E5%9B%BE%E5%B0%B1%E6%98%AF%E6%9C%89%E5%90%91%E7%9A%84%E6%95%B0%E6%8D%AE%E6%B5%81%E5%9B%BE%E3%80%82XLA%EF%BC%88Accelerated%20Linear%20Algebra%EF%BC%89%E6%98%AF%E4%B8%80%E7%A7%8D%E4%B8%93%E9%97%A8%E5%AF%B9%20TensorFlow%20%E4%B8%AD%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0%E8%BF%90%E7%AE%97%E8%BF%9B%E8%A1%8C%E4%BC%98%E5%8C%96%E7%9A%84%E7%BC%96%E8%AF%91%E5%99%A8%EF%BC%8C%E5%BD%93%E6%89%93%E5%BC%80%20JIT%EF%BC%88Just%20In%20Time%EF%BC%89%E7%BC%96%E8%AF%91%E6%A8%A1%E5%BC%8F%E6%97%B6%EF%BC%8C%E4%BE%BF%E4%BC%9A%E4%BD%BF%E7%94%A8%20XLA%20%E7%BC%96%E8%AF%91%E5%99%A8%E3%80%82))
  - 📝 这一小节等于什么都没说，就是给了两个概念，可以使用 XLA 优化编译

- > 超时问题主要源于两个方面，一方面，更新、加载模型和处理 TensorFlow Serving 请求的线程共用一个线程池，导致切换模型时无法处理请求；另一方面，模型加载后，计算图采用 Lazy Initialization 方式，导致第一次请求需要等待计算图初始化。 [[tensorflow]]   ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/46591057#js_content:~:text=%E8%B6%85%E6%97%B6%E9%97%AE%E9%A2%98%E4%B8%BB%E8%A6%81%E6%BA%90%E4%BA%8E%E4%B8%A4%E4%B8%AA%E6%96%B9%E9%9D%A2%EF%BC%8C%E4%B8%80%E6%96%B9%E9%9D%A2%EF%BC%8C%E6%9B%B4%E6%96%B0%E3%80%81%E5%8A%A0%E8%BD%BD%E6%A8%A1%E5%9E%8B%E5%92%8C%E5%A4%84%E7%90%86%20TensorFlow%20Serving%20%E8%AF%B7%E6%B1%82%E7%9A%84%E7%BA%BF%E7%A8%8B%E5%85%B1%E7%94%A8%E4%B8%80%E4%B8%AA%E7%BA%BF%E7%A8%8B%E6%B1%A0%EF%BC%8C%E5%AF%BC%E8%87%B4%E5%88%87%E6%8D%A2%E6%A8%A1%E5%9E%8B%E6%97%B6%E6%97%A0%E6%B3%95%E5%A4%84%E7%90%86%E8%AF%B7%E6%B1%82%EF%BC%9B%E5%8F%A6%E4%B8%80%E6%96%B9%E9%9D%A2%EF%BC%8C%E6%A8%A1%E5%9E%8B%E5%8A%A0%E8%BD%BD%E5%90%8E%EF%BC%8C%E8%AE%A1%E7%AE%97%E5%9B%BE%E9%87%87%E7%94%A8%20Lazy%20Initialization%20%E6%96%B9%E5%BC%8F%EF%BC%8C%E5%AF%BC%E8%87%B4%E7%AC%AC%E4%B8%80%E6%AC%A1%E8%AF%B7%E6%B1%82%E9%9C%80%E8%A6%81%E7%AD%89%E5%BE%85%E8%AE%A1%E7%AE%97%E5%9B%BE%E5%88%9D%E5%A7%8B%E5%8C%96%E3%80%82))
  - 📝 TF 模型更新的两大耗时原因：1、更新与预估共用线程池，导致模型更新时，预估线程被占用；2、模型未预热

- > 谷歌也基于此推出 What-If-Tools 来帮助模型开发者对模型深入分析。  ([🌐 摘要链接](https://zhuanlan.zhihu.com/p/46591057#js_content:~:text=%E8%B0%B7%E6%AD%8C%E4%B9%9F%E5%9F%BA%E4%BA%8E%E6%AD%A4%E6%8E%A8%E5%87%BA%20What-If-Tools%20%E6%9D%A5%E5%B8%AE%E5%8A%A9%E6%A8%A1%E5%9E%8B%E5%BC%80%E5%8F%91%E8%80%85%E5%AF%B9%E6%A8%A1%E5%9E%8B%E6%B7%B1%E5%85%A5%E5%88%86%E6%9E%90%E3%80%82))

