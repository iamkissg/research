title:: 【Reading Highlights】深度 ctr 预估中 id 到 embedding 目前工业界主流是端到端直接学习还是预训练?
source:: https://www.zhihu.com/question/333087916/answer/743912318
tags:: [[简悦]] [[特征工程]]  [[embedding]]   [[reading_highlights]]
date:: 2022年05月19日 07:14:11


- > 目前我们还没有成功部署一个真正的完全端到端的深度学习模型。问题的难点还是在于加上 embedding 层之后，模型的复杂度、权重数量都上了一个量级，模型的收敛速度比预训练 embedding 的方式慢了一个量级。  ([🌐 摘要链接](https://www.zhihu.com/question/333087916/answer/743912318#js_content:~:text=%E7%9B%AE%E5%89%8D%E6%88%91%E4%BB%AC%E8%BF%98%E6%B2%A1%E6%9C%89%E6%88%90%E5%8A%9F%E9%83%A8%E7%BD%B2%E4%B8%80%E4%B8%AA%E7%9C%9F%E6%AD%A3%E7%9A%84%E5%AE%8C%E5%85%A8%E7%AB%AF%E5%88%B0%E7%AB%AF%E7%9A%84%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A8%A1%E5%9E%8B%E3%80%82%E9%97%AE%E9%A2%98%E7%9A%84%E9%9A%BE%E7%82%B9%E8%BF%98%E6%98%AF%E5%9C%A8%E4%BA%8E%E5%8A%A0%E4%B8%8A%20embedding%20%E5%B1%82%E4%B9%8B%E5%90%8E%EF%BC%8C%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%A4%8D%E6%9D%82%E5%BA%A6%E3%80%81%E6%9D%83%E9%87%8D%E6%95%B0%E9%87%8F%E9%83%BD%E4%B8%8A%E4%BA%86%E4%B8%80%E4%B8%AA%E9%87%8F%E7%BA%A7%EF%BC%8C%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%94%B6%E6%95%9B%E9%80%9F%E5%BA%A6%E6%AF%94%E9%A2%84%E8%AE%AD%E7%BB%83%20embedding%20%E7%9A%84%E6%96%B9%E5%BC%8F%E6%85%A2%E4%BA%86%E4%B8%80%E4%B8%AA%E9%87%8F%E7%BA%A7%E3%80%82))
  - 📝 即使是使用了 2022 年的大模型，带 embedding 的模型，embedding 还会占参数的大头。

- > 在不考虑模型更新这个因素的影响下，我之前的离线测试结果是，端到端模型比预训练模型的 AUC 大概好出 0.1%-0.3% 这样的量级，没有显著到我愿意舍弃预训练 embedding 方法灵活性和易用性的程度。  ([🌐 摘要链接](https://www.zhihu.com/question/333087916/answer/743912318#js_content:~:text=%E5%9C%A8%E4%B8%8D%E8%80%83%E8%99%91%E6%A8%A1%E5%9E%8B%E6%9B%B4%E6%96%B0%E8%BF%99%E4%B8%AA%E5%9B%A0%E7%B4%A0%E7%9A%84%E5%BD%B1%E5%93%8D%E4%B8%8B%EF%BC%8C%E6%88%91%E4%B9%8B%E5%89%8D%E7%9A%84%E7%A6%BB%E7%BA%BF%E6%B5%8B%E8%AF%95%E7%BB%93%E6%9E%9C%E6%98%AF%EF%BC%8C%E7%AB%AF%E5%88%B0%E7%AB%AF%E6%A8%A1%E5%9E%8B%E6%AF%94%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E7%9A%84%20AUC%20%E5%A4%A7%E6%A6%82%E5%A5%BD%E5%87%BA%200.1%25-0.3%25%20%E8%BF%99%E6%A0%B7%E7%9A%84%E9%87%8F%E7%BA%A7%EF%BC%8C%E6%B2%A1%E6%9C%89%E6%98%BE%E8%91%97%E5%88%B0%E6%88%91%E6%84%BF%E6%84%8F%E8%88%8D%E5%BC%83%E9%A2%84%E8%AE%AD%E7%BB%83%20embedding%20%E6%96%B9%E6%B3%95%E7%81%B5%E6%B4%BB%E6%80%A7%E5%92%8C%E6%98%93%E7%94%A8%E6%80%A7%E7%9A%84%E7%A8%8B%E5%BA%A6%E3%80%82))
  - 📝 王喆的实验，（注意时效性，该回答比较久了），端到端的方式比与预训练 embedding，离线 auc 特别显著的提升。

- > 预训练 embedding 时可以加入更多的 side information，采用更灵活的网络捕捉 item 之间的结构信息，这些不是跟主模型在一起端到端训练能够做到的。  ([🌐 摘要链接](https://www.zhihu.com/question/333087916/answer/743912318#js_content:~:text=%E9%A2%84%E8%AE%AD%E7%BB%83%20embedding%20%E6%97%B6%E5%8F%AF%E4%BB%A5%E5%8A%A0%E5%85%A5%E6%9B%B4%E5%A4%9A%E7%9A%84%20side%20information%EF%BC%8C%E9%87%87%E7%94%A8%E6%9B%B4%E7%81%B5%E6%B4%BB%E7%9A%84%E7%BD%91%E7%BB%9C%E6%8D%95%E6%8D%89%20item%20%E4%B9%8B%E9%97%B4%E7%9A%84%E7%BB%93%E6%9E%84%E4%BF%A1%E6%81%AF%EF%BC%8C%E8%BF%99%E4%BA%9B%E4%B8%8D%E6%98%AF%E8%B7%9F%E4%B8%BB%E6%A8%A1%E5%9E%8B%E5%9C%A8%E4%B8%80%E8%B5%B7%E7%AB%AF%E5%88%B0%E7%AB%AF%E8%AE%AD%E7%BB%83%E8%83%BD%E5%A4%9F%E5%81%9A%E5%88%B0%E7%9A%84%E3%80%82))
  - 📝 这是预训练 embedding 很重要的一点：加入 side information，理论上可以追加无限的信息量。

- > hash trick 有两种常见的策略：

1.  hash 后的维度跟之前接近，主要是省掉一个映射表同步的工程成本，加速计算。
2.  hash 后的维度显著小于原始维度，相当于把大量实体当作同一类看待，作为一种降维方式。

一般来说，1 基本上没有太多负面作用，如果工程成本下降很多那么就应该用，冲突率要自己控制一下。

2 的话就又回到之前的玄学问题上，我们也不知道这种方式到底破坏了多少信息，又能降低多少给模型增加的容量。  ([🌐 摘要链接](https://www.zhihu.com/question/333087916/answer/743912318#js_content:~:text=hash%20trick%20%E6%9C%89%E4%B8%A4%E7%A7%8D%E5%B8%B8%E8%A7%81%E7%9A%84%E7%AD%96%E7%95%A5%EF%BC%9Ahash%20%E5%90%8E%E7%9A%84%E7%BB%B4%E5%BA%A6%E8%B7%9F%E4%B9%8B%E5%89%8D%E6%8E%A5%E8%BF%91%EF%BC%8C%E4%B8%BB%E8%A6%81%E6%98%AF%E7%9C%81%E6%8E%89%E4%B8%80%E4%B8%AA%E6%98%A0%E5%B0%84%E8%A1%A8%E5%90%8C%E6%AD%A5%E7%9A%84%E5%B7%A5%E7%A8%8B%E6%88%90%E6%9C%AC%EF%BC%8C%E5%8A%A0%E9%80%9F%E8%AE%A1%E7%AE%97%E3%80%82hash%20%E5%90%8E%E7%9A%84%E7%BB%B4%E5%BA%A6%E6%98%BE%E8%91%97%E5%B0%8F%E4%BA%8E%E5%8E%9F%E5%A7%8B%E7%BB%B4%E5%BA%A6%EF%BC%8C%E7%9B%B8%E5%BD%93%E4%BA%8E%E6%8A%8A%E5%A4%A7%E9%87%8F%E5%AE%9E%E4%BD%93%E5%BD%93%E4%BD%9C%E5%90%8C%E4%B8%80%E7%B1%BB%E7%9C%8B%E5%BE%85%EF%BC%8C%E4%BD%9C%E4%B8%BA%E4%B8%80%E7%A7%8D%E9%99%8D%E7%BB%B4%E6%96%B9%E5%BC%8F%E3%80%82%E4%B8%80%E8%88%AC%E6%9D%A5%E8%AF%B4%EF%BC%8C1%20%E5%9F%BA%E6%9C%AC%E4%B8%8A%E6%B2%A1%E6%9C%89%E5%A4%AA%E5%A4%9A%E8%B4%9F%E9%9D%A2%E4%BD%9C%E7%94%A8%EF%BC%8C%E5%A6%82%E6%9E%9C%E5%B7%A5%E7%A8%8B%E6%88%90%E6%9C%AC%E4%B8%8B%E9%99%8D%E5%BE%88%E5%A4%9A%E9%82%A3%E4%B9%88%E5%B0%B1%E5%BA%94%E8%AF%A5%E7%94%A8%EF%BC%8C%E5%86%B2%E7%AA%81%E7%8E%87%E8%A6%81%E8%87%AA%E5%B7%B1%E6%8E%A7%E5%88%B6%E4%B8%80%E4%B8%8B%E3%80%822%20%E7%9A%84%E8%AF%9D%E5%B0%B1%E5%8F%88%E5%9B%9E%E5%88%B0%E4%B9%8B%E5%89%8D%E7%9A%84%E7%8E%84%E5%AD%A6%E9%97%AE%E9%A2%98%E4%B8%8A%EF%BC%8C%E6%88%91%E4%BB%AC%E4%B9%9F%E4%B8%8D%E7%9F%A5%E9%81%93%E8%BF%99%E7%A7%8D%E6%96%B9%E5%BC%8F%E5%88%B0%E5%BA%95%E7%A0%B4%E5%9D%8F%E4%BA%86%E5%A4%9A%E5%B0%91%E4%BF%A1%E6%81%AF%EF%BC%8C%E5%8F%88%E8%83%BD%E9%99%8D%E4%BD%8E%E5%A4%9A%E5%B0%91%E7%BB%99%E6%A8%A1%E5%9E%8B%E5%A2%9E%E5%8A%A0%E7%9A%84%E5%AE%B9%E9%87%8F%E3%80%82))
  - 📝 实践中用过此处的 trick2，但是没有对比过不用 hash 降维的效果。个人担心会引入不明的噪声，慎用。

- > 预训练出的 embedding 直接放进 ctr model 里面会存在最严重问题：**训练出的 embedding 的意义和 ctr 的目标完全不一致。**比如做召回阶段的 deep 模型的目标是衡量两个商品之间的相似性，但是 ctr 做的是预测用户点击商品的概率，初始化一个不相关的 embedding 会给模型带来更大的负担，更慢地收敛。 [[预训练模型]]   ([🌐 摘要链接](https://www.zhihu.com/question/333087916/answer/743912318#js_content:~:text=%E9%A2%84%E8%AE%AD%E7%BB%83%E5%87%BA%E7%9A%84%20embedding%20%E7%9B%B4%E6%8E%A5%E6%94%BE%E8%BF%9B%20ctr%20model%20%E9%87%8C%E9%9D%A2%E4%BC%9A%E5%AD%98%E5%9C%A8%E6%9C%80%E4%B8%A5%E9%87%8D%E9%97%AE%E9%A2%98%EF%BC%9A%E8%AE%AD%E7%BB%83%E5%87%BA%E7%9A%84%20embedding%20%E7%9A%84%E6%84%8F%E4%B9%89%E5%92%8C%20ctr%20%E7%9A%84%E7%9B%AE%E6%A0%87%E5%AE%8C%E5%85%A8%E4%B8%8D%E4%B8%80%E8%87%B4%E3%80%82%E6%AF%94%E5%A6%82%E5%81%9A%E5%8F%AC%E5%9B%9E%E9%98%B6%E6%AE%B5%E7%9A%84%20deep%20%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%9B%AE%E6%A0%87%E6%98%AF%E8%A1%A1%E9%87%8F%E4%B8%A4%E4%B8%AA%E5%95%86%E5%93%81%E4%B9%8B%E9%97%B4%E7%9A%84%E7%9B%B8%E4%BC%BC%E6%80%A7%EF%BC%8C%E4%BD%86%E6%98%AF%20ctr%20%E5%81%9A%E7%9A%84%E6%98%AF%E9%A2%84%E6%B5%8B%E7%94%A8%E6%88%B7%E7%82%B9%E5%87%BB%E5%95%86%E5%93%81%E7%9A%84%E6%A6%82%E7%8E%87%EF%BC%8C%E5%88%9D%E5%A7%8B%E5%8C%96%E4%B8%80%E4%B8%AA%E4%B8%8D%E7%9B%B8%E5%85%B3%E7%9A%84%20embedding%20%E4%BC%9A%E7%BB%99%E6%A8%A1%E5%9E%8B%E5%B8%A6%E6%9D%A5%E6%9B%B4%E5%A4%A7%E7%9A%84%E8%B4%9F%E6%8B%85%EF%BC%8C%E6%9B%B4%E6%85%A2%E5%9C%B0%E6%94%B6%E6%95%9B%E3%80%82))
  - 📝 认可的点：预训练和端到端的 embedding 肯定是有差异的
  - 📝 不认可的点：embedding 通常不会拿来就预估，后面的层是能够处理预训练 embedding 的，不然要后续的深度神经网络干嘛呢？
  - 📝 预训练的目标之一正是：将部分学习工作前置，尽可能地学习在任务训练时难以习得的特征。18 年以来，预训练在 NLP 领域的野火燎原之势，是最好的佐证。

- > **一个有意义的尝试**就是用它来初始化 DIN 的 behavior embedding 和 item embedding，计算内积来代替 attention，效果可能会比 DIN 要好。  ([🌐 摘要链接](https://www.zhihu.com/question/333087916/answer/743912318#js_content:~:text=%E4%B8%80%E4%B8%AA%E6%9C%89%E6%84%8F%E4%B9%89%E7%9A%84%E5%B0%9D%E8%AF%95%E5%B0%B1%E6%98%AF%E7%94%A8%E5%AE%83%E6%9D%A5%E5%88%9D%E5%A7%8B%E5%8C%96%20DIN%20%E7%9A%84%20behavior%20embedding%20%E5%92%8C%20item%20embedding%EF%BC%8C%E8%AE%A1%E7%AE%97%E5%86%85%E7%A7%AF%E6%9D%A5%E4%BB%A3%E6%9B%BF%20attention%EF%BC%8C%E6%95%88%E6%9E%9C%E5%8F%AF%E8%83%BD%E4%BC%9A%E6%AF%94%20DIN%20%E8%A6%81%E5%A5%BD%E3%80%82))
  - 📝 同意。
  - 📝 +[[DIN]]

