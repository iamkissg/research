- DIN Highlight 1(5/31/2022, 10:46:46 PM)
- “In these methods large scale sparse input features are first mapped into low dimensional embedding vectors, and then transformed into fixed-length vectors in a group-wise manner, finally concatenated together to fed into a multilayer perceptron (MLP) to learn the nonlinear relations among features.” (Zhou et al., 2018, p. 1059)Embedding&MLP 的推荐范式
- “by designing a local activation unit to adaptively learn the representation of user interests from historical behaviors with respect to a certain ad.” (Zhou et al., 2018, p. 1059)DIN 的创新点：提出新的激活函数（[[注意力机制]]），来学习针对某一商品/广告的、从用户行为序列中提取的兴趣表示
- “On the other hand, it is not necessary to compress all the diverse interests of a certain user into the same vector when predicting a candidate ad because only part of user’s interests will inuence his/her action (to click or not to click).” (Zhou et al., 2018, p. 1060)在[[王喆]]的文章里看到盖坤老师的留言：DIN 原始的目标是捕捉用户的多峰兴趣
- “By introducing a local activation unit, DIN pays attentions to the related user interests by soft-searching for relevant parts of historical behaviors and takes a weighted sum pooling to obtain the representation of user interests with respect to the candidate ad.” (Zhou et al., 2018, p. 1060)[[WDL]] 用了 average pooling更好的做法是时间衰减加权此处使用[[Attention]]来加权，更加强调了商品间的关系
- “we develop a novel mini-batch aware regularization where only parameters of non-zero features appearing in each mini-batch participate in the calculation of L2-norm, making the computation acceptable.” (Zhou et al., 2018, p. 1060)本文的另一点创新：一种新的正则化技术
- “design a data adaptive activation function, which generalizes commonly used PReLU[11] by adaptively adjusting the rectied point w.r.t. distribution of inputs and is shown to be helpful for training industrial networks with sparse features.” (Zhou et al., 2018, p. 1060)本文的又一点创新：一种新的激活函数
- “It is shown that the use of attention can help capturing the main intent of query or ad.” (Zhou et al., 2018, p. 1061)行业共识：注意力能够捕捉意图
- “It is worth mentioning that users with rich historical behaviors contain diverse interests.” (Zhou et al., 2018, p. 1061)丰富的用户行为包含了多样的兴趣，即盖坤老师讲的多峰兴趣
- “Obviously the displayed ad only matches or activates part of interests” (Zhou et al., 2018, p. 1061)DIN 希望实现的，兴趣聚焦
- “In summary, interests of user with rich behaviors are diverse and could be locally activated given certain ads.” (Zhou et al., 2018, p. 1061)综合以上两点，DIN 要做的就是基于用户的历史行为，在给定商品的情况下，发掘用户兴趣
- “Data in industrial CTR prediction tasks is mostly in a multi-group categorial form,” (Zhou et al., 2018, p. 1061) CTR预测的数据，多为多分组的类别形式
- “ti [j] is the j-th element of ti and ti [j] ∈ {0, 1}. ∑Ki j=1 ti [j] = k. Vector ti with k = 1 refers to one-hot encoding and k > 1 refers to multi-hot encoding.” (Zhou et al., 2018, p. 1061) 可以是one-hot，可以是multi-hot（多类目）我还真不清楚 multi-hot 怎么处理，用 mllib 之类的工具做 encoding 就行吗？
- “user behavior features are typically multi-hot encoding vectors and contain rich information of user interests.” (Zhou et al., 2018, p. 1061)如果不考虑时间序列，用户的历史行为就是在所有商品的超大向量上的 multi-hot也就是把所有的 one-hot vector 加起来
- “it is a common practice [3, 4] to transform the list of embedding vectors via a pooling layer to get a xed-length” (Zhou et al., 2018, p. 1062) 用pooling将变长处理成定长
- “Instead of expressing all user’s diverse interests with the same vector, DIN adaptively calculate the representation vector of user interests by taking into consideration the relevance of historical behaviors w.r.t. candidate ad.” (Zhou et al., 2018, p. 1063)对候选商品/广告，计算它与用户行为序列的相关性，自适应用户行为的表征
- “Specically, activation units are applied on the user behavior features, which performs as a weighted sum pooling to adaptively calculate user representation vU given a candidate ad A,” (Zhou et al., 2018, p. 1063) 当前的广告和历史行为计算它对历史行为的 attention，高相似度就高权重
- “Traditional attention methods lose the resolution on the numerical scale of vU by normalizing of the output of a(·).” (Zhou et al., 2018, p. 1063) 归一化怎么就损失了权重的数值精度呢？
- “Rapid jumping and sudden ending over these interests causes the sequence data of user behaviors to seem to be” (Zhou et al., 2018, p. 1063) 用户行为中的噪声
- “model performance falls rapidly after the rst epoch during training without regularization, as the dark green line shown in Fig.4” (Zhou et al., 2018, p. 1063)不带正则化的训练，在第一个 epoch 之后，模型性能迅速下降
- “It is not practical to directly apply traditional regularization methods, such as `2 and `1 regularization, on training networks with sparse inputs and hundreds of millions of parameters.” (Zhou et al., 2018, p. 1063) 对于输入是稀疏的，包含百万级特征的神经网络，直接应用L1、L2正则化不合适
- “Only parameters of non-zero sparse features appearing in each mini-batch needs to be updated in the scenario of SGD based optimization methods without regularization. However, when adding `2 regularization it needs to calculate L2-norm over the whole parameters for each mini-batch, which leads to extremely heavy computations and is unacceptable with parameters scaling up to hundreds of millions.” (Zhou et al., 2018, p. 1063) 数据量达到千万、亿级的时候，L2正则化的计算量太大
- “In fact, it is the embedding dictionary that contributes most of the parameters for CTR networks and arises the diculty of heavy computation.” (Zhou et al., 2018, p. 1063) embedding 占了参数量的大头，计算开销巨大
- “In the training phrase, E[s] and V ar [s] is the mean and variance of input in each mini-batch. In the testing phrase, E[s] and V ar [s] is calculated by moving averages E[s] and V ar [s] over data.” (Zhou et al., 2018, p. 1064) 这样分阶段的会不会不是好的选择，造成了训练时与预测时的差异。不如一直用移动平均，或者全局的平均这一做法和 dropout 是类似的思路
- “The key idea of Dice is to adaptively adjust the rectified point according to distribution of input data, whose value is set to be the mean of input. Besides, Dice controls smoothly to switch between the two channels.” (Zhou et al., 2018, p. 1064)Dice 激活的关键是：根据输入数据的分布，自适应地调整整流点（设置为数据的平局数）
- “For all the deep models, the dimensionality of embedding vector is 12 for the whole 16 groups of features. Layers of MLP is set by 192 × 200 × 80 × 2. Due to the huge size of data, we set the mini-batch size to be 5000 and use Adam[14] as the optimizer. We apply exponential decay, in which learning rate starts at 0.001 and decay rate is set to 0.9.” (Zhou et al., 2018, p. 1064)DIN 的模型配置：1. embedding dim 全部设为 122. MLP 的 hidden size 分别是 192、200、80、23. 训练的批量大小是 50004. 使用 Adam 作为优化器，初始学习率为 0.001，衰减稀疏为 0.9
- “averaging AUC over users and is shown to be more relevant to online performance in display advertising system.” (Zhou et al., 2018, p. 1065)在[[计算广告]]中，对用户平均的 auc 与线上性能更一致这个计算量会不会太大了
- “RelaImpr” (Zhou et al., 2018, p. 1065) 本文实验的另一个观测指标，考察模型的相对进步公式为$\frac{auc_{exp}-0.5}{auc_{base}-0.5}-1$
- “when training deep models with ne-grained features (e.g., features of goods_ids with dimension of 0.6 billion in Table 1), serious overtting occurs after the rst epoch without any regularization, which causes the model performance to drop rapidly,” (Zhou et al., 2018, p. 1066)商品维度的特征高达 6000 千万上文已有描述：不使用正则化，第一个 epoch 之后，模型性能迅速下跌
- “Filter visited goods_id by occurrence frequency in samples and leave only the most frequent ones. In our setting, top 20 million goods_ids are left.” (Zhou et al., 2018, p. 1066)这是数据降采样的方式，使用商品的浏览次数作为过滤条件，只取 top 2000 万的商品这是本文对比提出的 MBA 方法的一种正则化技术
- “Parameters associated with frequent features are less over-regularized.” (Zhou et al., 2018, p. 1066)高频特征的参数更不容易被过正则化
- “MBA. Our proposed Mini-Batch Aware regularization method (Eq.4). Regularization parameter λ for both DiFacto and MBA is searched and set to be 0.01.” (Zhou et al., 2018, p. 1066) MBA 是本文的一个创新点
- “Besides, well trained models with goods_ids features show better AUC performance than without them. This is duo to the richer information that ne-grained features contained. Considering this, although frequency lter performs slightly better than dropout, it throws away most of low frequent ids and may lose room for models to make better use of ne-grained features.” (Zhou et al., 2018, p. 1066) 尽管Filter表现得更好，但是它丢弃了许多重要信息，它的空间比 dropout 会更有限
- “DIN trained with the proposed regularizer and activation function contributes up to 10.0% CTR and 3.8% RPM(Revenue” (Zhou et al., 2018, p. 1066)DIN 的线上收益真不小，两位数了！
- “We can see that goods with same category almost belong to one cluster, which shows the clustering property of DIN embeddings clearly.” (Zhou et al., 2018, p. 1067)同类目的商品倾向于属于同一个聚类，表明 DIN embedding 的有效性