tags:: [[paper_highlights]]

- Annotations(6/8/2022, 6:49:07 AM)
- â€œABSTRACTâ€ (Kang et al., 2021, p. 840)
- â€œIn this paper, we propose an alternative embedding framework Deep Hash Embedding (DHE), replacing embedding tables by a deep embedding network to compute embeddings on the fly.â€ (Kang et al., 2021, p. 840)æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªæ–°çš„ embedding æ¡†æ¶ Deep Hash Embeddingï¼ˆDHEï¼‰ï¼Œç”¨æ·±åº¦ç½‘ç»œä»£æ›¿äº† embedding table
- â€œDHE first encodes the feature value to a unique identifier vector with multiple hashing functions and transformations, and then applies a DNN to convert the identifier vector to an embedding. The encoding module is deterministic, non-learnable, and free of storage, while the embedding network is updated during the training time to learn embedding generation.â€ (Kang et al., 2021, p. 840)DHE å°†ç‰¹å¾åˆ° embedding çš„è¿‡ç¨‹æ‹†è§£ä¸ºäº† encode ä¸ decode çš„ä¸¤æ­¥ã€‚Encode é˜¶æ®µï¼Œä½¿ç”¨å¤šä¸ªå“ˆå¸Œå‡½æ•°å°†ç‰¹å¾ç¼–ç ä¸ºä¸€ä¸ªå…·æœ‰å”¯ä¸€æ€§çš„å‘é‡ï¼ˆé one-hotï¼‰ï¼›decode é˜¶æ®µï¼Œä½¿ç”¨æ·±åº¦ç¥ç»ç½‘ç»œæ¥å­¦ä¹  embeddingã€‚ç¼–ç æ¨¡å—æ˜¯ç¡®å®šçš„ã€ä¸å¯å­¦ä¹ çš„ã€æ— å­˜å‚¨é‡çš„ï¼›è§£ç æ¨¡å—æ˜¯å¯è®­ç»ƒçš„ã€‚æˆ‘çš„ä¸€ä¸ªç–‘é—®æ˜¯ï¼šä½¿ç”¨äº†å¤šä¸ªå“ˆå¸Œå‡½æ•°ï¼Œé‚£ä¹ˆå¦‚ä½•ä¿å­˜å®ƒä»¬ï¼Œä»¥ä½¿å®ƒä»¬åœ¨çº¿ä¸Šå’Œçº¿ä¸‹åŒæ—¶å¯ç”¨ï¼Ÿ
- â€œ1 INTRODUCTIONâ€ (Kang et al., 2021, p. 840)
- (Kang et al., 2021, p. 840)DHE ä¸å¸¸è§„ one-hot çš„å¯¹æ¯”ç¤ºæ„å›¾ã€‚DHE çš„å‚æ•°é‡æ˜¯ä¸åŒå±‚çš„ emb_dim çš„ä¹˜ç§¯å’Œï¼Œone-hot çš„å‚æ•°é‡å°±æ˜¯ vocab_size ä¸æœ€ç»ˆ emb_dim çš„ä¹˜ç§¯ã€‚å¯¹äºè¶…å¤§ vocab_sizeï¼ŒDHE å¯ä»¥å¿«é€Ÿç¼©å‡ embedding éƒ¨åˆ†çš„å‚æ•°é‡ã€‚
- â€œthere are several challenges when applying embedding learning in recommendation: â€¢ Huge vocabulary size: Recommender systems usually need to handle high-cardinality categorical features (e.g. billions of video IDs for online video sharing platforms).â€ (Kang et al., 2021, p. 840)æ¨èç³»ç»Ÿä¸­ä½¿ç”¨ embedding çš„æŒ‘æˆ˜ 1ï¼šè¶…å¤§çš„ vocab_sizeï¼Œç±»å¦‚ç±»ç›®ã€ID ä½œä¸ºç‰¹å¾æ—¶ï¼Œvocab_size å°±æ˜¯ç³»ç»Ÿä¸­ç±»ç›®ã€ID çš„æ€»æ•°
- â€œâ€¢ Dynamic nature of input: Unlike vocabularies of words that are relatively static, the vocabulary in recommender systems could be highly dynamic. New users and new items enter the system on a daily basis, and stale items are gradually vanishing.â€ (Kang et al., 2021, p. 841)æ¨èç³»ç»Ÿä¸­ä½¿ç”¨ embedding çš„æŒ‘æˆ˜ 2ï¼šåŠ¨æ€è¾“å…¥ï¼Œæ¨èç³»ç»Ÿéœ€è¦æ—¶åˆ»é¢ä¸´æ–°ç”¨æˆ·ã€æ–°å•†å“çš„ OOV é—®é¢˜
- â€œâ€¢ Highly-skewed data distribution: The categorical features in recommendation data usually follow highly skewed powerlaw distributions. The small number of training examples on infrequent feature values hurts the embedding quality for the tail items significantly.â€ (Kang et al., 2021, p. 841)æ¨èç³»ç»Ÿä¸­ä½¿ç”¨ embedding çš„æŒ‘æˆ˜ 3ï¼šé«˜åº¦å€¾æ–œçš„æ•°æ®åˆ†å¸ƒï¼ˆå¹‚å¾‹åˆ†å¸ƒï¼‰ï¼Œæ¢è¨€ä¹‹ï¼Œé•¿å°¾ç°è±¡å¾ˆä¸¥é‡ï¼Œå°‘æ•°ä½é¢‘ç‰¹å¾å¯èƒ½ä¼šæŸå®³ embedding çš„æ€§èƒ½
- â€œthe one-hot representation often results in a huge embedding table especially for a large-vocab feature, and it also fails to adapt to out-of-vocab feature values.â€ (Kang et al., 2021, p. 841)one-hot ä¼šå¸¦æ¥è¶…å¤§çš„ embedding table å¸¦æ¥è¶…å¤§çš„å‚æ•°é‡ï¼Œå¹¶ä¸”æ— æ³•å¤„ç† oov ç‰¹å¾å€¼ã€‚
- â€œIn practice, to better handle new (i.e., out-of-vocab / unseen) feature values and reduce the storage cost, the hashing trick [36] is often adopted, that randomly maps feature values to a smaller number of hashing buckets, though the inevitable embedding collisions generally hurt performance.â€ (Kang et al., 2021, p. 841)å®è·µä¸­ï¼Œä¸ºäº†å¤„ç† one-hot å¸¦æ¥çš„ä¸¤ä¸ªé—®é¢˜ï¼Œé€šå¸¸ä¼šä½¿ç”¨ hashing trackï¼Œä½¿ç”¨å“ˆå¸Œå‡½æ•°å°†ç‰¹å¾å€¼æ˜ å°„åˆ°ä¸€ä¸ªæ›´å°çš„ç©ºé—´ï¼ˆé™ç»´äº†ï¼‰ï¼Œå†å–å¯¹åº”çš„ embeddingã€‚ä½¿ç”¨å“ˆå¸Œå‡½æ•°ï¼Œå“ˆå¸Œå†²çªæ˜¯æ— å¯é¿å…çš„ï¼Œå³ä¸¤ä¸ªä¸åŒçš„ç‰¹å¾å€¼å“ˆå¸Œåï¼Œä¸å·§è½åœ¨äº†åŒä¸€ä¸ªæ¡¶é‡Œï¼ŒåŸæœ¬å¯åŒºåˆ†çš„ç‰¹å¾å˜å¾—ä¸å¯åŒºåˆ†äº†ã€‚+æ—ç™»-æ–¯ç‰¹åŠ³æ–¯å®šç†
- â€œSpecifically, we use multiple hashing and appropriate transformations to generate a unique, deterministic, dense, and real-valued vector as the identifier encoding of the given feature value, and then the deep embedding network transforms the encoding to the final feature embeddings.â€ (Kang et al., 2021, p. 841)æ­¤å¤„æè¿°äº† DHE çš„ç¼–ç ç»“æœæ˜¯å”¯ä¸€çš„ã€ç¡®å®šçš„ã€ç¨ å¯†çš„ï¼ˆé one-hotï¼‰ã€å®å€¼çš„ã€‚è¿™ä¸åƒ embedding å—ï¼Ÿ
- â€œ2 PRELIMINARY: ONE-HOT BASED EMBEDDING LEARNINGâ€ (Kang et al., 2021, p. 841)
- â€œGenerally, the embedding function can be decomposed into two components: T = ğ¹ â—¦ ğ¸, where ğ¸ is an encoding function to represent feature values in some spaces, and ğ¹ is a decoding function to generate the embedding v.â€ (Kang et al., 2021, p. 841)å°† embedding function åˆ†è§£ä¸º encoder ä¸ decoderï¼Œç»Ÿä¸€ one-hot ä¸å…¶ä»–åŸºäºå“ˆå¸Œçš„æ–¹å¼çš„è¡¨ç¤ºã€‚
- â€œ2.1 One-hot Full Embeddingâ€ (Kang et al., 2021, p. 841)
- â€œSo we assume the feature values are already mapped to {1, 2, . . . , ğ‘›}, then the embedding approach creates an embedding table W âˆˆ R ğ‘›Ã—ğ‘‘ , and looks up its ğ‘ -th row Wğ‘  for the feature value ğ‘ . This is equivalent to the following: (i) we apply the encoding function ğ¸ to encode feature value ğ‘  with a one-hot encoding vector: ğ¸ (ğ‘ )=b âˆˆ {0, 1}ğ‘› where ğ‘ğ‘  =1 and ğ‘ ğ‘— =0 (ğ‘— â‰  ğ‘ ); (ii) we then apply the decoding function ğ¹ , a learnable linear transformation W âˆˆ Rğ‘›Ã—ğ‘‘ to generate the embedding e, that is, e = ğ¹ (b) = Wğ‘‡ b.â€ (Kang et al., 2021, p. 841)Encoder ã€decoder è§†è§’ä¸‹çš„ one-hot embeddingï¼š1. one-hot encoder å°†ç‰¹å¾å€¼ç¼–ç ä¸ºä»…å¯¹åº”ä½ç½®ä¸º 1 çš„ one-hot vector2. decoderï¼ˆä¹Ÿå°±æ˜¯ embedding layerï¼‰ æ˜¯ä¸€ä¸ªä¸€å±‚ç¥ç»ç½‘ç»œ
- â€œIn short, the embedding lookup process can be viewed as a 1-layer neural network (without bias terms) based on the one-hot encoding.â€ (Kang et al., 2021, p. 841)encoder-decoder è§†è§’ä¸‹ï¼Œone-hot embedding å°±æ˜¯åŸºäº one-hot encoding çš„ä¸å¸¦ bias çš„ä¸€å±‚ç¥ç»ç½‘ç»œ
- (Kang et al., 2021, p. 842)ä¸åŒ embedding æ–¹æ¡ˆçš„å…­ç»´å¯¹æ¯”
- â€œ2.2 One-hot Hash Embeddingâ€ (Kang et al., 2021, p. 842)
- â€œDespite the simplicity and effectiveness of full embeddings, such a scheme has two major issues in large-scale or dynamic settings: (i) the size of the embedding table grows linearly with the vocabulary size, which could cause a huge memory consumption.â€ (Kang et al., 2021, p. 842)one-hot embedding çš„ä¸¤ä¸ªé—®é¢˜ï¼šå‚æ•°é‡ä¸ vocab_size æˆæ­£æ¯”ï¼Œå¤§æ•°æ®æ—¶ä»£è¿™å¯èƒ½æ˜¯ä¸€ä¸ªå¤©æ–‡æ•°å­—
- â€œ(ii) in online learning settings where new values constantly arise, the full embedding scheme fails to handle unseen (out-of-vocab) feature values.â€ (Kang et al., 2021, p. 842)æ— æ³•å¤„ç†æ–°äººæ–°å“ç­‰ OOV é—®é¢˜
- â€œThe hashing trick [36] is a representative hashing method for reducing the dimension of the one-hot encoding for large vocabularies. The encoding function ğ¸ still maps a feature value into a one-hot vector, but with a different (typically smaller) cardinality of ğ‘š: ğ¸ (ğ‘ ) = b âˆˆ {0, 1}ğ‘š where ğ‘  âˆˆ ğ‘‰ , ğ‘ğ» (ğ‘ ) =1 and ğ‘ ğ‘— =0 ( ğ‘— â‰  ğ» (ğ‘ )). The hash function ğ» maps feature values (including unseen values) to {1, 2, . . . , ğ‘š} where ğ‘š is the hashed vocabulary size. The hash function ğ» seeks to distribute hashing values as uniformly as possible to reduce collision, though itâ€™s inevitable when ğ‘š < ğ‘›.â€ (Kang et al., 2021, p. 842)hashing trick æ–¹æ³•ï¼šå¯¹ç‰¹å¾å€¼å…ˆ hashï¼Œhash æ¡¶çš„ ID ä½œä¸ºæ–°çš„ç‰¹å¾å€¼ï¼Œå†åº”ç”¨ ont-hot embeddingã€‚å“ˆå¸Œæ¡¶æ•°<<åŸå§‹ vocab_size æ—¶ï¼Œèƒ½å¤Ÿæœ‰æ•ˆé™ä½å‚æ•°é‡ï¼Œé—®é¢˜æ˜¯ï¼šé™ç»´æ•ˆæœæœ‰å¤šå¥½ï¼Œå“ˆå¸Œå†²çªå°±æœ‰å¤šä¸¥é‡
- â€œIn summary, the hashing trick uses hashing to map feature values into ğ‘š-dim one-hot vectors, and then applies a 1-layer network to generate the embeddings.â€ (Kang et al., 2021, p. 842)hashing trick åœ¨ one-hot embedding ä¹‹å‰ï¼Œå¥—ç”¨äº†ä¸€ä¸ª hash mapping
- â€œAlthough the hashing trick is able to arbitrarily reduce the cardinality of the original vocabulary ğ‘‰ , it suffers from the embedding collision problem.â€ (Kang et al., 2021, p. 842)hashing trick çš„ä¸¤é¢æ€§ï¼šé™ç»´ä¸å“ˆå¸Œå†²çªçš„æ‹”æ²³
- â€œTo alleviate this issue, multiple hash functions have been used to generate multiple one-hot encodings: ğ¸ (ğ‘ ) = b = [b(1) ; b(2) ; . . . ; b(ğ‘˜) ] âˆˆ {0, 1}ğ‘šâˆ—ğ‘˜ where ğ‘ (ğ‘–) ğ» (ğ‘–) (ğ‘ ) =1 and ğ‘ (ğ‘–) ğ‘— =0 ( ğ‘— â‰  ğ» (ğ‘–) (ğ‘ )). Here, ğ‘˜ hash functions {ğ» (1), ğ» (2), . . . , ğ» (ğ‘˜) } are adopted to generate ğ‘˜ one-hot encodings {b(1), b(2), . . . , b(ğ‘˜) }, and the concatenation is used as the encoding. The core idea is that the concatenated encodings are less likely to be collided. We can lookup ğ‘˜ embeddings in ğ‘˜ embedding tables (respectively) and aggregate them into the final embedding.â€ (Kang et al., 2021, p. 842)hashing trick çš„æ”¹è¿›ï¼šæ—¢ç„¶å“ˆå¸Œå†²çªä¸å¯é¿å…ï¼Œé‚£ä¹ˆç”¨å¤šä¸ªä¸åŒçš„å“ˆå¸Œå‡½æ•°â€”â€”ä¸¤ä¸ªä¸åŒç‰¹å¾æ€»æ˜¯è½åœ¨åŒä¸€ä¸ªå“ˆå¸Œæ¡¶å†…çš„æ¦‚ç‡ä¼šéšç€å“ˆå¸Œå‡½æ•°çš„å¢åŠ è€Œé™ä½ï¼Œå†æŸ¥è¯¢å¤šæ¬¡ embedding tableï¼Œå¯¹ç»“æœè¿›è¡Œèšåˆã€‚
- â€œNote that existing methods didnâ€™t scale to large ğ‘˜ and the most commonly used variant is double hashing (ğ‘˜=2)â€ (Kang et al., 2021, p. 842)ç°æœ‰çš„æ–¹æ¡ˆå¹¶æ²¡æœ‰ä½¿ç”¨å¾ˆå¤šçš„å“ˆå¸Œå‡½æ•°ï¼Œé€šå¸¸å°±ä¸¤ä¸ª
- â€œ3 DEEP HASH EMBEDDINGS (DHE)â€ (Kang et al., 2021, p. 842)
- 3.1 Encoding Designâ€ (Kang et al., 2021, p. 842)
- â€œUniquenessâ€ (Kang et al., 2021, p. 842)Encoding è®¾è®¡åŸåˆ™ä¹‹å”¯ä¸€æ€§
- â€œThe encoding should be unique for each feature value. This is also the target of full embedding and multiple hashing methods.â€ (Kang et al., 2021, p. 842)ç‰¹å¾ç¼–ç ä¹‹å‰æ˜¯å”¯ä¸€çš„ï¼Œç¼–ç ä¹‹åä¹Ÿåº”è¯¥æ˜¯å”¯ä¸€çš„ï¼Œå¦åˆ™å°±ä¼šå¸¦æ¥ç¼–ç å†²çªï¼ˆå“ˆå¸Œå†²çªï¼‰ã€‚
- â€œThe collided encodings make the subsequent decoding function impossible to distinguish different feature values, which typically hurts model performance.â€ (Kang et al., 2021, p. 842)ç¼–ç å†²çªå°†ä½¿å¾—åç»­çš„è§£ç æ— æ³•åŒºåˆ†ä¸åŒçš„ç‰¹å¾å€¼
- â€œEqual Similarityâ€ (Kang et al., 2021, p. 842)Encoding è®¾è®¡åŸåˆ™ä¹‹ç­‰ç›¸ä¼¼æ€§
- â€œWe believe this introduces a wrong inductive bias (ID 8 and ID 9 are more similar), which may mislead the subsequent decoding function.â€ (Kang et al., 2021, p. 842)ç­‰ç›¸ä¼¼æ€§æ„å‘³ç€ï¼Œç¼–ç ä¸åº”è¯¥å¼•å…¥å½’çº³åç½®ã€‚one-hot ç¼–ç çš„ç‰¹ç‚¹æ˜¯ï¼Œæ‰€æœ‰å‘é‡ä¸¤ä¸¤ä¹‹é—´çš„ç›¸ä¼¼æ€§éƒ½æ˜¯ç›¸ç­‰çš„ã€‚
- â€œAs we donâ€™t know the semantic similarity among categorical features, we should make any two encodings be equally similar, and not introduce any inductive bias.â€ (Kang et al., 2021, p. 842)ç¼–ç ä¸åº”è¯¥å¼•å…¥å½’çº³åç½®çš„åŸå› æ˜¯ï¼šæˆ‘ä»¬æ— æ³•æå‰çŸ¥é“ç±»ç›®ç‰¹å¾ä¹‹é—´çš„ç›¸ä¼¼æ€§ï¼Œé‚£ä¹ˆä¿æŒåŸæ ·æ˜¯æœ€å¥½çš„åšæ³•ã€‚
- â€œHigh dimensionalityâ€ (Kang et al., 2021, p. 842)Encoding è®¾è®¡åŸåˆ™ä¹‹é«˜ç»´åº¦
- (Kang et al., 2021, p. 843)â€œAs high-dimensional spaces are often considered to be more separable (e.g. kernel methods), we believe the encoding dimension should be relatively high as wellâ€ (Kang et al., 2021, p. 843)é«˜ç»´ç©ºé—´æ›´å…·æœ‰å¯åˆ†æ€§ï¼ˆæ¯”å¦‚ SVM çš„æ ¸æ–¹æ³•ï¼‰ï¼Œæ‰€ä»¥ä½œè€…ä»¬è®¤ä¸ºç¼–ç çš„ç»´åº¦ç›¸å¯¹æ›´é«˜æ›´å¥½
- â€œHigh Shannon Entropyâ€ (Kang et al., 2021, p. 843)Encoding è®¾è®¡åŸåˆ™ä¹‹é«˜é¦™å†œç†µ
- â€œThe Shannon entropy [31] measures (in the unit of â€˜bitsâ€™) the information carried in a dimension. The high entropy requirement is to prevent redundant dimensions from the information theory perspective.â€ (Kang et al., 2021, p. 843)é¦™å†œç†µçš„å®šä¹‰ï¼šå‘é‡çš„æ¯ä¸€ç»´æºå¸¦çš„ä¿¡æ¯é‡æœ‰å¤šå°‘ã€‚ä»ä¿¡æ¯è®ºè§†è§’æ¥çœ‹ï¼Œé«˜é¦™å†œç†µèƒ½å¤Ÿé¿å…å†—ä½™ç»´åº¦ã€‚è¿™ä¸€ç‚¹å’Œé«˜ç»´åº¦æœ‰ä¸€ç‚¹æ‹‰æ‰¯çš„å‘³é“ï¼Œé«˜ç»´åº¦åœ¨å¼•å…¥å¯åŒºåˆ†æ€§çš„åŒæ—¶å¯èƒ½ä¼šå¼•å…¥å†—ä½™ç‰¹å¾ï¼Œé«˜é¦™å†œç†µåˆ™æå‡ºäº†é™ç»´çš„è¦æ±‚ã€‚
- â€œ3.2 Dense Hash Encodingâ€ (Kang et al., 2021, p. 843)
- â€œThe proposed encoding function ğ¸ : N â†’ Rğ‘˜ uses ğ‘˜ universal hash functions to map a feature value to a ğ‘˜-dimensional dense and real-valued encodings. Specifically, we have ğ¸â€²(ğ‘ ) = [ğ» (1) (ğ‘ ), ğ» (2) (ğ‘ ), . . . , ğ» (ğ‘˜) (ğ‘ )] where ğ» (ğ‘–) : N â†’ {1, 2, . . . , ğ‘š}.â€ (Kang et al., 2021, p. 843)DHE ä½¿ç”¨ k ä¸ª universal å“ˆå¸Œå‡½æ•°ï¼Œå°†ç‰¹å¾å€¼ç¼–ç ä¸ºä¸€ä¸ª k ç»´çš„å®å€¼å‘é‡
- â€œA nice property of universal hashing [4] is that the hashed values are evenly distributed (on average) over {1,2,. . . ,m}.â€ (Kang et al., 2021, p. 843)universal hashing çš„ä¼˜ç‚¹æ˜¯å“ˆå¸Œçš„ç»“æœæ˜¯å‡åŒ€åˆ†å¸ƒçš„ (ä¼ªä»£ç è§é™„å½•)
- â€œthe integer-based ğ¸â€²(ğ‘ ) encoding is not suitable to be used as the input to neural networks, as the input is typically real-valued and normalized for numeric stability. So we obtain real-valued encodings via appropriate transformations: ğ¸ (ğ‘ ) = transform(ğ¸â€²(ğ‘ )).â€ (Kang et al., 2021, p. 843)DHE çš„è§£ç æ¨¡å—æ˜¯ä¸€ä¸ª DNNï¼ŒDNN çš„è¾“å…¥é€šå¸¸æ˜¯å®å€¼å¹¶ä¸”ä¸ºäº†æ•°å€¼ç¨³å®šæ€§åšè¿‡å½’ä¸€åŒ–ï¼Œå› æ­¤ DHE çš„ encoder åœ¨å“ˆå¸Œä¹‹åï¼Œè¿˜æ¥äº†ä¸€ä¸ªè½¬æ¢å‡½æ•°ï¼Œå°†å“ˆå¸Œ ID è½¬æ¢ä¸ºä¸€ä¸ªå®å€¼
- â€œUniform Distribution. We simply normalize the encoding ğ¸â€² to the range of [âˆ’1, 1]. As the hashing values are evenly distributed (on average) among {1, 2, . . . , ğ‘š}, this approximates the uniform distribution ğ‘ˆ (âˆ’1, 1) reasonably well with a large ğ‘š.â€ (Kang et al., 2021, p. 843)æœ¬æ–‡æå‡ºå¹¶æœ€ç»ˆä½¿ç”¨çš„å°†å“ˆå¸Œ ID è½¬æ¢ä¸ºæœä»å‡åŒ€åˆ†å¸ƒçš„ [-1,1] çš„å€¼
- â€œGaussian Distribution. We first use the above transformation to obtain uniform distributed samples, and then apply the Boxâ€“Muller transform [3] which converts the uniformly distributed samples (i.e., ğ‘ˆ (âˆ’1, 1)) to the standard normal distribution N (0, 1).â€ (Kang et al., 2021, p. 843)æœ¬æ–‡æå‡ºçš„å¦ä¸€ç§åˆ†å¸ƒï¼Œåœ¨å‡åŒ€åˆ†å¸ƒçš„ç»“æœåŸºç¡€ä¸Šï¼Œå†æ‰§è¡Œä¸€æ¬¡ Box-Muller è½¬æ¢ï¼Œå°†å‡åŒ€åˆ†å¸ƒè½¬ä¸ºæ ‡å‡†æ­£æ€åˆ†å¸ƒ
- â€œEmpirically we found the two distributions work similarly well, and thus we choose the uniform distribution by default for simplicity.â€ (Kang et al., 2021, p. 843)å®éªŒçš„ç»éªŒæ˜¯ï¼Œå‡åŒ€åˆ†å¸ƒå’Œé«˜æ–¯åˆ†å¸ƒçš„æ•ˆæœå¾ˆæ¥è¿‘ï¼Œè€Œå‡åŒ€åˆ†å¸ƒçš„å®ç°æ›´å®¹æ˜“ï¼Œå› æ­¤ä½¿ç”¨äº†å¯¹åº”çš„è½¬æ¢å‡½æ•°
- â€œUnlike existing hashing methods limited to a few hash functions, we choose a relatively large ğ‘˜ for satisfying the high-dimensionality property (ğ‘˜=1024 in our experiments, though itâ€™s significantly smaller than ğ‘›). We empirically found our method significantly benefits from larger ğ‘˜â€ (Kang et al., 2021, p. 843)ä¸ä»¥å¾€çš„æ–¹æ³•ä¸åŒä¹‹å¤„ï¼šä½¿ç”¨äº†å¤§é‡å“ˆå¸Œå‡½æ•°ï¼Œè€Œ DHE ä¼¼ä¹ä»æ›´å¤šçš„å“ˆå¸Œå‡½æ•°ä¸­è·ç›Šæ›´å¤š
- â€œ3.3 Deep Embedding Networkâ€ (Kang et al., 2021, p. 843)
- â€œthe realvalued encoding is not applicable for embedding lookup. However, the mapping process is very similar to a highly non-linear feature transformation, where the input feature is fixed and non-learnable. Therefore, we use powerful deep neural networks (DNN) to model such a complex transformationâ€ (Kang et al., 2021, p. 843)è¿™é‡Œçš„è¯´æ³•æœ‰ç‚¹å› æœå€’ç½®çš„å‘³é“ã€‚Encoding é˜¶æ®µè¯´ï¼Œä¸ºäº†ä½¿ç”¨ DNNï¼Œæ‰€ä»¥åœ¨å“ˆå¸Œä¹‹åå†å°†ç‰¹å¾è½¬ä¸ºæœä»å‡åŒ€åˆ†å¸ƒçš„å®å€¼ï¼Œä½†æ˜¯è¿™é‡Œåˆè¯´ï¼Œå› ä¸ºå®å€¼ï¼Œåªèƒ½ä½¿ç”¨ DNNã€‚æœ¬èº«ï¼ŒDHE çš„ encoding å°±æ˜¯ç”¨å“ˆå¸Œæ¨¡æ‹Ÿäº†ä¸€æ¬¡ embeddingï¼Œè€Œä¸”å¾—åˆ°çš„å®å€¼å‘é‡ç”±äºæ˜¯ä¸å¯è®­ç»ƒçš„ï¼Œå°±åƒä½¿ç”¨äº†é™æ€ embedding vectorã€‚
- â€œMoreover, a recent study shows that deeper networks are more parameter-efficient than shallow networks [21]â€ (Kang et al., 2021, p. 843)ç ”ç©¶æ˜¾ç¤ºï¼Œæ›´æ·±çš„ç½‘ç»œçš„æ•ˆç‡æ¯”æµ…å±‚ç½‘ç»œæ›´é«˜
- â€œWe hope the hierarchical structures and non-linear activations enable DNNs to express the embedding function more efficiently than the one-hot encodings (i.e., 1-layer wide NN). This is motivated by recent research that shows that deep networks can approximate functions with much fewer parameters compared with wide and shallow networks [21].â€ (Kang et al., 2021, p. 844)ç ”ç©¶è¡¨æ˜ï¼šæ·±åº¦ç½‘ç»œèƒ½å¤Ÿä»¥æ›´å°‘çš„å‚æ•°æ¨¡æ‹Ÿå®½åº¦ç½‘ç»œï¼Œå› æ­¤æœ¬æ–‡çš„ä¸€ä¸ªåˆè¡·æ˜¯ï¼šå¸Œæœ› DNN çš„å±‚æ¬¡ç»“æ„ä¸éçº¿æ€§æ¿€æ´»å‡½æ•°èƒ½å¤Ÿæ¯” one-hot embedding å…·æœ‰æ›´å¼ºçš„è¡¨è¾¾èƒ½åŠ›ã€‚
- â€œA unique feature of DHE is that it does not use any embedding table lookup, while purely relies on hidden layers to memorize and compute embeddings on the fly.â€ (Kang et al., 2021, p. 844)DHE ä¸ä½¿ç”¨ embedding tableï¼Œå®ƒåˆ©ç”¨ç¥ç»ç½‘ç»œçš„éšè—å±‚åœ¨è¿è¡Œæ—¶è®¡ç®— embedding
- â€œNNs are often considered to be highly expressive and easy to overfit. However, we found the embedding network is underfitting instead of overfitting in our case, as the embedding generation task requires highly non-linear transformations from hash encodings to embeddings.â€ (Kang et al., 2021, p. 844)DHE çš„ embedding å€¾å‘äºæ¬ æ‹Ÿåˆ
- â€œWe suspect the default ReLU activation (ğ‘“ (ğ‘¥)=max(0, ğ‘¥)) is not expressive enough, as ReLU networks are piece-wise linear functions [1]. We tried various activation functions2 and found the recently proposed Mish activation [25] (ğ‘“ (ğ‘¥)=ğ‘¥ Â· tanh(ln(1 + ğ‘’ğ‘¥ ))) consistently performs better than ReLU and others.â€ (Kang et al., 2021, p. 844)DHE çš„ embedding network æ•°æ®ç”¨ mish ä½œä¸ºæ¿€æ´»å‡½æ•°
- â€œWe also found batch normalization (BN) [16] can stabilize the training and achieve better performance.â€ (Kang et al., 2021, p. 844)ä½¿ç”¨ BN ç¨³å®šè®­ç»ƒè¿‡ç¨‹
- â€œregularization methods like dropout are not beneficial, which again verifies the bottleneck of our embedding network is underfitting.â€ (Kang et al., 2021, p. 844)Dropout æ— æ•ˆï¼Œè¿›ä¸€æ­¥è¯æ˜äº† DHE çš„é—®é¢˜æ˜¯æ¬ æ‹Ÿåˆè€Œä¸æ˜¯è¿‡æ‹Ÿåˆ
- â€œ3.4 Side Features Enhanced Encodings for Generalizationâ€ (Kang et al., 2021, p. 844)
- â€œAn interesting extension for DHE utilizes side features for learning better encodings. This helps to inject structure into our encodings, and enable better generalization among feature values, and to new values.â€ (Kang et al., 2021, p. 844)DHE å¯ä»¥åœ¨ encodingä¸­æ·»åŠ è¾…åŠ©ä¿¡æ¯ï¼Œä»è€Œå¾—åˆ°æ›´å¥½çš„ç¼–ç ç»“æœã€‚é€šè¿‡æ·»åŠ è¾…åŠ©ä¿¡æ¯ï¼Œä¸€æ¥ç»“æ„åŒ–ä¿¡æ¯æ›´å¤šï¼ŒäºŒæ¥èƒ½å¤Ÿæ³›åŒ–è‡³æ–°å“ï¼ˆæ–°ç‰¹å¾ï¼‰+[[EGES]]
- â€œOne-hot based full embeddings inherit the property of categorical features, and generate the embeddings independently (i.e., the embeddings for any two IDs are independent). Thus, one-hot based schemes can be viewed as decentralized architectures that are good at memorization but fail to achieve generalization.â€ (Kang et al., 2021, p. 844)one-hot embeddings ç»§æ‰¿äº†ç±»ç›®ç‰¹å¾çš„å±æ€§ï¼Œç‹¬ç«‹ç”Ÿæˆæ¯ä¸ªç‰¹å¾çš„ embeddingï¼ˆå³ä»»æ„ä¸¤ä¸ª id å¯¹åº”çš„ embedding éƒ½æ˜¯ç‹¬ç«‹çš„ï¼‰ã€‚å› æ­¤ï¼Œå®ƒå¯ä»¥çœ‹ä½œå»ä¸­å¿ƒåŒ–çš„æ¶æ„ï¼Œ å…·æœ‰è‰¯å¥½çš„è®°å¿†èƒ½åŠ›ï¼Œä½†æ˜¯æ³›åŒ–æ€§å·®ã€‚
- â€œIn contrast, the DHE scheme is a centralized solution: any weight change in the embedding network will affect the embeddings for all feature values. We believe the centralized structure provides a potential opportunity for generalization.â€ (Kang et al., 2021, p. 844)DHE æ˜¯ä¸€ä¸ªä¸­å¿ƒåŒ–æ–¹æ¡ˆï¼Œembedding ç½‘ç»œçš„æƒé‡æ”¹å˜ä¹‹åï¼Œæ‰€æœ‰ç‰¹å¾å€¼çš„ embedding éƒ½ä¼šéšä¹‹æ”¹å˜ï¼Œè¿™å¯èƒ½æœ‰åˆ©äºæ³›åŒ–ã€‚
- â€œWe propose side feature enhanced encodings for DHE, and hope this will improve the generalization among feature values, and to new values. One straightforward way to enhance the encoding is directly concatenating the generalizable features and the hash encodings. If the dimensionality of the feature vector is too high, we could use locality-sensitive hashing [6] to significantly reduce the cardinality while preserving the tag similarity. The enhanced encoding is then fed into the deep embedding network for embedding generation. We think that the hash encoding provides a unique identifier for memorization while the other features enable the generalization ability.â€ (Kang et al., 2021, p. 844)ä½¿ç”¨ side feature è¿›ä¸€æ­¥å¢å¼º DHEã€‚ä¸€ç§ç›´è§‚çš„æ–¹æ¡ˆæ˜¯ï¼Œå°† side feature å’Œ hash encoding æ‹¼æ¥åå†è¾“å…¥åç»­çš„è§£ç æ¨¡å—å¦‚æœç‰¹å¾å‘é‡çš„ç»´åº¦å¤ªé«˜ï¼Œ ä½¿ç”¨å±€éƒ¨æ•æ„Ÿå“ˆå¸Œï¼ˆlocality-sensitive hashingï¼ŒLSHï¼‰æ¥é™ç»´çš„åŒæ—¶ä¿ç•™ç›¸ä¼¼åº¦åŸç‰¹å¾çš„å”¯ä¸€æ€§æœ‰åŠ©äºè®°å¿†åŒ–ï¼Œè€Œæ·»åŠ çš„ side feature èƒ½è¿½åŠ æ³›åŒ–èƒ½åŠ›
- â€œ3.5 Summaryâ€ (Kang et al., 2021, p. 844)
- â€œall the feature values share the whole embedding network for embedding generation, unlike the hashing trick that shares the same embedding vector for different feature values. This makes DHE free of the embedding collision issueâ€ (Kang et al., 2021, p. 844)DHE é€šè¿‡å¤šå“ˆå¸Œçš„ç¼–ç æ¨¡å—æ¥é¿å…å“ˆå¸Œå†²çªï¼Œè€Œåå† embeddingã€‚
- â€œThe bottleneck of DHE is the computation of the embedding network, though it could be accelerated by more powerful hardware and NN acceleration approaches like pruning [8].â€ (Kang et al., 2021, p. 844)åˆ’é‡ç‚¹ï¼šDHE çš„ç“¶é¢ˆåœ¨äº embedding network çš„è®¡ç®—é‡å˜å¤§äº†ï¼ˆä» one-hot embedding çš„å•å±‚ç½‘ç»œå˜æˆäº†å¤šå±‚ç½‘ç»œï¼Œç”šè‡³å¯ä»¥æ˜¯æ›´åŠ å¤æ‚çš„ç»“æ„ï¼‰
- â€œ4 EXPERIMENTSâ€ (Kang et al., 2021, p. 844)
- (Kang et al., 2021, p. 845)ä»¥ 1/4 çš„å‚æ•°é‡ï¼ˆå‚æ•°é‡çš„å‡å°‘åªå‘ç”Ÿåœ¨ embedding layerï¼‰ï¼ŒDHE èƒ½å¤Ÿå–åˆ°æ¥è¿‘äº one-hot embedding çš„æ€§èƒ½ã€‚
- â€œ4.2 Baselinesâ€ (Kang et al., 2021, p. 845)
- â€œThe Hashing Trick [36] A classical approach for handling large-vocab categorical features, which uses a single hash function to map feature value into a smaller vocab. The method often suffers from collision problems.â€ (Kang et al., 2021, p. 845)åŸºçº¿ä¹‹ Hashing trickï¼Œé€šè¿‡å•ä¸ªå“ˆå¸Œå‡½æ•°å°†ç‰¹å¾å€¼æ˜ å°„åˆ° vocab æ›´å°çš„ç©ºé—´ä¸­ï¼Œå†æ‰§è¡Œ one-hot embeddingã€‚ç¼ºç‚¹æ˜¯å“ˆå¸Œå†²çªä¸¥é‡ã€‚
- â€œBloom Embedding [30] Inspired by bloom filter [2], Bloom Embedding generates a binary encoding with multiple hash functions. Then a linear layer is applied to the encoding to recover the embedding for the given feature value.â€ (Kang et al., 2021, p. 845)åŸºçº¿ä¹‹ bloom embeddingã€‚å—å¯å‘äºå¸ƒéš†è¿‡æ»¤å™¨ï¼Œä½¿ç”¨å¤šä¸ªå“ˆå¸Œå‡½æ•°ï¼Œç”Ÿæˆä¸€ä¸ª binary encodingï¼Œå†ä½¿ç”¨ä¸€ä¸ªçº¿æ€§å±‚ç”Ÿæˆ embedding
- â€œHash Embedding (HashEmb) [34] HashEmb uses multiple (typically two) hash functions and lookups the corresponding embeddings. Then a weighted sum of the embeddings is adopted, where the weights are learned and dedicated for each feature value.â€ (Kang et al., 2021, p. 845)åŸºçº¿ä¹‹ hash embeddingï¼ˆHashEmbï¼‰ï¼Œä½¿ç”¨å¤šä¸ªå“ˆå¸Œå‡½æ•°å’Œå¤šä¸ª embedding tableï¼ˆä»¥æ­¤å¯¹å†²å“ˆå¸Œå†²çªï¼‰ï¼Œå†å¯¹æ‰€æœ‰çš„ embedding åŠ æƒæ±‚å’Œï¼Œæƒé‡æ˜¯å¯å­¦ä¹ çš„ã€‚
- â€œHybrid Hashing [38] A recently proposed method uses onehot full embedding for frequent feature values, and uses double hashing for others.â€ (Kang et al., 2021, p. 845)åŸºçº¿ä¹‹ Hybrid hashingï¼Œæ‚äº¤æ–¹æ³•ï¼Œå¯¹é«˜é¢‘ç‰¹å¾ä½¿ç”¨ one-hot full embeddingï¼ˆä¸å‹ç¼©ï¼‰ï¼Œå¯¹éé«˜é¢‘ç‰¹å¾ä½¿ç”¨ double hashing æŠ€å·§
- â€œCompositional Embedding [32] A recently proposed method adopts two complementary hashing for avoiding hashing collision.â€ (Kang et al., 2021, p. 845)åŸºçº¿ä¹‹ compositional embeddingï¼Œä½¿ç”¨äº’è¡¥çš„ hashing æ¥é¿å…å“ˆå¸Œå†²çª
- â€œ4.3 Performance Comparison (RQ1)â€ (Kang et al., 2021, p. 845)
- â€œWe observed that DHE effectively approximates Full Embeddingâ€™s performance. In most cases, DHE achieves similar AUC with only 1/4 of the full model size. This verifies the effectiveness and efficiency (in model sizes) of DHEâ€™s hash encoding and deep embedding network,â€ (Kang et al., 2021, p. 845)DHE ä»¥ 1/4 çš„æ¨¡å‹å¤§å°ï¼ˆå‚æ•°é‡ï¼‰ï¼Œå–å¾—äº† one-hot full embedding æ¥è¿‘çš„æ€§èƒ½ã€‚
- â€œWe can see that DHE significantly outperforms hashing-based baselines in most cases. This is attributed to its unique hash encoding, which is free of collision and easy for the embedding network to distinguish, and the expressive deep structures for embedding generation.â€ (Kang et al., 2021, p. 845)ä½œè€…å°† DHE çš„æœ‰æ•ˆæ€§å½’å› äº hash encoding çš„å”¯ä¸€æ€§ï¼Œé¿å…äº†å“ˆå¸Œå†²çªï¼›embedding network çš„å¼ºå¤§è¡¨è¾¾èƒ½åŠ›ã€‚
- â€œThe Hash Trick [36] performs inferior to other methods, especially when the model size is small. This shows that the hash collisions severely hurt the performance.â€ (Kang et al., 2021, p. 845)hash trick æ–¹æ³•è¡¨ç°æœ€å·®ã€‚æ¨¡å‹è¶Šå°ï¼Œæ•ˆæœè¶Šå·®ï¼Œè¿™æ˜¯å› ä¸ºå“ˆå¸Œå†²çªæ›´åŠ ä¸¥é‡äº†ã€‚
- â€œ4.4 Comparison of Encoding Schemes (RQ2)â€ (Kang et al., 2021, p. 846)
- â€œWe can see that our proposed dense hash encoding with the uniform distribution is the best performer, while the Gaussian distribution variant is the runner-up. The binary encoding performs slightly inferior, and we think it is due to its wrong inductive bias (some IDs have more similar encodings) and the relatively low dimensionality (i.e., âŒˆlog(ğ‘›)âŒ‰).â€ (Kang et al., 2021, p. 846)æ¶ˆèå®éªŒï¼ŒéªŒè¯ä¸åŒè½¬æ¢å‡½æ•°ï¼ˆå¯¹åº”ä¸åŒçš„ hash encoding çš„åˆ†å¸ƒï¼‰çš„æ•ˆæœï¼Œå‡åŒ€åˆ†å¸ƒæœ€å°ï¼Œé«˜æ–¯åˆ†å¸ƒæ¬¡ä¹‹ï¼Œbinary encoding å†æ¬¡ã€‚binary encoding çš„é—®é¢˜å¯èƒ½å‡ºåœ¨å®ƒå¼•å…¥äº†å½’çº³åç½®ä»¥åŠè¿‡å°çš„ vocabã€‚
- (Kang et al., 2021, p. 846)â€œ4.5 Scalability Regarding the Number of Hash Functions (RQ3)â€ (Kang et al., 2021, p. 846)
- â€œWhen ğ‘˜ further increases to more than 100, we can still observe performance gains of DHE, while the one-hot hashing baselines donâ€™t benefit from more hash functions. We suspect the reason for the poor utilization of multiple hashing is that each embedding will be shared ğ‘˜ times more than single hashing (if sharing embedding tables), and this leads to more collisions.â€ (Kang et al., 2021, p. 846)å“ˆå¸Œå‡½æ•°æ•°é‡å¤§äº 100 æ—¶ï¼ŒåŸºçº¿çš„æ€§èƒ½å¼€å§‹åœæ»ï¼Œè¿™å¯èƒ½æ˜¯å› ä¸ºåœ¨å…±äº« embedding table çš„æƒ…å†µä¸‹ï¼Œéšç€ hash æ•°çš„å¢åŠ ï¼Œå†²çªåŠ å‰§ã€‚
- â€œIf creating ğ‘˜ embedding tables (i.e., not sharing), given the same memory budget, the size for each table will be ğ‘˜ times smaller, which again causes the collision issue.â€ (Kang et al., 2021, p. 846)å¦‚æœä¸å…±äº«å‚æ•°ï¼Œå³ä¸ºæ¯ä¸ª hash å‡½æ•°åˆ†é…ä¸€å¼  embedding tableï¼Œåœ¨ä¿æŒå­˜å‚¨å®¹é‡ç›¸ç­‰çš„æƒ…å†µä¸‹ï¼Œæ¯å¼  embedding table å°†å˜å°ï¼ŒåŒæ ·ä¼šå¸¦æ¥å“ˆå¸Œå†²çªã€‚ä½†æ˜¯æ²¡è¯´éè¦ä¿æŒç›¸åŒçš„å­˜å‚¨å®¹é‡å‘€ã€‚å› ä¸ºåŸºçº¿çš„ k æœ¬èº«å°±ä¸å¤§ï¼Œé¢å¤–çš„å¼€é”€å¹¶ä¸é«˜çš„ã€‚
- (Kang et al., 2021, p. 846)æ¶ˆèå®éªŒï¼ŒéªŒè¯å“ˆå¸Œå‡½æ•°çš„æ•°é‡å¯¹ DHE æ€§èƒ½çš„å½±å“ã€‚å½“æ•°é‡åœ¨ 8 ä»¥å†…ï¼ˆå«ï¼‰æ—¶ï¼ŒDHE ä¸å¦‚åŸºçº¿ï¼Œè€Œå½“æ•°é‡ç»§ç»­å¢åŠ ï¼ŒDHE çš„æ€§èƒ½å¼€å§‹åè¶…ã€‚åè¿‡æ¥çœ‹ï¼ŒåŸºçº¿åœ¨å“ˆå¸Œæ•°é‡åªæœ‰ä¸ªä½æ•°çš„æ—¶å€™ï¼Œå°±å·²ç»å–å¾—äº†ä¸ DHE å‡ åä¸ªå“ˆå¸Œæ•°é‡æ¥è¿‘çš„æ€§èƒ½çš„ã€‚åªæ˜¯æ²¡å¾—ç»§ç»­æé«˜ï¼ˆk åœ¨ 32 è‡³ 128 ä¹‹é—´ï¼Œä¼¼ä¹æ˜¯ä¸€ä¸ªè®¡ç®—é‡ä¸å¤ªå¤§åˆæœ‰æ•ˆæœçš„é€‰æ‹©ï¼‰
- â€œ4.6 Normalization and Activation (RQ4)â€ (Kang et al., 2021, p. 846)
- (Kang et al., 2021, p. 846)æ¶ˆèå®éªŒï¼ŒéªŒè¯äº† BN å’Œ mish çš„æœ‰æ•ˆæ€§
- â€œ4.7 The Effect of Depth (RQ4)â€ (Kang et al., 2021, p. 846)
- â€œwe didnâ€™t see further improvement with more hidden layers, presumably because each layerâ€™s width is too narrow or due to trainability issues on deep networksâ€ (Kang et al., 2021, p. 846)
- (Kang et al., 2021, p. 847)DHE çš„æ€§èƒ½éš embedding network å±‚æ•°çš„å¢åŠ å…ˆæå‡åä¸‹é™ï¼Œæœ€ä¼˜æ€§èƒ½åœ¨ 5å±‚å·¦å³ã€‚å¯¹äºéšåçš„æ€§èƒ½ä¸‹é™ï¼Œå¯èƒ½çš„åŸå› æ˜¯ç¥ç»ç½‘ç»œçš„å®½åº¦å¤ªçª„äº†ï¼Œæ— æ³•è®­ç»ƒæ›´æ·±çš„ç½‘ç»œäº†ã€‚
- â€œ4.8 Neural Architectures (RQ4)â€ (Kang et al., 2021, p. 847)
- â€œWe can see that the simple equalwidth MLP performs the best, and adding residual connections also slightly hurts the performance. We suspect that the low-level representations are not useful in our case, so that the attempts (as in computer vision) utilizing low-level features (like DenseNet [15] or ResNet [12]) didnâ€™t achieve better performance.â€ (Kang et al., 2021, p. 847)
- (Kang et al., 2021, p. 847)embedding network çš„ç»“æ„å¯¹ DHE æ€§èƒ½çš„å½±å“ï¼Œç­‰å®½ MLP æœ€å¥½ã€‚å…¶ä»–æ›´å¤æ‚çš„ç»“æ„å¹¶æ²¡æœ‰å–å¾—æ›´å¥½çš„æ•ˆæœï¼Œä½œè€…ä»¬æ€€ç–‘è¿™æ˜¯å› ä¸º low-level è¡¨ç¤ºçš„æœ‰æ•ˆæ€§æœ‰é™ï¼Œå¤æ‚ç»“æ„æ— æ³•å……åˆ†åˆ©ç”¨ã€‚
- â€œ4.9 Side Feature Enhanced Encodings (RQ5)â€ (Kang et al., 2021, p. 847)
- â€œThe results are shown in Table 7. We can see that using side features only in the encoding and only in the MLP have similar performance. This shows DHEâ€™s item embeddings effectively capture the Genres information, and verifies the generalization ability of item embeddings generated by DHE with enhanced encodings.â€ (Kang et al., 2021, p. 847)
- (Kang et al., 2021, p. 847)side feature å¯¹ DHE çš„å¢ç›Šæ•ˆæœã€‚è¡¨ä¸­çš„ genres æ˜¯ side informationï¼Œåœ¨ hash encoding ä¸­å¢åŠ ä¹‹åï¼ŒDHE çš„æ€§èƒ½å¾—åˆ°è¿›ä¸€æ­¥çš„æå‡ã€‚è¿™è¯´æ˜ DHE èƒ½å¤Ÿåˆ©ç”¨ side feature è·å¾—æ³›åŒ–èƒ½åŠ›çš„ã€‚
- â€œ4.10 Efficiency (RQ6)â€ (Kang et al., 2021, p. 847)
- â€œWith GPUs, DHE is about 9x slower than full embedding, and 4.5x slower than hash embeddings. However, we can see that DHE significantly benefits from GPU acceleration, while full embeddings donâ€™t. This is because the embedding lookup process in full embeddings is hard to accelerate by GPUs.â€ (Kang et al., 2021, p. 847)
- (Kang et al., 2021, p. 847)DHE çš„è®¡ç®—æ•ˆç‡ç“¶é¢ˆï¼Œcpu ä¸‹æ€§èƒ½å·®äº† 22 å€ï¼ŒGPU ä¸‹ å·®äº† 9 å€ã€‚ç¡¬ä»¶åŠ é€Ÿæ•ˆæœæ˜æ˜¾ï¼Œä½†æ˜¯ DHE å°±æ˜¯ç”¨æ—¶é—´æ•ˆç‡æ¢äº†é¢„ä¼°æ€§èƒ½ï¼Œå¦‚æœæ—¶é—´æ•ˆç‡æ˜¯ä¸å¯æ¥å—çš„ï¼Œéœ€è¦è®¾ç½®æ›´å°çš„ hash encoding size å’Œæ›´å°çš„ embedding networkã€‚ä½†æ˜¯å…¶æ€§èƒ½å¯èƒ½å‡ºç°ä¸¥é‡ä¸‹æ»‘ï¼ˆè§è¡¨4ï¼‰
- â€œA. Analysis on Encoding Propertiesâ€ (Kang et al., 2021, p. 849)
- â€œA.1 Uniquenessâ€ (Kang et al., 2021, p. 849)
- â€œDefinition .1 (Uniqueness in encoding). An encoding function ğ¸ is a unique encoding if ğ‘ƒ (ğ¸ (ğ‘¥)) = ğ‘ƒ (ğ¸ (ğ‘¦)) < ğœ–, âˆ€ğ‘¥, ğ‘¦ âˆˆ ğ‘‰ , where ğœ– is a near-zero constantâ€ (Kang et al., 2021, p. 849)ç¼–ç å”¯ä¸€æ€§çš„æ•°å­¦å®šä¹‰
- â€œFor hashing methods, the probability of having collision is 1 âˆ’ ğ‘’ âˆ’ ğ‘› (ğ‘›âˆ’1) 2 ğ‘š where ğ‘š is the total number of hashing buckets (ğ‘š2 buckets for double hashing), according to [4]. The probability is 1.0, and 0.39 for one-hot hashing and double one-hot hashing, respectively. For DHE, the number of possible hashing buckets is ğ‘šğ‘˜ = 106144, and the collision rate is extremely small.â€ (Kang et al., 2021, p. 849)
- â€œA.2 Equal Similarityâ€ (Kang et al., 2021, p. 849)
- â€œDefinition .2 (Equal similarity in encoding). An encoding functions ğ¸ is a equally similar encoding if E[Euclidean_distance(ğ¸ (ğ‘¥)âˆ’ ğ¸ (ğ‘¦))] = ğ‘, âˆ€ğ‘¥, ğ‘¦ âˆˆ ğ‘‰ , where ğ‘ is a non-zero constant.â€ (Kang et al., 2021, p. 849)ç¼–ç çš„ç­‰ç›¸ä¼¼åº¦çš„æ•°å­¦å®šä¹‰
- â€œA.4 High Shannon Entropyâ€ (Kang et al., 2021, p. 849)
- â€œDefinition .3 (High Shannon Entropy). An encoding functions ğ¸ has the high entropy property if for any dimension ğ‘–, the entropy ğ» (ğ¸ (ğ‘¥)ğ‘– ) = ğ» âˆ—, (ğ‘¥ âˆˆ ğ‘‰ ), where ğ» âˆ— = log ğ‘œ is the max entropy for ğ‘œ outcomes (e.g. ğ» âˆ— = 1 for binary outcome).â€ (Kang et al., 2021, p. 849)ç¼–ç çš„é«˜é¦™å†œç†µçš„æ•°å­¦å®šä¹‰
- (Kang et al., 2021, p. 849)DHE çš„ä¼ªä»£ç ï¼šDenseHashingEncoding + EmbeddingNetwork
- â€œB. Experimental Setupâ€ (Kang et al., 2021, p. 849)
- (Kang et al., 2021, p. 850)Deep Hash Encoding çš„ä¼ªä»£ç ï¼Œä½¿ç”¨ universal hashing $((ax+b) % p) % m$
- (Kang et al., 2021, p. 850)â€œB.2 Implementation Details & Hyper-parametersâ€ (Kang et al., 2021, p. 850)
- â€œFor DHE we use the same hyper-parameters for both datasets: ğ‘˜=1024 hash functions to generate the hash encoding vector, followed by a 5-layer feedforward neural network with Batch Normalization [16] and Mish activation function [25]â€ (Kang et al., 2021, p. 850)