tags:: [[paper_highlights]]

- Annotations(6/8/2022, 6:49:07 AM)
- “ABSTRACT” (Kang et al., 2021, p. 840)
- “In this paper, we propose an alternative embedding framework Deep Hash Embedding (DHE), replacing embedding tables by a deep embedding network to compute embeddings on the fly.” (Kang et al., 2021, p. 840)本文提出了一个新的 embedding 框架 Deep Hash Embedding（DHE），用深度网络代替了 embedding table
- “DHE first encodes the feature value to a unique identifier vector with multiple hashing functions and transformations, and then applies a DNN to convert the identifier vector to an embedding. The encoding module is deterministic, non-learnable, and free of storage, while the embedding network is updated during the training time to learn embedding generation.” (Kang et al., 2021, p. 840)DHE 将特征到 embedding 的过程拆解为了 encode 与 decode 的两步。Encode 阶段，使用多个哈希函数将特征编码为一个具有唯一性的向量（非 one-hot）；decode 阶段，使用深度神经网络来学习 embedding。编码模块是确定的、不可学习的、无存储量的；解码模块是可训练的。我的一个疑问是：使用了多个哈希函数，那么如何保存它们，以使它们在线上和线下同时可用？
- “1 INTRODUCTION” (Kang et al., 2021, p. 840)
- (Kang et al., 2021, p. 840)DHE 与常规 one-hot 的对比示意图。DHE 的参数量是不同层的 emb_dim 的乘积和，one-hot 的参数量就是 vocab_size 与最终 emb_dim 的乘积。对于超大 vocab_size，DHE 可以快速缩减 embedding 部分的参数量。
- “there are several challenges when applying embedding learning in recommendation: • Huge vocabulary size: Recommender systems usually need to handle high-cardinality categorical features (e.g. billions of video IDs for online video sharing platforms).” (Kang et al., 2021, p. 840)推荐系统中使用 embedding 的挑战 1：超大的 vocab_size，类如类目、ID 作为特征时，vocab_size 就是系统中类目、ID 的总数
- “• Dynamic nature of input: Unlike vocabularies of words that are relatively static, the vocabulary in recommender systems could be highly dynamic. New users and new items enter the system on a daily basis, and stale items are gradually vanishing.” (Kang et al., 2021, p. 841)推荐系统中使用 embedding 的挑战 2：动态输入，推荐系统需要时刻面临新用户、新商品的 OOV 问题
- “• Highly-skewed data distribution: The categorical features in recommendation data usually follow highly skewed powerlaw distributions. The small number of training examples on infrequent feature values hurts the embedding quality for the tail items significantly.” (Kang et al., 2021, p. 841)推荐系统中使用 embedding 的挑战 3：高度倾斜的数据分布（幂律分布），换言之，长尾现象很严重，少数低频特征可能会损害 embedding 的性能
- “the one-hot representation often results in a huge embedding table especially for a large-vocab feature, and it also fails to adapt to out-of-vocab feature values.” (Kang et al., 2021, p. 841)one-hot 会带来超大的 embedding table 带来超大的参数量，并且无法处理 oov 特征值。
- “In practice, to better handle new (i.e., out-of-vocab / unseen) feature values and reduce the storage cost, the hashing trick [36] is often adopted, that randomly maps feature values to a smaller number of hashing buckets, though the inevitable embedding collisions generally hurt performance.” (Kang et al., 2021, p. 841)实践中，为了处理 one-hot 带来的两个问题，通常会使用 hashing track，使用哈希函数将特征值映射到一个更小的空间（降维了），再取对应的 embedding。使用哈希函数，哈希冲突是无可避免的，即两个不同的特征值哈希后，不巧落在了同一个桶里，原本可区分的特征变得不可区分了。+林登-斯特劳斯定理
- “Specifically, we use multiple hashing and appropriate transformations to generate a unique, deterministic, dense, and real-valued vector as the identifier encoding of the given feature value, and then the deep embedding network transforms the encoding to the final feature embeddings.” (Kang et al., 2021, p. 841)此处描述了 DHE 的编码结果是唯一的、确定的、稠密的（非 one-hot）、实值的。这不像 embedding 吗？
- “2 PRELIMINARY: ONE-HOT BASED EMBEDDING LEARNING” (Kang et al., 2021, p. 841)
- “Generally, the embedding function can be decomposed into two components: T = 𝐹 ◦ 𝐸, where 𝐸 is an encoding function to represent feature values in some spaces, and 𝐹 is a decoding function to generate the embedding v.” (Kang et al., 2021, p. 841)将 embedding function 分解为 encoder 与 decoder，统一 one-hot 与其他基于哈希的方式的表示。
- “2.1 One-hot Full Embedding” (Kang et al., 2021, p. 841)
- “So we assume the feature values are already mapped to {1, 2, . . . , 𝑛}, then the embedding approach creates an embedding table W ∈ R 𝑛×𝑑 , and looks up its 𝑠-th row W𝑠 for the feature value 𝑠. This is equivalent to the following: (i) we apply the encoding function 𝐸 to encode feature value 𝑠 with a one-hot encoding vector: 𝐸 (𝑠)=b ∈ {0, 1}𝑛 where 𝑏𝑠 =1 and 𝑏 𝑗 =0 (𝑗 ≠ 𝑠); (ii) we then apply the decoding function 𝐹 , a learnable linear transformation W ∈ R𝑛×𝑑 to generate the embedding e, that is, e = 𝐹 (b) = W𝑇 b.” (Kang et al., 2021, p. 841)Encoder 、decoder 视角下的 one-hot embedding：1. one-hot encoder 将特征值编码为仅对应位置为 1 的 one-hot vector2. decoder（也就是 embedding layer） 是一个一层神经网络
- “In short, the embedding lookup process can be viewed as a 1-layer neural network (without bias terms) based on the one-hot encoding.” (Kang et al., 2021, p. 841)encoder-decoder 视角下，one-hot embedding 就是基于 one-hot encoding 的不带 bias 的一层神经网络
- (Kang et al., 2021, p. 842)不同 embedding 方案的六维对比
- “2.2 One-hot Hash Embedding” (Kang et al., 2021, p. 842)
- “Despite the simplicity and effectiveness of full embeddings, such a scheme has two major issues in large-scale or dynamic settings: (i) the size of the embedding table grows linearly with the vocabulary size, which could cause a huge memory consumption.” (Kang et al., 2021, p. 842)one-hot embedding 的两个问题：参数量与 vocab_size 成正比，大数据时代这可能是一个天文数字
- “(ii) in online learning settings where new values constantly arise, the full embedding scheme fails to handle unseen (out-of-vocab) feature values.” (Kang et al., 2021, p. 842)无法处理新人新品等 OOV 问题
- “The hashing trick [36] is a representative hashing method for reducing the dimension of the one-hot encoding for large vocabularies. The encoding function 𝐸 still maps a feature value into a one-hot vector, but with a different (typically smaller) cardinality of 𝑚: 𝐸 (𝑠) = b ∈ {0, 1}𝑚 where 𝑠 ∈ 𝑉 , 𝑏𝐻 (𝑠) =1 and 𝑏 𝑗 =0 ( 𝑗 ≠ 𝐻 (𝑠)). The hash function 𝐻 maps feature values (including unseen values) to {1, 2, . . . , 𝑚} where 𝑚 is the hashed vocabulary size. The hash function 𝐻 seeks to distribute hashing values as uniformly as possible to reduce collision, though it’s inevitable when 𝑚 < 𝑛.” (Kang et al., 2021, p. 842)hashing trick 方法：对特征值先 hash，hash 桶的 ID 作为新的特征值，再应用 ont-hot embedding。哈希桶数<<原始 vocab_size 时，能够有效降低参数量，问题是：降维效果有多好，哈希冲突就有多严重
- “In summary, the hashing trick uses hashing to map feature values into 𝑚-dim one-hot vectors, and then applies a 1-layer network to generate the embeddings.” (Kang et al., 2021, p. 842)hashing trick 在 one-hot embedding 之前，套用了一个 hash mapping
- “Although the hashing trick is able to arbitrarily reduce the cardinality of the original vocabulary 𝑉 , it suffers from the embedding collision problem.” (Kang et al., 2021, p. 842)hashing trick 的两面性：降维与哈希冲突的拔河
- “To alleviate this issue, multiple hash functions have been used to generate multiple one-hot encodings: 𝐸 (𝑠) = b = [b(1) ; b(2) ; . . . ; b(𝑘) ] ∈ {0, 1}𝑚∗𝑘 where 𝑏 (𝑖) 𝐻 (𝑖) (𝑠) =1 and 𝑏 (𝑖) 𝑗 =0 ( 𝑗 ≠ 𝐻 (𝑖) (𝑠)). Here, 𝑘 hash functions {𝐻 (1), 𝐻 (2), . . . , 𝐻 (𝑘) } are adopted to generate 𝑘 one-hot encodings {b(1), b(2), . . . , b(𝑘) }, and the concatenation is used as the encoding. The core idea is that the concatenated encodings are less likely to be collided. We can lookup 𝑘 embeddings in 𝑘 embedding tables (respectively) and aggregate them into the final embedding.” (Kang et al., 2021, p. 842)hashing trick 的改进：既然哈希冲突不可避免，那么用多个不同的哈希函数——两个不同特征总是落在同一个哈希桶内的概率会随着哈希函数的增加而降低，再查询多次 embedding table，对结果进行聚合。
- “Note that existing methods didn’t scale to large 𝑘 and the most commonly used variant is double hashing (𝑘=2)” (Kang et al., 2021, p. 842)现有的方案并没有使用很多的哈希函数，通常就两个
- “3 DEEP HASH EMBEDDINGS (DHE)” (Kang et al., 2021, p. 842)
- 3.1 Encoding Design” (Kang et al., 2021, p. 842)
- “Uniqueness” (Kang et al., 2021, p. 842)Encoding 设计原则之唯一性
- “The encoding should be unique for each feature value. This is also the target of full embedding and multiple hashing methods.” (Kang et al., 2021, p. 842)特征编码之前是唯一的，编码之后也应该是唯一的，否则就会带来编码冲突（哈希冲突）。
- “The collided encodings make the subsequent decoding function impossible to distinguish different feature values, which typically hurts model performance.” (Kang et al., 2021, p. 842)编码冲突将使得后续的解码无法区分不同的特征值
- “Equal Similarity” (Kang et al., 2021, p. 842)Encoding 设计原则之等相似性
- “We believe this introduces a wrong inductive bias (ID 8 and ID 9 are more similar), which may mislead the subsequent decoding function.” (Kang et al., 2021, p. 842)等相似性意味着，编码不应该引入归纳偏置。one-hot 编码的特点是，所有向量两两之间的相似性都是相等的。
- “As we don’t know the semantic similarity among categorical features, we should make any two encodings be equally similar, and not introduce any inductive bias.” (Kang et al., 2021, p. 842)编码不应该引入归纳偏置的原因是：我们无法提前知道类目特征之间的相似性，那么保持原样是最好的做法。
- “High dimensionality” (Kang et al., 2021, p. 842)Encoding 设计原则之高维度
- (Kang et al., 2021, p. 843)“As high-dimensional spaces are often considered to be more separable (e.g. kernel methods), we believe the encoding dimension should be relatively high as well” (Kang et al., 2021, p. 843)高维空间更具有可分性（比如 SVM 的核方法），所以作者们认为编码的维度相对更高更好
- “High Shannon Entropy” (Kang et al., 2021, p. 843)Encoding 设计原则之高香农熵
- “The Shannon entropy [31] measures (in the unit of ‘bits’) the information carried in a dimension. The high entropy requirement is to prevent redundant dimensions from the information theory perspective.” (Kang et al., 2021, p. 843)香农熵的定义：向量的每一维携带的信息量有多少。从信息论视角来看，高香农熵能够避免冗余维度。这一点和高维度有一点拉扯的味道，高维度在引入可区分性的同时可能会引入冗余特征，高香农熵则提出了降维的要求。
- “3.2 Dense Hash Encoding” (Kang et al., 2021, p. 843)
- “The proposed encoding function 𝐸 : N → R𝑘 uses 𝑘 universal hash functions to map a feature value to a 𝑘-dimensional dense and real-valued encodings. Specifically, we have 𝐸′(𝑠) = [𝐻 (1) (𝑠), 𝐻 (2) (𝑠), . . . , 𝐻 (𝑘) (𝑠)] where 𝐻 (𝑖) : N → {1, 2, . . . , 𝑚}.” (Kang et al., 2021, p. 843)DHE 使用 k 个 universal 哈希函数，将特征值编码为一个 k 维的实值向量
- “A nice property of universal hashing [4] is that the hashed values are evenly distributed (on average) over {1,2,. . . ,m}.” (Kang et al., 2021, p. 843)universal hashing 的优点是哈希的结果是均匀分布的 (伪代码见附录)
- “the integer-based 𝐸′(𝑠) encoding is not suitable to be used as the input to neural networks, as the input is typically real-valued and normalized for numeric stability. So we obtain real-valued encodings via appropriate transformations: 𝐸 (𝑠) = transform(𝐸′(𝑠)).” (Kang et al., 2021, p. 843)DHE 的解码模块是一个 DNN，DNN 的输入通常是实值并且为了数值稳定性做过归一化，因此 DHE 的 encoder 在哈希之后，还接了一个转换函数，将哈希 ID 转换为一个实值
- “Uniform Distribution. We simply normalize the encoding 𝐸′ to the range of [−1, 1]. As the hashing values are evenly distributed (on average) among {1, 2, . . . , 𝑚}, this approximates the uniform distribution 𝑈 (−1, 1) reasonably well with a large 𝑚.” (Kang et al., 2021, p. 843)本文提出并最终使用的将哈希 ID 转换为服从均匀分布的 [-1,1] 的值
- “Gaussian Distribution. We first use the above transformation to obtain uniform distributed samples, and then apply the Box–Muller transform [3] which converts the uniformly distributed samples (i.e., 𝑈 (−1, 1)) to the standard normal distribution N (0, 1).” (Kang et al., 2021, p. 843)本文提出的另一种分布，在均匀分布的结果基础上，再执行一次 Box-Muller 转换，将均匀分布转为标准正态分布
- “Empirically we found the two distributions work similarly well, and thus we choose the uniform distribution by default for simplicity.” (Kang et al., 2021, p. 843)实验的经验是，均匀分布和高斯分布的效果很接近，而均匀分布的实现更容易，因此使用了对应的转换函数
- “Unlike existing hashing methods limited to a few hash functions, we choose a relatively large 𝑘 for satisfying the high-dimensionality property (𝑘=1024 in our experiments, though it’s significantly smaller than 𝑛). We empirically found our method significantly benefits from larger 𝑘” (Kang et al., 2021, p. 843)与以往的方法不同之处：使用了大量哈希函数，而 DHE 似乎从更多的哈希函数中获益更多
- “3.3 Deep Embedding Network” (Kang et al., 2021, p. 843)
- “the realvalued encoding is not applicable for embedding lookup. However, the mapping process is very similar to a highly non-linear feature transformation, where the input feature is fixed and non-learnable. Therefore, we use powerful deep neural networks (DNN) to model such a complex transformation” (Kang et al., 2021, p. 843)这里的说法有点因果倒置的味道。Encoding 阶段说，为了使用 DNN，所以在哈希之后再将特征转为服从均匀分布的实值，但是这里又说，因为实值，只能使用 DNN。本身，DHE 的 encoding 就是用哈希模拟了一次 embedding，而且得到的实值向量由于是不可训练的，就像使用了静态 embedding vector。
- “Moreover, a recent study shows that deeper networks are more parameter-efficient than shallow networks [21]” (Kang et al., 2021, p. 843)研究显示，更深的网络的效率比浅层网络更高
- “We hope the hierarchical structures and non-linear activations enable DNNs to express the embedding function more efficiently than the one-hot encodings (i.e., 1-layer wide NN). This is motivated by recent research that shows that deep networks can approximate functions with much fewer parameters compared with wide and shallow networks [21].” (Kang et al., 2021, p. 844)研究表明：深度网络能够以更少的参数模拟宽度网络，因此本文的一个初衷是：希望 DNN 的层次结构与非线性激活函数能够比 one-hot embedding 具有更强的表达能力。
- “A unique feature of DHE is that it does not use any embedding table lookup, while purely relies on hidden layers to memorize and compute embeddings on the fly.” (Kang et al., 2021, p. 844)DHE 不使用 embedding table，它利用神经网络的隐藏层在运行时计算 embedding
- “NNs are often considered to be highly expressive and easy to overfit. However, we found the embedding network is underfitting instead of overfitting in our case, as the embedding generation task requires highly non-linear transformations from hash encodings to embeddings.” (Kang et al., 2021, p. 844)DHE 的 embedding 倾向于欠拟合
- “We suspect the default ReLU activation (𝑓 (𝑥)=max(0, 𝑥)) is not expressive enough, as ReLU networks are piece-wise linear functions [1]. We tried various activation functions2 and found the recently proposed Mish activation [25] (𝑓 (𝑥)=𝑥 · tanh(ln(1 + 𝑒𝑥 ))) consistently performs better than ReLU and others.” (Kang et al., 2021, p. 844)DHE 的 embedding network 数据用 mish 作为激活函数
- “We also found batch normalization (BN) [16] can stabilize the training and achieve better performance.” (Kang et al., 2021, p. 844)使用 BN 稳定训练过程
- “regularization methods like dropout are not beneficial, which again verifies the bottleneck of our embedding network is underfitting.” (Kang et al., 2021, p. 844)Dropout 无效，进一步证明了 DHE 的问题是欠拟合而不是过拟合
- “3.4 Side Features Enhanced Encodings for Generalization” (Kang et al., 2021, p. 844)
- “An interesting extension for DHE utilizes side features for learning better encodings. This helps to inject structure into our encodings, and enable better generalization among feature values, and to new values.” (Kang et al., 2021, p. 844)DHE 可以在 encoding中添加辅助信息，从而得到更好的编码结果。通过添加辅助信息，一来结构化信息更多，二来能够泛化至新品（新特征）+[[EGES]]
- “One-hot based full embeddings inherit the property of categorical features, and generate the embeddings independently (i.e., the embeddings for any two IDs are independent). Thus, one-hot based schemes can be viewed as decentralized architectures that are good at memorization but fail to achieve generalization.” (Kang et al., 2021, p. 844)one-hot embeddings 继承了类目特征的属性，独立生成每个特征的 embedding（即任意两个 id 对应的 embedding 都是独立的）。因此，它可以看作去中心化的架构， 具有良好的记忆能力，但是泛化性差。
- “In contrast, the DHE scheme is a centralized solution: any weight change in the embedding network will affect the embeddings for all feature values. We believe the centralized structure provides a potential opportunity for generalization.” (Kang et al., 2021, p. 844)DHE 是一个中心化方案，embedding 网络的权重改变之后，所有特征值的 embedding 都会随之改变，这可能有利于泛化。
- “We propose side feature enhanced encodings for DHE, and hope this will improve the generalization among feature values, and to new values. One straightforward way to enhance the encoding is directly concatenating the generalizable features and the hash encodings. If the dimensionality of the feature vector is too high, we could use locality-sensitive hashing [6] to significantly reduce the cardinality while preserving the tag similarity. The enhanced encoding is then fed into the deep embedding network for embedding generation. We think that the hash encoding provides a unique identifier for memorization while the other features enable the generalization ability.” (Kang et al., 2021, p. 844)使用 side feature 进一步增强 DHE。一种直观的方案是，将 side feature 和 hash encoding 拼接后再输入后续的解码模块如果特征向量的维度太高， 使用局部敏感哈希（locality-sensitive hashing，LSH）来降维的同时保留相似度原特征的唯一性有助于记忆化，而添加的 side feature 能追加泛化能力
- “3.5 Summary” (Kang et al., 2021, p. 844)
- “all the feature values share the whole embedding network for embedding generation, unlike the hashing trick that shares the same embedding vector for different feature values. This makes DHE free of the embedding collision issue” (Kang et al., 2021, p. 844)DHE 通过多哈希的编码模块来避免哈希冲突，而后再 embedding。
- “The bottleneck of DHE is the computation of the embedding network, though it could be accelerated by more powerful hardware and NN acceleration approaches like pruning [8].” (Kang et al., 2021, p. 844)划重点：DHE 的瓶颈在于 embedding network 的计算量变大了（从 one-hot embedding 的单层网络变成了多层网络，甚至可以是更加复杂的结构）
- “4 EXPERIMENTS” (Kang et al., 2021, p. 844)
- (Kang et al., 2021, p. 845)以 1/4 的参数量（参数量的减少只发生在 embedding layer），DHE 能够取到接近于 one-hot embedding 的性能。
- “4.2 Baselines” (Kang et al., 2021, p. 845)
- “The Hashing Trick [36] A classical approach for handling large-vocab categorical features, which uses a single hash function to map feature value into a smaller vocab. The method often suffers from collision problems.” (Kang et al., 2021, p. 845)基线之 Hashing trick，通过单个哈希函数将特征值映射到 vocab 更小的空间中，再执行 one-hot embedding。缺点是哈希冲突严重。
- “Bloom Embedding [30] Inspired by bloom filter [2], Bloom Embedding generates a binary encoding with multiple hash functions. Then a linear layer is applied to the encoding to recover the embedding for the given feature value.” (Kang et al., 2021, p. 845)基线之 bloom embedding。受启发于布隆过滤器，使用多个哈希函数，生成一个 binary encoding，再使用一个线性层生成 embedding
- “Hash Embedding (HashEmb) [34] HashEmb uses multiple (typically two) hash functions and lookups the corresponding embeddings. Then a weighted sum of the embeddings is adopted, where the weights are learned and dedicated for each feature value.” (Kang et al., 2021, p. 845)基线之 hash embedding（HashEmb），使用多个哈希函数和多个 embedding table（以此对冲哈希冲突），再对所有的 embedding 加权求和，权重是可学习的。
- “Hybrid Hashing [38] A recently proposed method uses onehot full embedding for frequent feature values, and uses double hashing for others.” (Kang et al., 2021, p. 845)基线之 Hybrid hashing，杂交方法，对高频特征使用 one-hot full embedding（不压缩），对非高频特征使用 double hashing 技巧
- “Compositional Embedding [32] A recently proposed method adopts two complementary hashing for avoiding hashing collision.” (Kang et al., 2021, p. 845)基线之 compositional embedding，使用互补的 hashing 来避免哈希冲突
- “4.3 Performance Comparison (RQ1)” (Kang et al., 2021, p. 845)
- “We observed that DHE effectively approximates Full Embedding’s performance. In most cases, DHE achieves similar AUC with only 1/4 of the full model size. This verifies the effectiveness and efficiency (in model sizes) of DHE’s hash encoding and deep embedding network,” (Kang et al., 2021, p. 845)DHE 以 1/4 的模型大小（参数量），取得了 one-hot full embedding 接近的性能。
- “We can see that DHE significantly outperforms hashing-based baselines in most cases. This is attributed to its unique hash encoding, which is free of collision and easy for the embedding network to distinguish, and the expressive deep structures for embedding generation.” (Kang et al., 2021, p. 845)作者将 DHE 的有效性归因于 hash encoding 的唯一性，避免了哈希冲突；embedding network 的强大表达能力。
- “The Hash Trick [36] performs inferior to other methods, especially when the model size is small. This shows that the hash collisions severely hurt the performance.” (Kang et al., 2021, p. 845)hash trick 方法表现最差。模型越小，效果越差，这是因为哈希冲突更加严重了。
- “4.4 Comparison of Encoding Schemes (RQ2)” (Kang et al., 2021, p. 846)
- “We can see that our proposed dense hash encoding with the uniform distribution is the best performer, while the Gaussian distribution variant is the runner-up. The binary encoding performs slightly inferior, and we think it is due to its wrong inductive bias (some IDs have more similar encodings) and the relatively low dimensionality (i.e., ⌈log(𝑛)⌉).” (Kang et al., 2021, p. 846)消融实验，验证不同转换函数（对应不同的 hash encoding 的分布）的效果，均匀分布最小，高斯分布次之，binary encoding 再次。binary encoding 的问题可能出在它引入了归纳偏置以及过小的 vocab。
- (Kang et al., 2021, p. 846)“4.5 Scalability Regarding the Number of Hash Functions (RQ3)” (Kang et al., 2021, p. 846)
- “When 𝑘 further increases to more than 100, we can still observe performance gains of DHE, while the one-hot hashing baselines don’t benefit from more hash functions. We suspect the reason for the poor utilization of multiple hashing is that each embedding will be shared 𝑘 times more than single hashing (if sharing embedding tables), and this leads to more collisions.” (Kang et al., 2021, p. 846)哈希函数数量大于 100 时，基线的性能开始停滞，这可能是因为在共享 embedding table 的情况下，随着 hash 数的增加，冲突加剧。
- “If creating 𝑘 embedding tables (i.e., not sharing), given the same memory budget, the size for each table will be 𝑘 times smaller, which again causes the collision issue.” (Kang et al., 2021, p. 846)如果不共享参数，即为每个 hash 函数分配一张 embedding table，在保持存储容量相等的情况下，每张 embedding table 将变小，同样会带来哈希冲突。但是没说非要保持相同的存储容量呀。因为基线的 k 本身就不大，额外的开销并不高的。
- (Kang et al., 2021, p. 846)消融实验，验证哈希函数的数量对 DHE 性能的影响。当数量在 8 以内（含）时，DHE 不如基线，而当数量继续增加，DHE 的性能开始反超。反过来看，基线在哈希数量只有个位数的时候，就已经取得了与 DHE 几十个哈希数量接近的性能的。只是没得继续提高（k 在 32 至 128 之间，似乎是一个计算量不太大又有效果的选择）
- “4.6 Normalization and Activation (RQ4)” (Kang et al., 2021, p. 846)
- (Kang et al., 2021, p. 846)消融实验，验证了 BN 和 mish 的有效性
- “4.7 The Effect of Depth (RQ4)” (Kang et al., 2021, p. 846)
- “we didn’t see further improvement with more hidden layers, presumably because each layer’s width is too narrow or due to trainability issues on deep networks” (Kang et al., 2021, p. 846)
- (Kang et al., 2021, p. 847)DHE 的性能随 embedding network 层数的增加先提升后下降，最优性能在 5层左右。对于随后的性能下降，可能的原因是神经网络的宽度太窄了，无法训练更深的网络了。
- “4.8 Neural Architectures (RQ4)” (Kang et al., 2021, p. 847)
- “We can see that the simple equalwidth MLP performs the best, and adding residual connections also slightly hurts the performance. We suspect that the low-level representations are not useful in our case, so that the attempts (as in computer vision) utilizing low-level features (like DenseNet [15] or ResNet [12]) didn’t achieve better performance.” (Kang et al., 2021, p. 847)
- (Kang et al., 2021, p. 847)embedding network 的结构对 DHE 性能的影响，等宽 MLP 最好。其他更复杂的结构并没有取得更好的效果，作者们怀疑这是因为 low-level 表示的有效性有限，复杂结构无法充分利用。
- “4.9 Side Feature Enhanced Encodings (RQ5)” (Kang et al., 2021, p. 847)
- “The results are shown in Table 7. We can see that using side features only in the encoding and only in the MLP have similar performance. This shows DHE’s item embeddings effectively capture the Genres information, and verifies the generalization ability of item embeddings generated by DHE with enhanced encodings.” (Kang et al., 2021, p. 847)
- (Kang et al., 2021, p. 847)side feature 对 DHE 的增益效果。表中的 genres 是 side information，在 hash encoding 中增加之后，DHE 的性能得到进一步的提升。这说明 DHE 能够利用 side feature 获得泛化能力的。
- “4.10 Efficiency (RQ6)” (Kang et al., 2021, p. 847)
- “With GPUs, DHE is about 9x slower than full embedding, and 4.5x slower than hash embeddings. However, we can see that DHE significantly benefits from GPU acceleration, while full embeddings don’t. This is because the embedding lookup process in full embeddings is hard to accelerate by GPUs.” (Kang et al., 2021, p. 847)
- (Kang et al., 2021, p. 847)DHE 的计算效率瓶颈，cpu 下性能差了 22 倍，GPU 下 差了 9 倍。硬件加速效果明显，但是 DHE 就是用时间效率换了预估性能，如果时间效率是不可接受的，需要设置更小的 hash encoding size 和更小的 embedding network。但是其性能可能出现严重下滑（见表4）
- “A. Analysis on Encoding Properties” (Kang et al., 2021, p. 849)
- “A.1 Uniqueness” (Kang et al., 2021, p. 849)
- “Definition .1 (Uniqueness in encoding). An encoding function 𝐸 is a unique encoding if 𝑃 (𝐸 (𝑥)) = 𝑃 (𝐸 (𝑦)) < 𝜖, ∀𝑥, 𝑦 ∈ 𝑉 , where 𝜖 is a near-zero constant” (Kang et al., 2021, p. 849)编码唯一性的数学定义
- “For hashing methods, the probability of having collision is 1 − 𝑒 − 𝑛 (𝑛−1) 2 𝑚 where 𝑚 is the total number of hashing buckets (𝑚2 buckets for double hashing), according to [4]. The probability is 1.0, and 0.39 for one-hot hashing and double one-hot hashing, respectively. For DHE, the number of possible hashing buckets is 𝑚𝑘 = 106144, and the collision rate is extremely small.” (Kang et al., 2021, p. 849)
- “A.2 Equal Similarity” (Kang et al., 2021, p. 849)
- “Definition .2 (Equal similarity in encoding). An encoding functions 𝐸 is a equally similar encoding if E[Euclidean_distance(𝐸 (𝑥)− 𝐸 (𝑦))] = 𝑐, ∀𝑥, 𝑦 ∈ 𝑉 , where 𝑐 is a non-zero constant.” (Kang et al., 2021, p. 849)编码的等相似度的数学定义
- “A.4 High Shannon Entropy” (Kang et al., 2021, p. 849)
- “Definition .3 (High Shannon Entropy). An encoding functions 𝐸 has the high entropy property if for any dimension 𝑖, the entropy 𝐻 (𝐸 (𝑥)𝑖 ) = 𝐻 ∗, (𝑥 ∈ 𝑉 ), where 𝐻 ∗ = log 𝑜 is the max entropy for 𝑜 outcomes (e.g. 𝐻 ∗ = 1 for binary outcome).” (Kang et al., 2021, p. 849)编码的高香农熵的数学定义
- (Kang et al., 2021, p. 849)DHE 的伪代码：DenseHashingEncoding + EmbeddingNetwork
- “B. Experimental Setup” (Kang et al., 2021, p. 849)
- (Kang et al., 2021, p. 850)Deep Hash Encoding 的伪代码，使用 universal hashing $((ax+b) % p) % m$
- (Kang et al., 2021, p. 850)“B.2 Implementation Details & Hyper-parameters” (Kang et al., 2021, p. 850)
- “For DHE we use the same hyper-parameters for both datasets: 𝑘=1024 hash functions to generate the hash encoding vector, followed by a 5-layer feedforward neural network with Batch Normalization [16] and Mish activation function [25]” (Kang et al., 2021, p. 850)