- ABSTRACTâ€ (Zhou et al., 2018, p. 1)
  â€œDeep Interest Network (DIN) which tackles this challenge by designing a local activation unit to adaptively learn the representation of user interests from historical behaviors with respect to a certain ad.â€ (Zhou et al., 2018, p. 1)
  DIN çš„è¦ç‚¹ï¼šç»™å®šç‰¹å®šå¹¿å‘Š/å•†å“ï¼Œä½¿ç”¨å±€éƒ¨æ¿€æ´»å•å…ƒï¼ˆä¸€ç§ attentionï¼‰ä»ç”¨æˆ·å†å²è¡Œä¸ºä¸­è‡ªé€‚åº”ï¼ˆåŠ æƒï¼‰åœ°å­¦ä¹ ç”¨æˆ·å…´è¶£è¡¨ç¤º
- â€œBesides, we develop two techniques: mini-batch aware regularization and data adaptive activation function which can help training industrial deep networks with hundreds of millions of parameters.â€ (Zhou et al., 2018, p. 1)
  DIN è®ºæ–‡çš„å¦å¤–ä¸¤ç‚¹è´¡çŒ®ï¼šmini-batch aware æ­£åˆ™åŒ–ï¼Œæ–°çš„æ¿€æ´»å‡½æ•° dice
- 1 INTRODUCTIONâ€ (Zhou et al., 2018, p. 1)
  â€œThese methods follow a similar Embedding&MLP paradigm: large scale sparse input features are first mapped into low dimensional embedding vectors, and then transformed into fixed-length vectors in a group-wise manner, finally concatenated together to fed into fully connected layers (also known as multilayer perceptron, MLP) to learn the nonlinear relations among features.â€ (Zhou et al., 2018, p. 1)
  æ·±åº¦å­¦ä¹ æ¨èç³»ç»Ÿçš„ä¼ ç»ŸèŒƒå¼ï¼šembedding+mlpï¼Œembedding ä½œä¸ºç‰¹å¾æŠ½å–å™¨
- â€œUsers might be interested in different kinds of goods simultaneously when visiting the e-commerce site. That is to say, user interests are diverse.â€ (Zhou et al., 2018, p. 1)
  DIN çš„å‡ºå‘ç‚¹ï¼šç”¨æˆ·å…´è¶£å…·æœ‰å¤šæ ·æ€§ï¼Œä»–å¯èƒ½å¯¹ä¸åŒå•†å“éƒ½æœ‰å…´è¶£
- â€œWhen it comes to CTR prediction task, user interests are usually captured from user behavior data. Embedding&MLP methods learn the representation of all interests for a certain user by transforming the embedding vectors of user behaviors into a fixed-length vector, which is in an euclidean space where all usersâ€™ representation vectors are.â€ (Zhou et al., 2018, p. 1)
  åœ¨ CTR é¢„ä¼°ä»»åŠ¡ä¸­ï¼Œç”¨æˆ·å…´è¶£è¡¨ç°åœ¨ç”¨æˆ·è¡Œä¸ºä¸­
- â€œdiverse interests of the user are compressed into a fixed-length vector, which limits the expressive ability of Embedding&MLP methods.â€ (Zhou et al., 2018, p. 1)
  ç”¨æˆ·å…´è¶£çš„è¡¨ç¤ºæœ€ç»ˆæ˜¯ä¸€ä¸ªå®šé•¿çš„å‘é‡ï¼Œè¿™é‡Œçš„å…³é”®æ˜¯ä¸åŠ åŒºåˆ†çš„èšåˆç”¨æˆ·å…´è¶£ï¼ˆpooling ç­‰æ–¹å¼ï¼‰ï¼Œä¼šé€ æˆä¿¡æ¯çš„æå¤§ä¸¢å¤±ï¼Œä»è€Œé™åˆ¶äº† Embedding&MLP çš„è¡¨è¾¾èƒ½åŠ›
- â€œOn the other hand, it is not necessary to compress all the diverse interests of a certain user into the same vector when predicting a candidate ad because only part of userâ€™s interests will influence his/her action (to click or not to click).â€ (Zhou et al., 2018, p. 1)
  ç»™å®šå€™é€‰é¡¹ï¼ˆå¾…æ’å•†å“/å¹¿å‘Šï¼‰ï¼Œåœ¨è¡¨ç¤ºç”¨æˆ·å…´è¶£çš„æ—¶å€™ï¼Œä½¿ç”¨ä»–æ‰€æœ‰çš„è¡Œä¸ºï¼ˆè¡Œä¸ºè¡¨è¾¾äº†å…´è¶£ï¼‰æ˜¯éå¿…è¦çš„ï¼Œå› ä¸ºåªæœ‰è¡Œä¸ºè¿‡çš„å•†å“æ‰ä¸å€™é€‰é¡¹ç›¸å…³ã€‚
- â€œBy introducing a local activation unit, DIN pays attentions to the related user interests by soft-searching for relevant parts of historical behaviors and takes a weighted sum pooling to obtain the representation of user interests with respect to the candidate ad.â€ (Zhou et al., 2018, p. 1)
  ç»¼ä¸Šä¸¤ç‚¹ï¼šDIN ä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶æ¥è½¯æœç´¢ï¼ˆåŒºåˆ«äºç¡¬è¿‡æ»¤ï¼‰ç”¨æˆ·å†å²è¡Œä¸ºä¸­ä¸ç»™å®šå€™é€‰é¡¹ç›¸å…³çš„éƒ¨åˆ†ï¼Œä½¿ç”¨åŠ æƒå¹³å‡çš„æ–¹å¼æ¥è·å¾—å¯¹äºå€™é€‰é¡¹çš„ç”¨æˆ·å…´è¶£
- adding with traditional â„“2 regularization, the computation turns to be unacceptable, which needs to calculate L2-norm over the whole parameters (with size scaling up to billions in our situation) for each mini-batch.â€ (Zhou et al., 2018, p. 2)
  å¯¹äºæ¨èç³»ç»Ÿæ¨¡å‹çš„è®­ç»ƒï¼Œä½¿ç”¨L2æ­£åˆ™ï¼Œå®ƒä¼šå¯¹æ‰€æœ‰çš„å‚æ•°è®¡ç®—L2æ­£åˆ™é¡¹ã€‚ç”±äºæ¨èç³»ç»Ÿäº¿çº§çš„å·¨å¤§å‚æ•°é‡ï¼Œè®¡ç®—æ•ˆç‡æ˜¯éš¾ä»¥æ¥å—çš„ã€‚
- â€œwe develop a novel mini-batch aware regularization where only parameters of non-zero features appearing in each mini-batch participate in the calculation of L2-norm, making the computation acceptable.â€ (Zhou et al., 2018, p. 2)
  DIN ç”±æ­¤æå‡ºäº† mini-batch aware regularizationï¼Œå³é’ˆå¯¹å½“å‰çš„æ‰¹é‡æ•°æ®ä¸­éé›¶ç‰¹å¾å¯¹åº”çš„å‚æ•°è®¡ç®—L2æ­£åˆ™é¡¹ï¼Œä»è€Œæå¤§åœ°å‡å°‘è®¡ç®—é‡ã€‚
- â€œwe design a data adaptive activation function, which generalizes commonly used PReLU[12] by adaptively adjusting the rectified point w.r.t. distribution of inputs and is shown to be helpful for training industrial networks with sparse features.â€ (Zhou et al., 2018, p. 2)
  DIN dice æ˜¯ PReLU çš„å˜å½¢ï¼Œæ”¹åŠ¨äº†ä¸¤ç‚¹ï¼š1ã€æ ¹æ®è¾“å…¥æ•°æ®çš„åˆ†å¸ƒæ¥è®¡ç®—æ•´æµç‚¹ï¼Œè€Œä¸æ˜¯å›ºå®šä¸º0ï¼Œä»æ›²çº¿å›¾ä¸Šçœ‹ï¼Œç›¸å½“äºå·¦å³å¹³ç§»äº†ï¼›2ã€åŸæ¥çš„é˜¶è·ƒå‡½æ•°å˜å¾—æ›´åŠ å¹³æ»‘
- â€œ4 DEEP INTEREST NETWORKâ€ (Zhou et al., 2018, p. 3)
  â€œ4.1 Feature Representationâ€ (Zhou et al., 2018, p. 3)
  â€œThen one instance can be represent as x = [tT 1 ,tT 2 , ...tT M ]T in a group-wise manner, where M is number of feature groups, ÃM i=1 Ki = K, K is dimensionality of the entire feature spaceâ€ (Zhou et al., 2018, p. 3)
- (Zhou et al., 2018, p. 3)
  æ¨èç³»ç»Ÿç‰¹å¾çš„åˆ’åˆ†åŠå…¶ vocab å¤§å°ï¼ˆç»´åº¦ï¼‰ï¼šç”¨æˆ·ç”»åƒã€ç”¨æˆ·è¡Œä¸ºã€å¹¿å‘Š/å•†å“ç‰¹å¾ã€ä¸Šä¸‹æ–‡ç‰¹å¾ã€‚ç”¨æˆ·è¡Œä¸ºç‰¹å¾çš„ç»´åº¦ç›¸å¯¹æ›´é«˜ï¼Œæ•°æ®æ›´åŠ ç¨€ç–ï¼Œè¿™æ˜¯å› ä¸ºä½¿ç”¨çš„å¤šå°‘ç±»ç›®ã€IDå‹ç‰¹å¾ã€‚DIN ç”¨åˆ°äº†å•†å“IDã€åº—é“ºIDã€å•†å“ç±»ç›®IDã€‚
- (Zhou et al., 2018, p. 3)
  ä»ä¸Šè¡¨å’Œå½“å‰ç¤ºä¾‹ï¼Œå¯ä»¥çœ‹åˆ° DIN å°†ç”¨æˆ·å†å²è¡Œä¸ºè¿‡çš„å•†å“åˆ—è¡¨ç›´æ¥å½“ä½œä¸€ä¸ª multi-hot å‘é‡æ¥å¤„ç†äº†ã€‚è¿˜åŸè¿™ä¸ªè¿‡ç¨‹ï¼Œç”¨æˆ·å†å²è¡Œä¸ºè¿‡çš„å•†å“åˆ—è¡¨ï¼Œé¦–å…ˆæ˜¯ä¸€ä¸ª one-hot vector çš„åˆ—è¡¨ï¼Œä¹Ÿå¯ä»¥çœ‹ä½œä¸€ä¸ª one-hot matrixï¼Œå°†çŸ©é˜µæŒ‰å•†å“ç»´åº¦æ±‚â€œä¸ï¼ˆANDï¼‰â€ï¼Œå°±å‹ç¼©æˆäº†ä¸€ä¸ª multi-hot vectorã€‚
  çœ‹ä½œåˆ—è¡¨/åºåˆ—çš„å½¢å¼ï¼Œä¼šæ›´åŠ è‡ªç„¶ï¼Œç¼ºç‚¹æ˜¯ï¼Œè¿™ä¸ªåˆ—è¡¨æ˜¯å˜é•¿çš„ã€‚ä» multi-hot vector çš„è§†è§’æ¥çœ‹çš„å¥½å¤„æ˜¯ï¼Œå®¹æ˜“å°†å…¶å¯¹åº”åˆ°ç‰¹å¾æŠ½å–çš„å®šé•¿å‘é‡ï¼ˆä»å‘é‡åˆ°å‘é‡ï¼‰ã€‚
- â€œNote that in our setting, there are no combination features. We capture the interaction of features with deep neural network.â€ (Zhou et al., 2018, p. 3)
  æœ¬æ–‡çš„å®éªŒä¸­ï¼Œæ²¡æœ‰å®éªŒç‰¹å¾ç»„åˆï¼Œè€Œæ˜¯ä½¿ç”¨æ·±åº¦æ¨¡å‹æ¥åšéšå¼çš„ç‰¹å¾ç»„åˆã€‚
- â€œ4.2 Base Model(Embedding&MLP)â€ (Zhou et al., 2018, p. 3)
- (Zhou et al., 2018, p. 4)
  sum-pooling ä¸ DIN çš„ weighted-sum pooling çš„æ¨¡å‹ç»“æ„å¯¹æ¯”ï¼Œå³ä¸Šè§’æ˜¯ DIN æå‡ºçš„ local activation unit çš„è®¡ç®—å½¢å¼
- â€œ4.3 The structure of Deep Interest Networkâ€ (Zhou et al., 2018, p. 4)
  â€œThe local activation characteristic of user interests gives us inspiration to design a novel model named deep interest network(DIN).â€ (Zhou et al., 2018, p. 4)
  å†æ¬¡å¼ºè°ƒäº† DIN çš„è®¾è®¡åˆè¡·ï¼šç”¨æˆ·å…´è¶£çš„å±€éƒ¨æ¿€æ´»ç‰¹æ€§
- â€œbehaviors related to displayed ad greatly contribute to the click action. DIN simulates this process by paying attention to the representation of locally activated interests w.r.t. given ad.â€ (Zhou et al., 2018, p. 4)
  DIN ç”¨å±€éƒ¨æ¿€æ´»å•å…ƒï¼ˆæ³¨æ„åŠ›æœºåˆ¶ï¼‰æ¨¡æ‹Ÿäº†ç”¨æˆ·è¡Œä¸ºåºåˆ—ä¸­ä¸ç»™å®šå¹¿å‘Š/å•†å“çš„ç›¸å…³åº¦è®¡ç®—ï¼Œè®¡ç®—å¾—åˆ°çš„ç›¸å…³åº¦ï¼Œä¹Ÿæ˜¯å…´è¶£å€¼ã€‚
- â€œHowever different from traditional attention method, the constraint of Ã i wi = 1 is relaxed in Eq.(3), aiming to reserve the intensity of user interests. That is, normalization with softmax on the output of a(Â·) is abandoned. Instead, value of Ã i wi is treated as an approximation of the intensity of activated user interests to some degree.â€ (Zhou et al., 2018, p. 5)
  DIN çš„å±€éƒ¨æ¿€æ´»ä¸å¸¸è§„ attention çš„å·®å¼‚ç‚¹ï¼šDIN å¸Œæœ›ä¿ç•™ç”¨æˆ·å…´è¶£çš„å¼ºåº¦ï¼Œæ‰€ä»¥æ²¡æœ‰ä½¿ç”¨ softmax åšå½’ä¸€åŒ–ã€‚
- â€œWe have tried LSTM to model user historical behavior data in the sequential manner. But it shows no improvement. Different from text which is under the constraint of grammar in NLP task, the sequence of user historical behaviors may contain multiple concurrent interests. Rapid jumping and sudden ending over these interests causes the sequence data of user behaviors to seem to be noisy.â€ (Zhou et al., 2018, p. 5)
  æ­¤æ—¶ï¼ˆDIN åœ¨é˜¿é‡Œå®éªŒçš„æ—¶é—´å¤§çº¦æ˜¯ 17å¹´ï¼‰ï¼Œå°è¯•è¿‡ä½¿ç”¨ LSTM æ¥å»ºæ¨¡ç”¨æˆ·è¡Œä¸ºåºåˆ—ï¼Œæ²¡æœ‰å–å¾—å¯è§‚çš„æå‡ã€‚è¿™é‡Œä½œè€…çš„æ¨æµ‹æ˜¯ï¼šç”¨æˆ·è¡Œä¸ºåºåˆ—ä¸­åŒæ—¶åŒ…å«äº†å¤šç§å…´è¶£ï¼Œç›¸é‚»è¡Œä¸ºä¹‹é—´çš„è”ç³»ä¸å¦‚ NLP çš„è¯ã€å¥ä¹‹é—´é‚£ä¹ˆå¼ºï¼Œæ¢è¨€ä¹‹ï¼Œå°±æ˜¯ç”¨æˆ·å†å²è¡Œä¸ºçš„è·³è·ƒæ€§æ›´å¼ºï¼Œéš¾ä»¥è¢« RNN/LSTM æ‰€æ•è·ã€‚
  åé¢ä»–ä»¬æå‡ºçš„ DIEN ç®—æ˜¯å¯¹æ­¤å¤„æœªè§£é—®é¢˜çš„ç»§ç»­æ±‚è§£ã€‚
  æˆ‘çœ‹è¿‡DUPNçš„æ¨¡å‹ç¤ºæ„å›¾ï¼Œå®ƒæ˜¯å…ˆè¿‡  LSTMï¼Œå†è¿‡ attention çš„
- â€œ5 TRAINING TECHNIQUESâ€ (Zhou et al., 2018, p. 5)
  â€œ5.1 Mini-batch Aware Regularizationâ€ (Zhou et al., 2018, p. 5)
  â€œOnly parameters of non-zero sparse features appearing in each mini-batch needs to be updated in the scenario of SGD based optimization methods without regularization. However, when adding â„“2 regularization it needs to calculate L2-norm over the whole parameters for each mini-batch, which leads to extremely heavy computations and is unacceptable with parameters scaling up to hundreds of millions.â€ (Zhou et al., 2018, p. 5)
  æ— æ­£åˆ™é¡¹æ—¶ï¼ŒSGD åªä¼šæ›´æ–° mini-batch ä¸­éé›¶ç‰¹å¾å¯¹åº”çš„å‚æ•°ï¼›ä½†æ˜¯åŠ å…¥ L2 æ­£åˆ™é¡¹ä¹‹åï¼Œç”±äºå®ƒçš„è®¡ç®—æ˜¯é’ˆå¯¹æ•´ä¸ªå‚æ•°ç©ºé—´çš„ï¼Œå› æ­¤ä¼šå¯¹æ‰€æœ‰å‚æ•°è®¡ç®—æ­£åˆ™é¡¹ã€‚è¿™å¸¦æ¥äº†å·¨å¤§çš„è®¡ç®—å¼€é”€ã€‚
- (Zhou et al., 2018, p. 5)
  Diceï¼ˆå…¬å¼è§ä¸‹ï¼‰ ä¸ PReLU çš„å¯¹æ¯”ï¼Œä¸¤ç‚¹å·®å¼‚ï¼š1ã€æ•´æµç‚¹ä¸å†å›ºå®šï¼Œæ ¹æ®è¾“å…¥æ•°æ®è®¡ç®—ï¼›2ã€æ›²çº¿å˜å¾—æ›´åŠ å¹³æ»‘ï¼Œä¸å†æ˜¯é˜¶è·ƒä¿¡å·
- â€œ5.2 Data Adaptive Activation Functionâ€ (Zhou et al., 2018, p. 5)
  â€œPReLU takes a hard rectified point with value of 0, which may be not suitable when the inputs of each layer follow different distributions.â€ (Zhou et al., 2018, p. 5)
  PReLU çš„æ•´æµç‚¹å›ºå®šä¸º 0ï¼Œè¿™å¯¹äºæœä»ä¸åŒåˆ†å¸ƒçš„æ•°æ®æ˜¯ç¡¬ä¼¤ã€‚
- (Zhou et al., 2018, p. 5)
  Dice çš„å‡½æ•°å¼
- â€œIn the training phrase, E[s] and V ar [s] is the mean and variance of input in each mini-batch. In the testing phrase, E[s] and V ar [s] is calculated by moving averages E[s] and V ar [s] over data. Îµ is a small constant which is set to be 10âˆ’8 in our practice.â€ (Zhou et al., 2018, p. 5)
  è®­ç»ƒè¿‡ç¨‹ä¸­ï¼ŒDice ä½¿ç”¨äº†æ¯ä¸ª mini-batch çš„å‡å€¼ä¸æ–¹å·®ï¼›æµ‹è¯•æ—¶ï¼Œåˆ™ä½¿ç”¨æ•°æ®é›†ä¸Šçš„ç§»åŠ¨å¹³å‡æ¥è®¡ç®—å‡å€¼ä¸æ–¹å·®ã€‚
- â€œ6 EXPERIMENTSâ€ (Zhou et al., 2018, p. 6)
  â€œ6.1 Datasets and Experimental Setupâ€ (Zhou et al., 2018, p. 6)
  â€œAmazon Datasetâ€ (Zhou et al., 2018, p. 6)
  â€œMovieLens Datasetâ€ (Zhou et al., 2018, p. 6)
  â€œWe segment the data into training and testing dataset based on userID.â€ (Zhou et al., 2018, p. 6)
  ä½¿ç”¨ç”¨æˆ· ID æ¥åˆ’åˆ†æ•°æ®é›†
  æ›´å¸¸è§„çš„åšæ³•åº”è¯¥æ˜¯ç”¨ {date} åšæµ‹è¯•é›†ï¼Œ{date}ä¹‹å‰çš„ N å¤©æ•°æ®åšè®­ç»ƒé›†ï¼Œé˜²æ­¢æ•°æ®ç©¿è¶Šã€‚
- â€œAlibaba Datasetâ€ (Zhou et al., 2018, p. 6)
  â€œFor all the deep models, the dimensionality of embedding vector is 12 for the whole 16 groups of features. Layers of MLP is set by 192 Ã— 200 Ã— 80 Ã— 2. Due to the huge size of data, we set the mini-batch size to be 5000 and use Adam[15] as the optimizer.â€ (Zhou et al., 2018, p. 6)
- â€œexponential decay, in which learning rate starts at 0.001 and decay rate is set to 0.9.â€ (Zhou et al., 2018, p. 6)
  DIN çš„æ¨¡å‹è®¾ç½®ï¼šemb_dim=12ï¼Œmlp çš„ hidden_size=192ã€200ã€80ã€2ï¼Œbatch_size=5000ï¼Œä½¿ç”¨ Adamï¼Œlr=0.001ï¼Œè¡°é€€ç‡ä¸º 0.9
- â€œ6.3 Metricsâ€ (Zhou et al., 2018, p. 6)
  â€œAn variation of user weighted AUC is introduced in [7, 13] which measures the goodness of intra-user order by averaging AUC over users and is shown to be more relevant to online performance in display advertising system.â€ (Zhou et al., 2018, p. 6)
- (Zhou et al., 2018, p. 6)
  ä½¿ç”¨ GAUC ä½œä¸ºæŒ‡æ ‡ï¼ŒæŒ‰ç”¨æˆ·åˆ†ç»„è®¡ç®— AUCï¼Œå†æŒ‰æ›å…‰é‡æ±‚åŠ æƒå¹³å‡ã€‚æŒ‰ç…§æœ¬æ–‡çš„è¯´æ³•ï¼ŒGAUC æ›´èƒ½åæ˜ çº¿ä¸Šæ€§èƒ½ã€‚
- â€œBesides, we follow [25] to introduce RelaImpr metric to measure relative improvement over models. For a random guesser, the value of AUC is 0.5.â€ (Zhou et al., 2018, p. 6)
- (Zhou et al., 2018, p. 6)
  æœ¬æ–‡å®éªŒçš„å¦ä¸€ä¸ª metricï¼ŒAUC çš„ç›¸å¯¹æå‡åº¦
- (Zhou et al., 2018, p. 7)
  Alibaba æ•°æ®é›†ä¸Šï¼Œä¸åŒæ¨¡å‹çš„è®­ç»ƒæŸå¤±ã€‚ç»¿è‰²ä¸Šçº¿æ˜¯ä¸ç”¨æ­£åˆ™é¡¹çš„ caseï¼Œåœ¨ä¸€ä¸ª epoch ä¹‹åï¼Œå¼€å§‹ä¸¥é‡è¿‡æ‹Ÿåˆã€‚
- (Zhou et al., 2018, p. 7)
  åœ¨ MovieLens å’Œ Amazon æ•°æ®é›†ä¸Šï¼ŒDIN ä¸åŸºçº¿çš„å¯¹æ¯”ï¼Œè¯æ˜äº†å…¶æ•ˆæœï¼ˆDIN ğŸ‚ï¸ğŸºï¸ï¼‰
- (Zhou et al., 2018, p. 7)
  æ­£åˆ™åŒ–çš„æ¶ˆèå®éªŒï¼Œä½¿ç”¨æœ¬æ–‡æå‡ºçš„ mini-batch aware regularizationï¼ˆMBAï¼‰æ•ˆæœæœ€å¥½
- â€œ6.5 Performance of regularizationâ€ (Zhou et al., 2018, p. 7)
  â€œâ€¢ Dropout[22]. Randomly discard 50% of feature ids in each sample. â€¢ Filter. Filter visited Ğ´oods_id by occurrence frequency in samples and leave only the most frequent ones. In our setting, top 20 million Ğ´oods_ids are left. â€¢ Regularization in DiFacto[16]. Parameters associated with frequent features are less over-regularized. â€¢ MBA. Our proposed Mini-Batch Aware regularization method (Eq.4). Regularization parameter Î» for both DiFacto and MBA is searched and set to be 0.01.â€ (Zhou et al., 2018, p. 7)
  æ­£åˆ™åŒ–æ¶ˆèå®éªŒçš„å¯¹ç…§ç»„ï¼š1ã€Dropoutï¼Œæ³¨æ„çœ‹ droupout çš„å¯¹è±¡æ˜¯ ID ä»¬ï¼›2ã€è¿‡æ»¤ï¼Œè¿‡æ»¤æ‰é•¿å°¾å•†å“ IDï¼›3ã€Difactoï¼Œå¯¹é«˜é¢‘å’Œé•¿å°¾ç‰¹å¾ä½¿ç”¨ä¸åŒçš„æ­£åˆ™åŒ–åŠ›åº¦ï¼Œé«˜é¢‘ç‰¹å¾æ›´ä¸å®¹æ˜“è¿‡æ­£åˆ™åŒ–ï¼Œå› æ­¤åŠ å¼ºå¯¹ä»–ä»¬çš„æ­£åˆ™ï¼Œå‡å¼±é•¿å°¾ç‰¹å¾çš„æ­£åˆ™
- â€œDropout prevents quick overfitting but causes slower convergence. Frequency filter relieves overfitting to a degree. Regularization in DiFacto sets a greater penalty on Ğ´oods_id with high frequency, which performs worse than frequency filter.â€ (Zhou et al., 2018, p. 8)
  å®éªŒä¸­ï¼ŒDropout ID ç‰¹å¾ï¼Œèƒ½å¤Ÿæœ‰æ•ˆé˜²æ­¢è¿‡æ‹Ÿåˆï¼Œä½†æ˜¯è®­ç»ƒæ”¶æ•›å˜å¾—æ›´æ…¢ï¼›è¿‡æ»¤é•¿å°¾ç‰¹å¾èƒ½å¤Ÿç¼“è§£è¿‡æ‹Ÿåˆï¼›DiFacto æ³•ä¹Ÿæœ‰ä¸€ç‚¹æ•ˆæœã€‚
- â€œ6.7 Result from online A/B testingâ€ (Zhou et al., 2018, p. 8)
  â€œCareful online A/B testing in the display advertising system in Alibaba was conducted from 2017-05 to 2017-06. During almost a monthâ€™s testing, DIN trained with the proposed regularizer and activation function contributes up to 10.0% CTR and 3.8% RPM(Revenue Per Mille) promotion4 compared with the introduced BaseModel, the last version of our online-serving model.â€ (Zhou et al., 2018, p. 8)
  çº¿ä¸Šåšäº†è¿‘ä¸€ä¸ªæœˆçš„ ab å®éªŒï¼ŒCTR æå‡æ˜¾è‘—ï¼š+10%ï¼
- (Zhou et al., 2018, p. 8)
  DIN ä¸åŸºçº¿åœ¨ Alibaba æ•°æ®é›†ä¸Šçš„æ€§èƒ½å¯¹æ¯”
- â€œIn our practice, several important techniques are deployed for accelerating online serving of industrial deep networks under the CPU-GPU architecture: i) request batching which merges adjacent requests from CPU to take advantage of GPU power, ii) GPU memory optimization which improves the access pattern to reduce wasted transactions in GPU memory, iii) concurrent kernel computation which allows execution of matrix computations to be processed with multiple CUDA kernels concurrently.â€ (Zhou et al., 2018, p. 8)
  ä¸ºäº† DIN ä¸Šçº¿ï¼Œåšå‡ºçš„å·¥ç¨‹ä¼˜åŒ–çš„ç‚¹ï¼š1ã€ç›¸é‚»è¯·æ±‚åˆå¹¶åå†é¢„ä¼°ï¼Œå……åˆ†åˆ©ç”¨ GPU çš„ç®—åŠ›ï¼Œ2ã€GPU å†…å­˜ä¼˜åŒ–ï¼Œ3ã€å¹¶å‘ä¼˜åŒ–
- â€œ6.8 Visualization of DINâ€ (Zhou et al., 2018, p. 8)
  â€œTaking the young mother mentioned before as example, we randomly select 9 categories (dress, sport shoes, bags, etc) and 100 goods of each category as the candidate ads for her. Fig.6 shows the visualization of embedding vectors of goods with t-SNE[17] learned by DIN, in which points with same shape correspond to the same category. We can see that goods with same category almost belong to one cluster, which shows the clustering property of DIN embeddings clearly.â€ (Zhou et al., 2018, p. 8)
- â€œBesides, we color the points that represent candidate ads by the prediction value. Fig.6 is also a heat map of this motherâ€™s interest density distribution for potential candidates in embedding space. It shows DIN can form a multimodal interest density distributionâ€ (Zhou et al., 2018, p. 8)
- (Zhou et al., 2018, p. 9)
  DIN çš„å¯è§†åŒ–æ•ˆæœï¼Œç”¨æˆ·å†å²è¡Œä¸ºä¸å€™é€‰çš„ç›¸å…³åº¦
- (Zhou et al., 2018, p. 9)
  DIN çš„å¯è§†åŒ–æ•ˆæœï¼Œå¯ä»¥çœ‹åˆ°ï¼ŒDIN å…·æœ‰ä¸€å®šçš„èšç±»æ•ˆæœ